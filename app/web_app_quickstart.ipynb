{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "literary-issue",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "surprised-singer",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/assassin/anaconda3/envs/pytorch/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/assassin/anaconda3/envs/pytorch/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/assassin/anaconda3/envs/pytorch/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/assassin/anaconda3/envs/pytorch/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/assassin/anaconda3/envs/pytorch/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/assassin/anaconda3/envs/pytorch/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/assassin/anaconda3/envs/pytorch/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/assassin/anaconda3/envs/pytorch/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/assassin/anaconda3/envs/pytorch/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/assassin/anaconda3/envs/pytorch/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/assassin/anaconda3/envs/pytorch/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/assassin/anaconda3/envs/pytorch/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from tokenizer import BertTokenizerWithMapping\n",
    "from models.mlp import BertMTL, BertClassifier\n",
    "from models.params import MTLParams\n",
    "from copy import deepcopy\n",
    "import os \n",
    "from flask import Flask, request, redirect, url_for, render_template\n",
    "from azure_search.bing_utils import bing_wiki_search, get_wiki_docs\n",
    "from itertools import chain\n",
    "import numpy as np \n",
    "import csv\n",
    "\n",
    "\n",
    "bert_dir = 'bert-base-uncased'\n",
    "evi_finder_loc = './trained_models/fever/evidence_token_identifier.pt'\n",
    "cls_loc = 'trained_models/fever/evidence_classifier.pt'\n",
    "classes = [\"SUPPORTS\", \"REFUTES\"]\n",
    "device = torch.device('cpu')\n",
    "top = 3\n",
    "max_sentence = 30\n",
    "debug = True\n",
    "debug = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "thermal-drove",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading models\n"
     ]
    }
   ],
   "source": [
    "if debug:\n",
    "    print('debug, will not load models')\n",
    "else:\n",
    "    print(\"Loading models\")\n",
    "    tokenizer = BertTokenizerWithMapping.from_pretrained(bert_dir)\n",
    "    max_length = 512\n",
    "    use_half_precision = False\n",
    "\n",
    "    mtl_params = MTLParams(dim_cls_linear=256, num_labels=2, dim_exp_gru=128)\n",
    "\n",
    "    evi_finder = BertMTL(bert_dir=bert_dir,\n",
    "                         tokenizer=tokenizer,\n",
    "                         mtl_params=mtl_params,\n",
    "                         use_half_precision=False)\n",
    "    evi_finder.load_state_dict(torch.load(evi_finder_loc, map_location=device))\n",
    "\n",
    "    cls = BertClassifier(bert_dir=bert_dir,\n",
    "                         pad_token_id=tokenizer.pad_token_id,\n",
    "                         cls_token_id=tokenizer.cls_token_id,\n",
    "                         sep_token_id=tokenizer.sep_token_id,\n",
    "                         num_labels=mtl_params.num_labels,\n",
    "                         max_length=max_length,\n",
    "                         mtl_params=mtl_params,\n",
    "                         use_half_precision=False)\n",
    "    cls.load_state_dict(torch.load(cls_loc, map_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "sized-seminar",
   "metadata": {},
   "outputs": [],
   "source": [
    "app = Flask(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fatty-gender",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(query):\n",
    "    query = query.strip().lower()\n",
    "    return query\n",
    "\n",
    "\n",
    "@app.route('/', methods=['GET', 'POST']) \n",
    "def main_page():\n",
    "    if request.method == 'POST':\n",
    "        query = request.form['query']\n",
    "        query = clean(query)\n",
    "        return redirect(url_for('prediction', query=query))\n",
    "    return render_template('index.html')\n",
    "\n",
    "\n",
    "def preprocess(query, docs):\n",
    "    query = query.split()\n",
    "    tokenized_q, tokenized_q_token_slides = tokenizer.encode_docs([[query]])\n",
    "    tokenized_q = tokenized_q[0]\n",
    "    tokenized_q_token_slide = tokenized_q_token_slides[0]\n",
    "    docs_clean = [[list(chain.from_iterable([s.split() + ['.'] for s in d.lower().split('.')][:max_sentence]))] \n",
    "                  for d in docs]\n",
    "    docs = deepcopy(docs_clean)\n",
    "    tokenized_docs, tokenized_doc_token_slides = tokenizer.encode_docs(docs)\n",
    "    tokenized_docs = [list(chain.from_iterable(tokenized_docs[i])) for i in range(top)]\n",
    "    tokenized_doc_token_slides = [list(chain.from_iterable(tokenized_doc_token_slides[i]))\n",
    "                                  for i in range(top)]\n",
    "    return tokenized_q, tokenized_q_token_slide,\\\n",
    "           tokenized_docs, tokenized_doc_token_slides,\\\n",
    "           docs_clean\n",
    "\n",
    "\n",
    "def mark_evidence(queries, docs, hard_preds, wildcard='.'):\n",
    "    wildcard_tensor = tokenizer.convert_tokens_to_ids('.') * torch.ones(max_length).type(torch.int)\n",
    "    doc_max_len = max_length - 2 - len(queries[0])\n",
    "    docs = [d[:doc_max_len] if len(d) >= doc_max_len \n",
    "                            else torch.cat([d, torch.zeros(doc_max_len-len(d)).type(torch.int64)])\n",
    "            for d in docs]\n",
    "    new_docs = []\n",
    "    for q, d, e in zip(queries, docs, hard_preds):\n",
    "        temp = torch.cat([torch.zeros(1).type(torch.int64), q, torch.zeros(1).type(torch.int64), d])\n",
    "        temp = e * temp + (1-e) * wildcard_tensor\n",
    "        new_docs.append(temp[(len(queries[0]) + 2):].type(torch.int64))\n",
    "    return queries, new_docs\n",
    "\n",
    "\n",
    "def adapt_exp_pred(exp, doc):\n",
    "    exp = exp[:len(doc[0])] + [0] * (len(doc[0]) - len(exp))\n",
    "    return exp\n",
    "\n",
    "\n",
    "def merge_subtoken_exp_preds(exp_preds, slides):\n",
    "    ret = []\n",
    "    for p, ss in zip(exp_preds, slides):\n",
    "        p = p.tolist()\n",
    "        ret.append([max(p[s[0]:s[1]] + [0]) for s in ss])\n",
    "    return ret\n",
    "\n",
    "\n",
    "def color_cls_pred(c, \n",
    "                   pos_label='SUPPORTS', pos_color='green', \n",
    "                   neg_label='REFUTES', neg_color='red',\n",
    "                   default_color='gray'):\n",
    "    color = default_color\n",
    "    if c == pos_label:\n",
    "        color = pos_color\n",
    "    elif c == neg_label:\n",
    "        color = neg_color\n",
    "    return f'<p style=\"color:{color};\">{c}</p>'\n",
    "\n",
    "    \n",
    "def highlight_exp_pred(exp, doc):\n",
    "    ret = ''\n",
    "    abrcount = 0\n",
    "    abrflag = False  # for abbreviation\n",
    "    for e, w in zip(exp, doc[0]):\n",
    "        if e == 1:\n",
    "            ret += f'<span style=\"background-color:tomato; float: left\">{w}&nbsp;</span>'\n",
    "            abrcount = 0\n",
    "            abrflag = False\n",
    "        else:\n",
    "            if abrflag:\n",
    "                continue\n",
    "            abrcount += 1\n",
    "            if abrcount > 4:\n",
    "                abrflag = True\n",
    "                ret += f'<span style=\"float:left\">...&nbsp;</span>'\n",
    "            else:\n",
    "                ret += f'<span style=\"float:left\">{w}&nbsp;</span>'\n",
    "    return ret\n",
    "\n",
    "\n",
    "def postprocess(cls_preds, exp_preds, docs_clean, urls):\n",
    "    cls_strs = [color_cls_pred(c) for c in cls_preds]\n",
    "    evi_strs = [highlight_exp_pred(exp, doc) for exp, doc in zip(exp_preds, docs_clean)]\n",
    "    pred = {\n",
    "        'clses': cls_strs,\n",
    "        'evis': evi_strs,\n",
    "        'links': urls\n",
    "    }        \n",
    "    return pred\n",
    "    \n",
    "\n",
    "def get_bogus_pred():\n",
    "    pred = {\n",
    "        'clses': ['123123123' for i in range(3)],\n",
    "        'evis': ['absabsabs' for i in range(3)],\n",
    "        'links': ['www.abs.com' for i in range(3)],\n",
    "        'query': 'this is not a query'\n",
    "    }\n",
    "    return pred\n",
    "\n",
    "\n",
    "ugc_data_fname = 'data/ugc.csv' # user generated content\n",
    "mgc_data_fname = 'data/mgc.csv' # machine genarated content\n",
    "temp_data_fname = 'data/temp.csv'\n",
    "if not os.path.isfile(ugc_data_fname):\n",
    "    with open(ugc_data_fname, 'w+', newline='') as fout:\n",
    "        writer = csv.writer(fout)\n",
    "        writer.writerow('query url evidence label'.split())\n",
    "    \n",
    "if not os.path.isfile(mgc_data_fname):\n",
    "    with open(mgc_data_fname, 'w+', newline='') as fout:\n",
    "        writer = csv.writer(fout)\n",
    "        writer.writerow('query url evidence label'.split())\n",
    "        \n",
    "        \n",
    "def dump_quel(fname, query, urls, docs, exps, labels, mode='a+'):\n",
    "    with open(fname, mode, newline='') as fout:\n",
    "        for url, doc, exp, label in zip(urls, docs, exps, labels):\n",
    "#             print(doc, exp)\n",
    "            assert len(doc[0]) == len(exp)\n",
    "            writer = csv.writer(fout)\n",
    "            writer.writerow([query, url, list(zip(doc[0], exp)), label])\n",
    "            \n",
    "\n",
    "def restore_from_temp(temp_fname):\n",
    "    query = None\n",
    "    urls, docs, exps, labels = [], [], [], []\n",
    "    with open(temp_fname, 'r', newline='') as fin:\n",
    "        reader = csv.reader(fin)\n",
    "        for query, url, evidence, label in reader:\n",
    "            evidence = eval(evidence)\n",
    "            doc, exp = zip(*evidence)\n",
    "            urls.append(url)\n",
    "            docs.append([doc])\n",
    "            exps.append(exp)\n",
    "            labels.append(label)\n",
    "    return query, urls, docs, exps, labels\n",
    "\n",
    "\n",
    "@app.route('/select', methods=['GET', 'POST'])\n",
    "def select():\n",
    "    query, urls, docs, _, _ = restore_from_temp(temp_data_fname)\n",
    "    def _get_ugc(docs, form):\n",
    "        labels = []\n",
    "#         print([i for i in form.keys()])\n",
    "        evis = [[0 for w in doc[0]] for doc in docs]\n",
    "        for i in range(top):\n",
    "            labels.append(form[f\"cls{i}\"])\n",
    "#             print([form.get(f'exp{i},{j}') for j in range(3)])\n",
    "#             print(len(docs[i]))\n",
    "            for j in range(len(docs[i][0])):\n",
    "#                 print(evis, labels)\n",
    "                if form.get(f'exp{i},{j}') is not None:\n",
    "                    evis[i][j] = 1\n",
    "#         print(evis, labels)    \n",
    "        return evis, labels       \n",
    "    if request.method == 'POST':\n",
    "        evis, labels = _get_ugc(docs, request.form)\n",
    "        dump_quel(ugc_data_fname, query, urls, docs, evis, labels)\n",
    "        return render_template('thank_contrib.html')\n",
    "#     print(docs)\n",
    "    return render_template('select.html', query=query, docs=docs, urls=urls)\n",
    "\n",
    "                                        \n",
    "@app.route('/prediction/<query>', methods=['GET', 'POST'])\n",
    "def prediction(query):\n",
    "    if request.method == 'POST':\n",
    "#         print(request.form['satisfy'])\n",
    "        if request.form['satisfy'] == 'Yes!':\n",
    "            query, urls, docs, exps, labels = restore_from_temp(temp_data_fname)\n",
    "            with open(mgc_data_fname, 'a+') as fout:\n",
    "                for url, doc, exp, label in zip(urls, docs, exps, labels):\n",
    "                    fout.write(f\"{query}, {url}, {list(zip(doc[0], exp))}, {label}\\n\")\n",
    "            return render_template('thankyou.html')\n",
    "        if request.form['satisfy'] == 'No...':\n",
    "#             print(url_for('select', query, docs))\n",
    "            return redirect(url_for('select'))\n",
    "    if debug:\n",
    "        pred = get_bogus_pred()\n",
    "        query = pred['query']\n",
    "        docs_clean = [[['a' for a in range(3)]] for b in range(3) ]\n",
    "        exp_preds = [[0]*3 for i in range(3)]\n",
    "        cls_preds = ['REFUTES' for i in range(3)]\n",
    "        wiki_urls = pred['links']\n",
    "    else:\n",
    "        def _predict(exp, cls, queries, docs):\n",
    "            with torch.no_grad():\n",
    "                exp.eval()\n",
    "\n",
    "                aux_preds, exp_preds, att_masks = exp(queries, [i for i in range(top)], docs)\n",
    "\n",
    "                hard_exp_preds = torch.round(exp_preds)\n",
    "                queries, docs = mark_evidence(queries, docs, hard_exp_preds)\n",
    "\n",
    "                cls_preds = cls(queries, [i for i in range(top)], docs)\n",
    "\n",
    "                aux_preds = [classes[torch.argmax(p)] for p in aux_preds]\n",
    "                cls_preds = [classes[torch.argmax(p)] for p in cls_preds]\n",
    "                hard_exp_preds = [p[(len(queries[0]) + 2):] for p in hard_exp_preds]\n",
    "                hard_exp_preds = merge_subtoken_exp_preds(hard_exp_preds, tokenized_d_token_slides)\n",
    "            return aux_preds, cls_preds, hard_exp_preds, docs_clean\n",
    "\n",
    "        wiki_urls = bing_wiki_search(query)[:top]\n",
    "        orig_docs = [get_wiki_docs(url) for url in wiki_urls]\n",
    "        tokenized_q, tokenized_q_token_slide, tokenized_ds, tokenized_d_token_slides, docs_clean = \\\n",
    "            preprocess(query, orig_docs)\n",
    "        queries = [torch.tensor(tokenized_q[0], dtype=torch.long) for i in range(top)]\n",
    "        docs = [torch.tensor(s, dtype=torch.long) for s in tokenized_ds]\n",
    "        aux_preds, cls_preds, exp_preds, docs_clean = _predict(evi_finder, cls, queries, docs)\n",
    "        exp_preds = [adapt_exp_pred(exp, doc) for exp, doc in zip(exp_preds, docs_clean)]\n",
    "        pred = postprocess(cls_preds, exp_preds, docs_clean, wiki_urls)\n",
    "        pred['query'] = query\n",
    "    dump_quel(temp_data_fname, query, wiki_urls, docs_clean, exp_preds, cls_preds, 'w+')\n",
    "    return render_template('predict.html', pred=pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bright-toyota",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app \"__main__\" (lazy loading)\n",
      " * Environment: production\n",
      "\u001b[31m   WARNING: This is a development server. Do not use it in a production deployment.\u001b[0m\n",
      "\u001b[2m   Use a production WSGI server instead.\u001b[0m\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " * Running on http://127.0.0.1:8080/ (Press CTRL+C to quit)\n",
      "127.0.0.1 - - [23/Feb/2021 22:06:54] \"\u001b[37mGET / HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [23/Feb/2021 22:06:57] \"\u001b[32mPOST / HTTP/1.1\u001b[0m\" 302 -\n",
      "127.0.0.1 - - [23/Feb/2021 22:07:06] \"\u001b[37mGET /prediction/microsoft%20is%20a%20chinese%20company HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [23/Feb/2021 22:07:08] \"\u001b[32mPOST /prediction/microsoft%20is%20a%20chinese%20company HTTP/1.1\u001b[0m\" 302 -\n",
      "127.0.0.1 - - [23/Feb/2021 22:07:08] \"\u001b[37mGET /select HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [23/Feb/2021 22:16:03] \"\u001b[37mPOST /select HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [23/Feb/2021 22:16:05] \"\u001b[37mGET / HTTP/1.1\u001b[0m\" 200 -\n"
     ]
    }
   ],
   "source": [
    "app.run(host='127.0.0.1', port=8080)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
