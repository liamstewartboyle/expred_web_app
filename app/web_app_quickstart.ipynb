{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "literary-issue",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "southern-seller",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/assassin/anaconda3/envs/pytorch/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/assassin/anaconda3/envs/pytorch/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/assassin/anaconda3/envs/pytorch/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/assassin/anaconda3/envs/pytorch/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/assassin/anaconda3/envs/pytorch/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/assassin/anaconda3/envs/pytorch/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/assassin/anaconda3/envs/pytorch/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/assassin/anaconda3/envs/pytorch/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/assassin/anaconda3/envs/pytorch/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/assassin/anaconda3/envs/pytorch/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/assassin/anaconda3/envs/pytorch/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/assassin/anaconda3/envs/pytorch/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from tokenizer import BertTokenizerWithMapping\n",
    "from models.mlp import BertMTL, BertClassifier\n",
    "from models.params import MTLParams\n",
    "from copy import deepcopy\n",
    "import os \n",
    "from flask import Flask, request, redirect, url_for, render_template\n",
    "from azure_search.bing_utils import bing_wiki_search, get_wiki_docs\n",
    "from itertools import chain\n",
    "import numpy as np \n",
    "\n",
    "\n",
    "bert_dir = 'bert-base-uncased'\n",
    "evi_finder_loc = './trained_models/fever/evidence_token_identifier.pt'\n",
    "cls_loc = 'trained_models/fever/evidence_classifier.pt'\n",
    "classes = [\"SUPPORTS\", \"REFUTES\"]\n",
    "device = torch.device('cpu')\n",
    "top = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "thermal-drove",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Loading model\")\n",
    "tokenizer = BertTokenizerWithMapping.from_pretrained(bert_dir)\n",
    "max_length = 512\n",
    "use_half_precision = False\n",
    "\n",
    "mtl_params = MTLParams(dim_cls_linear=256, num_labels=2, dim_exp_gru=128)\n",
    "\n",
    "evi_finder = BertMTL(bert_dir=bert_dir,\n",
    "                     tokenizer=tokenizer,\n",
    "                     mtl_params=mtl_params,\n",
    "                     use_half_precision=False)\n",
    "evi_finder.load_state_dict(torch.load(evi_finder_loc, map_location=device))\n",
    "\n",
    "cls = BertClassifier(bert_dir=bert_dir,\n",
    "                     pad_token_id=tokenizer.pad_token_id,\n",
    "                     cls_token_id=tokenizer.cls_token_id,\n",
    "                     sep_token_id=tokenizer.sep_token_id,\n",
    "                     num_labels=mtl_params.num_labels,\n",
    "                     max_length=max_length,\n",
    "                     mtl_params=mtl_params,\n",
    "                     use_half_precision=False)\n",
    "cls.load_state_dict(torch.load(cls_loc, map_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "norman-present",
   "metadata": {},
   "outputs": [],
   "source": [
    "# query = 'microsoft is a chinese company'\n",
    "app = Flask(__name__)\n",
    "# import jinja2\n",
    "# env = jinja2.Environment()\n",
    "# env.globals.update(zip=zip)\n",
    "# app.jinja_env.filters['zip'] = zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "alone-warner",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(query):\n",
    "    query = query.strip().lower()\n",
    "    return query\n",
    "\n",
    "\n",
    "@app.route('/', methods=['GET', 'POST']) \n",
    "def main_page():\n",
    "    if request.method == 'POST':\n",
    "        query = request.form['query']\n",
    "        query = clean(query)\n",
    "        return redirect(url_for('prediction', query=query))\n",
    "    return render_template('index.html')\n",
    "\n",
    "\n",
    "def preprocess(query, docs):\n",
    "    query = query.split()\n",
    "    tokenized_q, tokenized_q_token_slides = tokenizer.encode_docs([[query]])\n",
    "    tokenized_q = tokenized_q[0]\n",
    "    tokenized_q_token_slide = tokenized_q_token_slides[0]\n",
    "    docs_clean = [[list(chain.from_iterable(s.split() + ['.'] for s in d.lower().split('.')))] \n",
    "                  for d in docs]\n",
    "    docs = deepcopy(docs_clean)\n",
    "    tokenized_docs, tokenized_doc_token_slides = tokenizer.encode_docs(docs)\n",
    "    tokenized_docs = [list(chain.from_iterable(tokenized_docs[i])) for i in range(top)]\n",
    "    tokenized_doc_token_slides = [list(chain.from_iterable(tokenized_doc_token_slides[i]))\n",
    "                                  for i in range(top)]\n",
    "    return tokenized_q, tokenized_q_token_slide,\\\n",
    "           tokenized_docs, tokenized_doc_token_slides,\\\n",
    "           docs_clean\n",
    "\n",
    "\n",
    "def mark_evidence(queries, docs, hard_preds, wildcard='.'):\n",
    "    wildcard_tensor = tokenizer.convert_tokens_to_ids('.') * torch.ones(max_length).type(torch.int)\n",
    "    doc_max_len = max_length - 2 - len(queries[0])\n",
    "    docs = [d[:doc_max_len] if len(d) >= doc_max_len \n",
    "                            else torch.cat([d, torch.zeros(doc_max_len-len(d)).type(torch.int64)])\n",
    "            for d in docs]\n",
    "    new_docs = []\n",
    "    for q, d, e in zip(queries, docs, hard_preds):\n",
    "        temp = torch.cat([torch.zeros(1).type(torch.int64), q, torch.zeros(1).type(torch.int64), d])\n",
    "        temp = e * temp + (1-e) * wildcard_tensor\n",
    "        new_docs.append(temp[(len(queries[0]) + 2):].type(torch.int64))\n",
    "    return queries, new_docs\n",
    "\n",
    "\n",
    "def merge_subtoken_exp_preds(exp_preds, slides):\n",
    "    ret = []\n",
    "    for p, ss in zip(exp_preds, slides):\n",
    "        p = p.tolist()\n",
    "        ret.append([max(p[s[0]:s[1]] + [0]) for s in ss])\n",
    "    return ret\n",
    "\n",
    "\n",
    "def color_cls_pred(c, \n",
    "                   pos_label='SUPPORTS', pos_color='green', \n",
    "                   neg_label='REFUTES', neg_color='red',\n",
    "                   default_color='gray'):\n",
    "    color = default_color\n",
    "    if c == pos_label:\n",
    "        color = pos_color\n",
    "    elif c == neg_label:\n",
    "        color = neg_color\n",
    "    return f'<p style=\"color:{color};\">{c}</p>'\n",
    "\n",
    "    \n",
    "def highlight_exp_pred(exp, doc):\n",
    "    ret = ''\n",
    "    abrcount = 0\n",
    "    abrflag = False  # for abbreviation\n",
    "    for e, w in zip(exp, doc[0]):\n",
    "        if e == 1:\n",
    "            ret += f'<span style=\"background-color:tomato; float: left\">{w}&nbsp;</span>'\n",
    "            abrcount = 0\n",
    "            abrflag = False\n",
    "        else:\n",
    "            if abrflag:\n",
    "                continue\n",
    "            abrcount += 1\n",
    "            if abrcount > 4:\n",
    "                abrflag = True\n",
    "                ret += f'<span style=\"float:left\">...&nbsp;</span>'\n",
    "            else:\n",
    "                ret += f'<span style=\"float:left\">{w}&nbsp;</span>'\n",
    "    return ret\n",
    "\n",
    "\n",
    "def postprocess(cls_preds, exp_preds, docs_clean, urls):\n",
    "    cls_strs = [color_cls_pred(c) for c in cls_preds]\n",
    "    evi_strs = [highlight_exp_pred(exp, doc) for exp, doc in zip(exp_preds, docs_clean)]\n",
    "    pred = {\n",
    "        'clses': cls_strs,\n",
    "        'evis': evi_strs,\n",
    "        'links': urls\n",
    "    }        \n",
    "    return pred\n",
    "    \n",
    "\n",
    "\n",
    "@app.route('/prediction/<query>')\n",
    "def prediction(query):\n",
    "    def _predict(exp, cls, queries, docs):\n",
    "        with torch.no_grad():\n",
    "            exp.eval()\n",
    "            aux_preds, exp_preds, att_masks = exp(queries,\n",
    "                                                  [i for i in range(top)], \n",
    "                                                  docs)\n",
    "            hard_exp_preds = torch.round(exp_preds)\n",
    "            queries, docs = mark_evidence(queries, docs, hard_exp_preds)\n",
    "            cls_preds = cls(queries, [i for i in range(top)], docs)\n",
    "            aux_preds = [classes[torch.argmax(p)] for p in aux_preds]\n",
    "            cls_preds = [classes[torch.argmax(p)] for p in cls_preds]\n",
    "            hard_exp_preds = [p[(len(queries[0]) + 2):] for p in hard_exp_preds]\n",
    "            hard_exp_preds = merge_subtoken_exp_preds(hard_exp_preds, tokenized_d_token_slides)\n",
    "        return aux_preds, cls_preds, hard_exp_preds, docs_clean   \n",
    "    \n",
    "    wiki_urls = bing_wiki_search(query)[:top]\n",
    "    orig_docs = [get_wiki_docs(url) for url in wiki_urls]\n",
    "    tokenized_q, tokenized_q_token_slide, tokenized_ds, tokenized_d_token_slides, docs_clean = \\\n",
    "        preprocess(query, orig_docs)\n",
    "    queries = [torch.tensor(tokenized_q[0], dtype=torch.long) for i in range(top)]\n",
    "    docs = [torch.tensor(s, dtype=torch.long) for s in tokenized_ds]\n",
    "    aux_preds, cls_preds, exp_preds, docs_clean = _predict(evi_finder, cls, queries, docs)\n",
    "    pred = postprocess(cls_preds, exp_preds, docs_clean, wiki_urls)\n",
    "    pred['query'] = query\n",
    "    return render_template('predict.html', pred=pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bright-toyota",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app \"__main__\" (lazy loading)\n",
      " * Environment: production\n",
      "\u001b[31m   WARNING: This is a development server. Do not use it in a production deployment.\u001b[0m\n",
      "\u001b[2m   Use a production WSGI server instead.\u001b[0m\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " * Running on http://127.0.0.1:8080/ (Press CTRL+C to quit)\n",
      "127.0.0.1 - - [23/Feb/2021 01:39:58] \"\u001b[37mGET /prediction/microsoft%20is%20a%20chinese%20company HTTP/1.1\u001b[0m\" 200 -\n"
     ]
    }
   ],
   "source": [
    "app.run(host='127.0.0.1', port=8080)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
