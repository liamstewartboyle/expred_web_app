{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "literary-issue",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
<<<<<<< Updated upstream
<<<<<<< Updated upstream
<<<<<<< Updated upstream
<<<<<<< Updated upstream
   "id": "explicit-colleague",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "surprised-singer",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/assassin/anaconda3/envs/pytorch/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/assassin/anaconda3/envs/pytorch/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/assassin/anaconda3/envs/pytorch/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/assassin/anaconda3/envs/pytorch/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/assassin/anaconda3/envs/pytorch/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/assassin/anaconda3/envs/pytorch/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/assassin/anaconda3/envs/pytorch/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/assassin/anaconda3/envs/pytorch/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/assassin/anaconda3/envs/pytorch/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/assassin/anaconda3/envs/pytorch/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/assassin/anaconda3/envs/pytorch/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/assassin/anaconda3/envs/pytorch/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
=======
=======
>>>>>>> Stashed changes
=======
>>>>>>> Stashed changes
=======
>>>>>>> Stashed changes
   "id": "dental-chick",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-625a68e1015a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# from keras.models import load_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# from keras.backend import set_session\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mskimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mresize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# import tensorflow as tf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/keras/lib/python3.7/site-packages/skimage/transform/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m                               \u001b[0mprobabilistic_hough_line\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhough_circle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                               hough_circle_peaks, hough_ellipse)\n\u001b[0;32m----> 5\u001b[0;31m from .radon_transform import (radon, iradon, iradon_sart,\n\u001b[0m\u001b[1;32m      6\u001b[0m                               order_angles_golden_ratio)\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mfinite_radon_transform\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfrt2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mifrt2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/keras/lib/python3.7/site-packages/skimage/transform/radon_transform.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterpolate\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0minterp1d\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstants\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgolden_ratio\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_warps\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwarp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_radon_transform\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msart_projection_update\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shared\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfft\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfftmodule\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/keras/lib/python3.7/site-packages/skimage/transform/_warps.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m                          ProjectiveTransform, _to_ndimage_mode)\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_warps_cy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_warp_fast\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmeasure\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mblock_reduce\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m from .._shared.utils import (get_bound_method_class, safe_as_int, warn,\n",
      "\u001b[0;32m~/anaconda3/envs/keras/lib/python3.7/site-packages/skimage/measure/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0msimple_metrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcompare_mse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompare_nrmse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompare_psnr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_structural_similarity\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcompare_ssim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_polygon\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mapproximate_polygon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubdivide_polygon\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mpnpoly\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpoints_in_poly\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrid_points_in_poly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m from ._moments import (moments, moments_central, moments_coords,\n",
      "\u001b[0;32m~/anaconda3/envs/keras/lib/python3.7/site-packages/skimage/measure/_polygon.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msignal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mapproximate_polygon\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcoords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/keras/lib/python3.7/site-packages/scipy/signal/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    308\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mspectral\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mwavelets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 310\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_peak_finding\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    311\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mwindows\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_window\u001b[0m  \u001b[0;31m# keep this one in signal namespace\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/keras/lib/python3.7/site-packages/scipy/signal/_peak_finding.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwavelets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcwt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mricker\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstats\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mscoreatpercentile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m from ._peak_finding_utils import (\n",
      "\u001b[0;32m~/anaconda3/envs/keras/lib/python3.7/site-packages/scipy/stats/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    389\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    390\u001b[0m \"\"\"\n\u001b[0;32m--> 391\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mstats\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    392\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdistributions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmorestats\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/keras/lib/python3.7/site-packages/scipy/stats/stats.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspecial\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mspecial\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlinalg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdistributions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmstats_basic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m from ._stats_mstats_common import (_find_repeats, linregress, theilslopes,\n",
      "\u001b[0;32m~/anaconda3/envs/keras/lib/python3.7/site-packages/scipy/stats/distributions.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m                                     rv_frozen)\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_continuous_distns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_discrete_distns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/keras/lib/python3.7/site-packages/scipy/stats/_continuous_distns.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspecial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ufuncs\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mscu\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_util\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_lazyselect\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_lazywhere\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_stats\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_rvs_sampling\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrvs_ratio_uniforms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m from ._tukeylambda_stats import (tukeylambda_variance as _tlvar,\n",
      "\u001b[0;32m_stats.pyx\u001b[0m in \u001b[0;36minit scipy.stats._stats\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
<<<<<<< Updated upstream
<<<<<<< Updated upstream
<<<<<<< Updated upstream
>>>>>>> Stashed changes
=======
>>>>>>> Stashed changes
=======
>>>>>>> Stashed changes
     ]
    }
   ],
   "source": [
<<<<<<< Updated upstream
<<<<<<< Updated upstream
<<<<<<< Updated upstream
    "import torch\n",
    "from tokenizer import BertTokenizerWithMapping\n",
    "from models.mlp import BertMTL, BertClassifier\n",
    "from models.params import MTLParams\n",
    "from copy import deepcopy\n",
    "import os \n",
    "from flask import Flask, request, redirect, url_for, render_template\n",
    "from azure_search.bing_utils import bing_wiki_search, get_wiki_docs\n",
    "from itertools import chain\n",
    "import numpy as np \n",
    "import csv\n",
    "import random\n",
    "\n",
    "session_id = hex(int(random.random()*1e13))[2:]\n",
    "\n",
    "ugc_data_fname = f'data/ugc_{session_id}.csv' # user generated content\n",
    "mgc_data_fname = f'data/mgc_{session_id}.csv' # machine genarated content\n",
    "temp_data_fname = f'data/temp_{session_id}.csv'\n",
    "temp_fname = f'data/temp_{session_id}.txt'\n",
    "bert_dir = 'bert-base-uncased'\n",
    "evi_finder_loc = './trained_models/fever/evidence_token_identifier.pt'\n",
    "cls_loc = 'trained_models/fever/evidence_classifier.pt'\n",
    "classes = [\"SUPPORTS\", \"REFUTES\"]\n",
    "device = torch.device('cpu')\n",
    "top = 3\n",
    "max_sentence = 30\n",
    "debug = True\n",
    "debug = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "thermal-drove",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading models\n"
=======
>>>>>>> Stashed changes
     ]
    }
   ],
   "source": [
<<<<<<< Updated upstream
    "if debug:\n",
    "    print('debug, will not load models')\n",
    "else:\n",
    "    print(\"Loading models\")\n",
    "    tokenizer = BertTokenizerWithMapping.from_pretrained(bert_dir)\n",
    "    max_length = 512\n",
    "    use_half_precision = False\n",
    "\n",
    "    mtl_params = MTLParams(dim_cls_linear=256, num_labels=2, dim_exp_gru=128)\n",
    "\n",
    "    evi_finder = BertMTL(bert_dir=bert_dir,\n",
    "                         tokenizer=tokenizer,\n",
    "                         mtl_params=mtl_params,\n",
    "                         use_half_precision=False)\n",
    "    evi_finder.load_state_dict(torch.load(evi_finder_loc, map_location=device))\n",
    "\n",
    "    cls = BertClassifier(bert_dir=bert_dir,\n",
    "                         pad_token_id=tokenizer.pad_token_id,\n",
    "                         cls_token_id=tokenizer.cls_token_id,\n",
    "                         sep_token_id=tokenizer.sep_token_id,\n",
    "                         num_labels=mtl_params.num_labels,\n",
    "                         max_length=max_length,\n",
    "                         mtl_params=mtl_params,\n",
    "                         use_half_precision=False)\n",
    "    cls.load_state_dict(torch.load(cls_loc, map_location=device))"
=======
=======
>>>>>>> Stashed changes
=======
>>>>>>> Stashed changes
=======
>>>>>>> Stashed changes
    "import os \n",
    "from flask import Flask, request, redirect, url_for, render_template\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "\n",
    "import pytorch\n",
    "import numpy as np \n",
    "\n",
    "print(\"Loading model\")\n",
    "global mtl_model\n",
    "global cls_model\n",
    "mtl_model = load_mtl_model('my_mtl_model')\n",
    "cls_modle = load_cls_model('my_cls_model')"
<<<<<<< Updated upstream
<<<<<<< Updated upstream
<<<<<<< Updated upstream
>>>>>>> Stashed changes
=======
>>>>>>> Stashed changes
=======
>>>>>>> Stashed changes
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
<<<<<<< Updated upstream
<<<<<<< Updated upstream
   "execution_count": 5,
   "id": "sized-seminar",
   "metadata": {},
   "outputs": [],
   "source": [
    "app = Flask(__name__)"
=======
>>>>>>> Stashed changes
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": 6,
   "id": "fatty-gender",
=======
   "execution_count": null,
   "id": "bright-toyota",
>>>>>>> Stashed changes
=======
   "execution_count": null,
   "id": "bright-toyota",
>>>>>>> Stashed changes
=======
   "execution_count": null,
   "id": "bright-toyota",
>>>>>>> Stashed changes
=======
   "execution_count": null,
   "id": "bright-toyota",
>>>>>>> Stashed changes
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(query):\n",
    "    query = query.strip().lower()\n",
    "    return query\n",
    "\n",
    "\n",
    "@app.route('/', methods=['GET', 'POST']) \n",
    "def main_page():\n",
    "    if request.method == 'POST':\n",
    "        query = request.form['query']\n",
    "        query = clean(query)\n",
    "        return redirect(url_for('prediction', query=query))\n",
    "    return render_template('index.html')\n",
    "\n",
    "\n",
<<<<<<< Updated upstream
<<<<<<< Updated upstream
<<<<<<< Updated upstream
<<<<<<< Updated upstream
    "def preprocess(query, docs):\n",
    "    query = query.split()\n",
    "    tokenized_q, tokenized_q_token_slides = tokenizer.encode_docs([[query]])\n",
    "    tokenized_q = tokenized_q[0]\n",
    "    tokenized_q_token_slide = tokenized_q_token_slides[0]\n",
    "    docs_clean = [[list(chain.from_iterable([s.split() + ['.'] for s in d.lower().split('.')][:max_sentence]))] \n",
    "                  for d in docs]\n",
    "    docs = deepcopy(docs_clean)\n",
    "    tokenized_docs, tokenized_doc_token_slides = tokenizer.encode_docs(docs)\n",
    "    tokenized_docs = [list(chain.from_iterable(tokenized_docs[i])) for i in range(top)]\n",
    "    tokenized_doc_token_slides = [list(chain.from_iterable(tokenized_doc_token_slides[i]))\n",
    "                                  for i in range(top)]\n",
    "    return tokenized_q, tokenized_q_token_slide,\\\n",
    "           tokenized_docs, tokenized_doc_token_slides,\\\n",
    "           docs_clean\n",
    "\n",
    "\n",
    "def mark_evidence(queries, docs, hard_preds, wildcard='.'):\n",
    "    wildcard_tensor = tokenizer.convert_tokens_to_ids('.') * torch.ones(max_length).type(torch.int)\n",
    "    doc_max_len = max_length - 2 - len(queries[0])\n",
    "    docs = [d[:doc_max_len] if len(d) >= doc_max_len \n",
    "                            else torch.cat([d, torch.zeros(doc_max_len-len(d)).type(torch.int64)])\n",
    "            for d in docs]\n",
    "    new_docs = []\n",
    "    for q, d, e in zip(queries, docs, hard_preds):\n",
    "        temp = torch.cat([torch.zeros(1).type(torch.int64), q, torch.zeros(1).type(torch.int64), d])\n",
    "        temp = e * temp + (1-e) * wildcard_tensor\n",
    "        new_docs.append(temp[(len(queries[0]) + 2):].type(torch.int64))\n",
    "    return queries, new_docs\n",
    "\n",
    "\n",
    "def adapt_exp_pred(exp, doc):\n",
    "    exp = exp[:len(doc[0])] + [0] * (len(doc[0]) - len(exp))\n",
    "    return exp\n",
    "\n",
    "\n",
    "def merge_subtoken_exp_preds(exp_preds, slides):\n",
    "    ret = []\n",
    "    for p, ss in zip(exp_preds, slides):\n",
    "        p = p.tolist()\n",
    "        ret.append([max(p[s[0]:s[1]] + [0]) for s in ss])\n",
    "    return ret\n",
    "\n",
    "\n",
    "def color_cls_pred(c, \n",
    "                   pos_label='SUPPORTS', pos_color='green', \n",
    "                   neg_label='REFUTES', neg_color='red',\n",
    "                   default_color='gray'):\n",
=======
=======
>>>>>>> Stashed changes
=======
>>>>>>> Stashed changes
=======
>>>>>>> Stashed changes
    "def preprocess(query):\n",
    "    pass\n",
    "    return query\n",
    "\n",
    "\n",
    "def color_cls_pred(c, \n",
    "                    pos_label=='SUPPORT', pos_color='green', \n",
    "                    neg_label='REFUTE', neg_color='red',\n",
    "                    default_color == 'gray'):\n",
<<<<<<< Updated upstream
<<<<<<< Updated upstream
<<<<<<< Updated upstream
>>>>>>> Stashed changes
=======
>>>>>>> Stashed changes
=======
>>>>>>> Stashed changes
=======
>>>>>>> Stashed changes
    "    color = default_color\n",
    "    if c == pos_label:\n",
    "        color = pos_color\n",
    "    elif c == neg_label:\n",
    "        color = neg_color\n",
<<<<<<< Updated upstream
<<<<<<< Updated upstream
<<<<<<< Updated upstream
<<<<<<< Updated upstream
    "    return f'<p style=\"color:{color};\">{c}</p>'\n",
    "\n",
    "    \n",
    "def highlight_exp_pred(exp, doc):\n",
    "    ret = ''\n",
    "    abrcount = 0\n",
    "    abrflag = False  # for abbreviation\n",
    "    for e, w in zip(exp, doc[0]):\n",
    "        if e == 1:\n",
    "            ret += f'<span style=\"background-color:tomato; float: left\">{w}&nbsp;</span>'\n",
    "            abrcount = 0\n",
    "            abrflag = False\n",
    "        else:\n",
    "            if abrflag:\n",
    "                continue\n",
    "            abrcount += 1\n",
    "            if abrcount > 4:\n",
    "                abrflag = True\n",
    "                ret += f'<span style=\"float:left\">...&nbsp;</span>'\n",
    "            else:\n",
    "                ret += f'<span style=\"float:left\">{w}&nbsp;</span>'\n",
    "    return ret\n",
    "\n",
    "\n",
    "def postprocess(cls_preds, exp_preds, docs_clean, urls):\n",
    "    cls_strs = [color_cls_pred(c) for c in cls_preds]\n",
    "    evi_strs = [highlight_exp_pred(exp, doc) for exp, doc in zip(exp_preds, docs_clean)]\n",
    "    pred = {\n",
    "        'clses': cls_strs,\n",
    "        'evis': evi_strs,\n",
    "        'links': urls\n",
=======
=======
>>>>>>> Stashed changes
=======
>>>>>>> Stashed changes
=======
>>>>>>> Stashed changes
    "    return = f'<p style=\"color:{color};\">{c}</p>'\n",
    "\n",
    "    \n",
    "def higglight_exp_pred(exp):\n",
    "    pass\n",
    "    return(str(exp))\n",
    "\n",
    "\n",
    "def postprocess(cls_preds, exp_preds):\n",
    "    cls_strs = [color_cls_pred(c) for c in cls_preds]\n",
    "    evi_strs = [highlight_exp_pred(exp) for exp in exp_preds]\n",
    "    pred = {\n",
    "        'cls0': cls_strs[0],\n",
    "        'cls1': cls_strs[1],\n",
    "        'cls2': cls_strs[2],\n",
    "        'evi0': evi_strs[0],\n",
    "        'evi1': evi_strs[1],\n",
    "        'evi2': evi_strs[2]\n",
<<<<<<< Updated upstream
<<<<<<< Updated upstream
<<<<<<< Updated upstream
>>>>>>> Stashed changes
=======
>>>>>>> Stashed changes
=======
>>>>>>> Stashed changes
=======
>>>>>>> Stashed changes
    "    }        \n",
    "    return pred\n",
    "    \n",
    "\n",
<<<<<<< Updated upstream
<<<<<<< Updated upstream
<<<<<<< Updated upstream
<<<<<<< Updated upstream
    "def get_bogus_pred():\n",
    "    pred = {\n",
    "        'clses': ['REFUTE' for i in range(3)],\n",
    "        'evis': [[1 for j in range(3)] for i in range(3)],\n",
    "        'links': ['www.abs.com' for i in range(3)],\n",
    "        'query': 'this is not a query'\n",
    "    }\n",
    "    return pred\n",
    "\n",
    "\n",
    "if not os.path.isfile(ugc_data_fname):\n",
    "    with open(ugc_data_fname, 'w+', newline='') as fout:\n",
    "        writer = csv.writer(fout)\n",
    "        writer.writerow('query url evidence label'.split())\n",
    "    \n",
    "if not os.path.isfile(mgc_data_fname):\n",
    "    with open(mgc_data_fname, 'w+', newline='') as fout:\n",
    "        writer = csv.writer(fout)\n",
    "        writer.writerow('query url evidence label'.split())\n",
    "        \n",
    "        \n",
    "def dump_quel(fname, query, urls, docs, exps, labels, mode='a+'):\n",
    "    with open(fname, mode, newline='') as fout:\n",
    "        for url, doc, exp, label in zip(urls, docs, exps, labels):\n",
    "#             print(doc, exp)\n",
    "            assert len(doc[0]) == len(exp)\n",
    "            writer = csv.writer(fout)\n",
    "            writer.writerow([query, url, list(zip(doc[0], exp)), label])\n",
    "            \n",
    "\n",
    "def restore_from_temp(temp_fname, idxs=None):\n",
    "    query = None\n",
    "    urls, docs, exps, labels = [], [], [], []\n",
    "    with open(temp_fname, 'r', newline='') as fin:\n",
    "        reader = csv.reader(fin)\n",
    "        for idx, (query, url, evidence, label) in enumerate(reader):\n",
    "            if idxs is None or idx in idxs:\n",
    "                evidence = eval(evidence)\n",
    "                doc, exp = zip(*evidence)\n",
    "                urls.append(url)\n",
    "                docs.append([doc])\n",
    "                exps.append(exp)\n",
    "                labels.append(label)\n",
    "    return query, urls, docs, exps, labels\n",
    "\n",
    "\n",
    "@app.route('/select', methods=['GET', 'POST'])\n",
    "def select():\n",
    "    with open(temp_fname, 'r') as fin:\n",
    "        disagree_idxs = eval(str(fin.read()))\n",
    "    query, urls, docs, exps, labels = restore_from_temp(temp_data_fname, idxs=disagree_idxs)\n",
    "    def _get_ugc(docs, form):\n",
    "        labels = []\n",
    "#         print([i for i in form.keys()])\n",
    "        evis = [[0 for w in doc[0]] for doc in docs]\n",
    "        bad_docs = []            \n",
    "        for i in range(len(evis)):\n",
    "            labels.append(form[f\"cls{i}\"])\n",
    "            if labels[-1] == 'BAD_DOC':\n",
    "                continue\n",
    "#             print([form.get(f'exp{i},{j}') for j in range(3)])\n",
    "#             print(len(docs[i]))\n",
    "            for j in range(len(docs[i][0])):\n",
    "#                 print(evis, labels)\n",
    "                if form.get(f'exp{i},{j}') is not None:\n",
    "                    evis[i][j] = 1\n",
    "#         print(evis, labels)    \n",
    "        return evis, labels       \n",
    "    if request.method == 'POST':\n",
    "        evis, labels = _get_ugc(docs, request.form)\n",
    "        dump_quel(ugc_data_fname, query, urls, docs, evis, labels)\n",
    "        return render_template('thank_contrib.html')\n",
    "#     print(docs)\n",
    "    return render_template('select.html', query=query, docs=docs, \n",
    "                           urls=urls, exps=exps, labels=labels)\n",
    "\n",
    "                                        \n",
    "@app.route('/prediction/<query>', methods=['GET', 'POST'])\n",
    "def prediction(query):\n",
    "    if request.method == 'POST':\n",
    "#         print(request.form['satisfy'])\n",
    "        query, urls, docs, exps, labels = restore_from_temp(temp_data_fname)\n",
    "        disagree_idx = []\n",
    "        for i in range(top):\n",
    "            if request.form[f'agree{i}'] == 'y':\n",
    "                dump_quel(mgc_data_fname, query[i:i+1], urls[i:i+1],\n",
    "                          docs[i:i+1], exps[i:i+1], labels[i:i+1])\n",
    "            else:\n",
    "                disagree_idx.append(i)\n",
    "        if len(disagree_idx) == 0:\n",
    "            return render_template('thankyou.html')\n",
    "        else:\n",
    "            with open(temp_fname, 'w+') as fout:\n",
    "                fout.write(str(disagree_idx))\n",
    "            return redirect(url_for('select'))\n",
    "#             with open(mgc_data_fname, 'a+') as fout:\n",
    "#                 for url, doc, exp, label in zip(urls, docs, exps, labels):\n",
    "#                     fout.write(f\"{query}, {url}, {list(zip(doc[0], exp))}, {label}\\n\")\n",
    "#             return render_template('thankyou.html')\n",
    "#         if request.form['satisfy'] == 'No...':\n",
    "# #             print(url_for('select', query, docs))\n",
    "#             return redirect(url_for('select'))\n",
    "    if debug:\n",
    "        pred = get_bogus_pred()\n",
    "        query = pred['query']\n",
    "        docs_clean = [[['a' for a in range(3)]] for b in range(3) ]\n",
    "        exp_preds = [[1]*3 for i in range(3)]\n",
    "        cls_preds = ['REFUTES' for i in range(3)]\n",
    "        wiki_urls = pred['links']\n",
    "    else:\n",
    "        def _predict(exp, cls, queries, docs):\n",
    "            with torch.no_grad():\n",
    "                exp.eval()\n",
    "\n",
    "                aux_preds, exp_preds, att_masks = exp(queries, [i for i in range(top)], docs)\n",
    "\n",
    "                hard_exp_preds = torch.round(exp_preds)\n",
    "                queries, docs = mark_evidence(queries, docs, hard_exp_preds)\n",
    "\n",
    "                cls_preds = cls(queries, [i for i in range(top)], docs)\n",
    "\n",
    "                aux_preds = [classes[torch.argmax(p)] for p in aux_preds]\n",
    "                cls_preds = [classes[torch.argmax(p)] for p in cls_preds]\n",
    "                hard_exp_preds = [p[(len(queries[0]) + 2):] for p in hard_exp_preds]\n",
    "                hard_exp_preds = merge_subtoken_exp_preds(hard_exp_preds, tokenized_d_token_slides)\n",
    "            return aux_preds, cls_preds, hard_exp_preds, docs_clean\n",
    "\n",
    "        wiki_urls = bing_wiki_search(query)[:top]\n",
    "        orig_docs = [get_wiki_docs(url) for url in wiki_urls]\n",
    "        tokenized_q, tokenized_q_token_slide, tokenized_ds, tokenized_d_token_slides, docs_clean = \\\n",
    "            preprocess(query, orig_docs)\n",
    "        queries = [torch.tensor(tokenized_q[0], dtype=torch.long) for i in range(top)]\n",
    "        docs = [torch.tensor(s, dtype=torch.long) for s in tokenized_ds]\n",
    "        aux_preds, cls_preds, exp_preds, docs_clean = _predict(evi_finder, cls, queries, docs)\n",
    "        exp_preds = [adapt_exp_pred(exp, doc) for exp, doc in zip(exp_preds, docs_clean)]\n",
    "        pred = postprocess(cls_preds, exp_preds, docs_clean, wiki_urls)\n",
    "        pred['query'] = query\n",
    "    dump_quel(temp_data_fname, query, wiki_urls, docs_clean, exp_preds, cls_preds, 'w+')\n",
    "    return render_template('predict.html', pred=pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bright-toyota",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app \"__main__\" (lazy loading)\n",
      " * Environment: production\n",
      "\u001b[31m   WARNING: This is a development server. Do not use it in a production deployment.\u001b[0m\n",
      "\u001b[2m   Use a production WSGI server instead.\u001b[0m\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " * Running on http://127.0.0.1:8080/ (Press CTRL+C to quit)\n",
      "[2021-03-01 20:06:20,829] ERROR in app: Exception on /select [GET]\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/assassin/anaconda3/envs/pytorch/lib/python3.7/site-packages/flask/app.py\", line 2447, in wsgi_app\n",
      "    response = self.full_dispatch_request()\n",
      "  File \"/home/assassin/anaconda3/envs/pytorch/lib/python3.7/site-packages/flask/app.py\", line 1952, in full_dispatch_request\n",
      "    rv = self.handle_user_exception(e)\n",
      "  File \"/home/assassin/anaconda3/envs/pytorch/lib/python3.7/site-packages/flask/app.py\", line 1821, in handle_user_exception\n",
      "    reraise(exc_type, exc_value, tb)\n",
      "  File \"/home/assassin/anaconda3/envs/pytorch/lib/python3.7/site-packages/flask/_compat.py\", line 39, in reraise\n",
      "    raise value\n",
      "  File \"/home/assassin/anaconda3/envs/pytorch/lib/python3.7/site-packages/flask/app.py\", line 1950, in full_dispatch_request\n",
      "    rv = self.dispatch_request()\n",
      "  File \"/home/assassin/anaconda3/envs/pytorch/lib/python3.7/site-packages/flask/app.py\", line 1936, in dispatch_request\n",
      "    return self.view_functions[rule.endpoint](**req.view_args)\n",
      "  File \"<ipython-input-6-072da98c6021>\", line 151, in select\n",
      "    with open(temp_fname, 'r') as fin:\n",
      "FileNotFoundError: [Errno 2] No such file or directory: 'data/temp_48990fd4ce6.txt'\n",
      "127.0.0.1 - - [01/Mar/2021 20:06:20] \"\u001b[35m\u001b[1mGET /select HTTP/1.1\u001b[0m\" 500 -\n",
      "[2021-03-01 20:06:23,279] ERROR in app: Exception on /select [GET]\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/assassin/anaconda3/envs/pytorch/lib/python3.7/site-packages/flask/app.py\", line 2447, in wsgi_app\n",
      "    response = self.full_dispatch_request()\n",
      "  File \"/home/assassin/anaconda3/envs/pytorch/lib/python3.7/site-packages/flask/app.py\", line 1952, in full_dispatch_request\n",
      "    rv = self.handle_user_exception(e)\n",
      "  File \"/home/assassin/anaconda3/envs/pytorch/lib/python3.7/site-packages/flask/app.py\", line 1821, in handle_user_exception\n",
      "    reraise(exc_type, exc_value, tb)\n",
      "  File \"/home/assassin/anaconda3/envs/pytorch/lib/python3.7/site-packages/flask/_compat.py\", line 39, in reraise\n",
      "    raise value\n",
      "  File \"/home/assassin/anaconda3/envs/pytorch/lib/python3.7/site-packages/flask/app.py\", line 1950, in full_dispatch_request\n",
      "    rv = self.dispatch_request()\n",
      "  File \"/home/assassin/anaconda3/envs/pytorch/lib/python3.7/site-packages/flask/app.py\", line 1936, in dispatch_request\n",
      "    return self.view_functions[rule.endpoint](**req.view_args)\n",
      "  File \"<ipython-input-6-072da98c6021>\", line 151, in select\n",
      "    with open(temp_fname, 'r') as fin:\n",
      "FileNotFoundError: [Errno 2] No such file or directory: 'data/temp_48990fd4ce6.txt'\n",
      "127.0.0.1 - - [01/Mar/2021 20:06:23] \"\u001b[35m\u001b[1mGET /select HTTP/1.1\u001b[0m\" 500 -\n",
      "127.0.0.1 - - [01/Mar/2021 20:06:43] \"\u001b[37mGET /prediction/microsoft%20is%20a%20chinese%20company HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [01/Mar/2021 20:07:01] \"\u001b[37mGET /prediction/microsoft%20is%20a%20chinese%20company HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [01/Mar/2021 20:07:16] \"\u001b[32mPOST /prediction/microsoft%20is%20a%20chinese%20company HTTP/1.1\u001b[0m\" 302 -\n",
      "127.0.0.1 - - [01/Mar/2021 20:07:16] \"\u001b[37mGET /select HTTP/1.1\u001b[0m\" 200 -\n"
     ]
    }
   ],
   "source": [
=======
=======
>>>>>>> Stashed changes
=======
>>>>>>> Stashed changes
=======
>>>>>>> Stashed changes
    "\n",
    "@app.route('/prediction/<query>')\n",
    "def predict(query):\n",
    "    \n",
    "    def _predict(model, query_input):\n",
    "        pass\n",
    "        return None, None\n",
    "    \n",
    "    query_input = preprocess(query)\n",
    "    \n",
    "    cls_pred, exp_pred = _predict(model, query_input)\n",
    "    \n",
    "    pred = postprocess(cls_pred, exp_pred)\n",
    "    pred['query'] = query\n",
    "    return render_template('predict.html', pred=pred)\n",
    "\n",
    "\n",
<<<<<<< Updated upstream
<<<<<<< Updated upstream
<<<<<<< Updated upstream
>>>>>>> Stashed changes
=======
>>>>>>> Stashed changes
=======
>>>>>>> Stashed changes
=======
>>>>>>> Stashed changes
    "app.run(host='127.0.0.1', port=8080)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
