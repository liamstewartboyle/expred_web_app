2017-12-03 07:18:57,502 - Starting Flask Service......
2017-12-03 07:32:46,685 - CombinedMultiDict([ImmutableMultiDict([('q', '{}'), ('1512304366576', '')]), ImmutableMultiDict([])])
2017-12-03 07:32:46,686 - {}
2017-12-03 07:32:51,596 - {'content_title': "Google's AI Wins Pivotal Second Game in Match With Go Grandmaster", 'content_text': 'After more than four hours of tight play and a rapid-fire endgame, Google\'s artificially intelligent Go-playing computer system has won a second contest against grandmaster Lee Sedol, taking a two-games-to-none lead in their historic best-of-five match in downtown Seoul.  The surprisingly skillful Google machine, known as AlphaGo, now needs only one more win to claim victory in the match. The Korean-born Lee Sedol will go down in defeat unless he takes each of the match\'s last three games. Though machines have beaten the best humans at chess, checkers, Othello, Scrabble, Jeopardy!, and so many other games considered tests of human intellect, they have never beaten the very best at Go. Game Three is set for Saturday afternoon inside Seoul\'s Four Seasons hotel.  The match is a way of judging the suddenly rapid progress of artificial intelligence. One of the machine-learning techniques at the heart of AlphaGo has already reinvented myriad online services inside Google and other big-name Internet companies, helping to identify images, recognize commands spoken into smartphones, improve search engine results, and more. Meanwhile, another AlphaGo technique is now driving experimental robotics at Google and places like the University of California at Berkeley. This week\'s match can show how far these technologies have come - and perhaps how far they will go.  Created in Asia over 2,500 year ago, Go is exponentially more complex than chess, and at least among humans, it requires an added degree of intuition. Lee Sedol is widely-regarded as the top Go player of the last decade, after winning more international titles than all but one other player. He is currently ranked number five in the world, and according to Demis Hassabis, who leads DeepMind, the Google AI lab that created AlphaGo, his team chose the Korean for this all-important match because they wanted an opponent who would be remembered as one of history\'s great players.  Although AlphaGo topped Lee Sedol in the match\'s first game on Wednesday afternoon, the outcome of Game Two was no easier to predict. In his 1996 match with IBM\'s Deep Blue supercomputer, world chess champion Gary Kasparov lost the first game but then came back to win the second game and, eventually, the match as a whole. It wasn\'t until the following year that Deep Blue topped Kasparov over the course of a six-game contest. The thing to realize is that, after playing AlphaGo for the first time on Wednesday, Lee Sedol could adjust his style of play - just as Kasparov did back in 1996. But AlphaGo could not. Because this Google creation relies so heavily on machine learning techniques, the DeepMind team needs a good four to six weeks to train a new incarnation of the system. And that means they can\'t really change things during this eight-day match.  "This is about teaching and learning," Hassabis told us just before Game Two. "One game is not enough data to learn from - for a machine - and training takes an awful lot of time.'}
2017-12-03 07:34:30,099 - CombinedMultiDict([ImmutableMultiDict([('q', '{}'), ('1512304469986', '')]), ImmutableMultiDict([])])
2017-12-03 07:34:30,099 - {}
2017-12-03 07:34:34,571 - {'content_title': "Google's AI Wins Pivotal Second Game in Match With Go Grandmaster", 'content_text': 'After more than four hours of tight play and a rapid-fire endgame, Google\'s artificially intelligent Go-playing computer system has won a second contest against grandmaster Lee Sedol, taking a two-games-to-none lead in their historic best-of-five match in downtown Seoul.  The surprisingly skillful Google machine, known as AlphaGo, now needs only one more win to claim victory in the match. The Korean-born Lee Sedol will go down in defeat unless he takes each of the match\'s last three games. Though machines have beaten the best humans at chess, checkers, Othello, Scrabble, Jeopardy!, and so many other games considered tests of human intellect, they have never beaten the very best at Go. Game Three is set for Saturday afternoon inside Seoul\'s Four Seasons hotel.  The match is a way of judging the suddenly rapid progress of artificial intelligence. One of the machine-learning techniques at the heart of AlphaGo has already reinvented myriad online services inside Google and other big-name Internet companies, helping to identify images, recognize commands spoken into smartphones, improve search engine results, and more. Meanwhile, another AlphaGo technique is now driving experimental robotics at Google and places like the University of California at Berkeley. This week\'s match can show how far these technologies have come - and perhaps how far they will go.  Created in Asia over 2,500 year ago, Go is exponentially more complex than chess, and at least among humans, it requires an added degree of intuition. Lee Sedol is widely-regarded as the top Go player of the last decade, after winning more international titles than all but one other player. He is currently ranked number five in the world, and according to Demis Hassabis, who leads DeepMind, the Google AI lab that created AlphaGo, his team chose the Korean for this all-important match because they wanted an opponent who would be remembered as one of history\'s great players.  Although AlphaGo topped Lee Sedol in the match\'s first game on Wednesday afternoon, the outcome of Game Two was no easier to predict. In his 1996 match with IBM\'s Deep Blue supercomputer, world chess champion Gary Kasparov lost the first game but then came back to win the second game and, eventually, the match as a whole. It wasn\'t until the following year that Deep Blue topped Kasparov over the course of a six-game contest. The thing to realize is that, after playing AlphaGo for the first time on Wednesday, Lee Sedol could adjust his style of play - just as Kasparov did back in 1996. But AlphaGo could not. Because this Google creation relies so heavily on machine learning techniques, the DeepMind team needs a good four to six weeks to train a new incarnation of the system. And that means they can\'t really change things during this eight-day match.  "This is about teaching and learning," Hassabis told us just before Game Two. "One game is not enough data to learn from - for a machine - and training takes an awful lot of time.'}
2017-12-03 07:36:38,411 - Starting Flask Service......
2017-12-03 07:36:44,240 - CombinedMultiDict([ImmutableMultiDict([('q', '{}'), ('1512304604129', '')]), ImmutableMultiDict([])])
2017-12-03 07:36:44,240 - {}
2017-12-03 07:36:47,712 - {'content_title': "Google's AI Wins Pivotal Second Game in Match With Go Grandmaster", 'content_text': 'After more than four hours of tight play and a rapid-fire endgame, Google\'s artificially intelligent Go-playing computer system has won a second contest against grandmaster Lee Sedol, taking a two-games-to-none lead in their historic best-of-five match in downtown Seoul.  The surprisingly skillful Google machine, known as AlphaGo, now needs only one more win to claim victory in the match. The Korean-born Lee Sedol will go down in defeat unless he takes each of the match\'s last three games. Though machines have beaten the best humans at chess, checkers, Othello, Scrabble, Jeopardy!, and so many other games considered tests of human intellect, they have never beaten the very best at Go. Game Three is set for Saturday afternoon inside Seoul\'s Four Seasons hotel.  The match is a way of judging the suddenly rapid progress of artificial intelligence. One of the machine-learning techniques at the heart of AlphaGo has already reinvented myriad online services inside Google and other big-name Internet companies, helping to identify images, recognize commands spoken into smartphones, improve search engine results, and more. Meanwhile, another AlphaGo technique is now driving experimental robotics at Google and places like the University of California at Berkeley. This week\'s match can show how far these technologies have come - and perhaps how far they will go.  Created in Asia over 2,500 year ago, Go is exponentially more complex than chess, and at least among humans, it requires an added degree of intuition. Lee Sedol is widely-regarded as the top Go player of the last decade, after winning more international titles than all but one other player. He is currently ranked number five in the world, and according to Demis Hassabis, who leads DeepMind, the Google AI lab that created AlphaGo, his team chose the Korean for this all-important match because they wanted an opponent who would be remembered as one of history\'s great players.  Although AlphaGo topped Lee Sedol in the match\'s first game on Wednesday afternoon, the outcome of Game Two was no easier to predict. In his 1996 match with IBM\'s Deep Blue supercomputer, world chess champion Gary Kasparov lost the first game but then came back to win the second game and, eventually, the match as a whole. It wasn\'t until the following year that Deep Blue topped Kasparov over the course of a six-game contest. The thing to realize is that, after playing AlphaGo for the first time on Wednesday, Lee Sedol could adjust his style of play - just as Kasparov did back in 1996. But AlphaGo could not. Because this Google creation relies so heavily on machine learning techniques, the DeepMind team needs a good four to six weeks to train a new incarnation of the system. And that means they can\'t really change things during this eight-day match.  "This is about teaching and learning," Hassabis told us just before Game Two. "One game is not enough data to learn from - for a machine - and training takes an awful lot of time.'}
2017-12-03 07:37:21,869 - CombinedMultiDict([ImmutableMultiDict([]), ImmutableMultiDict([])])
2017-12-03 07:37:21,875 - None
2017-12-03 07:37:30,003 - CombinedMultiDict([ImmutableMultiDict([]), ImmutableMultiDict([])])
2017-12-03 07:37:30,004 - None
2017-12-03 07:39:03,513 - Starting Flask Service......
2017-12-03 07:39:09,511 - 5a22fa6ae3e52a40f793f587&1512304607716
2017-12-03 07:39:09,511 - TextSummarizer ------------------------------------ Init
2017-12-03 07:39:09,574 - CombinedMultiDict([ImmutableMultiDict([]), ImmutableMultiDict([])])
2017-12-03 07:39:09,574 - None
2017-12-03 07:41:18,154 - Starting Flask Service......
2017-12-03 07:41:24,326 - CombinedMultiDict([ImmutableMultiDict([]), ImmutableMultiDict([])])
2017-12-03 07:41:24,327 - None
2017-12-03 07:41:24,376 - CombinedMultiDict([ImmutableMultiDict([]), ImmutableMultiDict([])])
2017-12-03 07:41:24,376 - None
2017-12-03 07:41:49,751 - CombinedMultiDict([ImmutableMultiDict([]), ImmutableMultiDict([])])
2017-12-03 07:41:49,752 - None
2017-12-03 07:42:02,237 - Starting Flask Service......
2017-12-03 07:42:08,044 - CombinedMultiDict([ImmutableMultiDict([]), ImmutableMultiDict([])])
2017-12-03 07:42:08,044 - None
2017-12-03 07:42:10,874 - CombinedMultiDict([ImmutableMultiDict([]), ImmutableMultiDict([])])
2017-12-03 07:42:10,874 - None
2017-12-03 07:42:14,969 - CombinedMultiDict([ImmutableMultiDict([]), ImmutableMultiDict([])])
2017-12-03 07:42:14,969 - None
2017-12-03 07:42:17,737 - CombinedMultiDict([ImmutableMultiDict([]), ImmutableMultiDict([])])
2017-12-03 07:42:17,737 - None
2017-12-03 07:42:33,034 - CombinedMultiDict([ImmutableMultiDict([]), ImmutableMultiDict([])])
2017-12-03 07:42:33,034 - None
2017-12-03 07:43:00,841 - CombinedMultiDict([ImmutableMultiDict([('q', '{}'), ('1512304980731', '')]), ImmutableMultiDict([])])
2017-12-03 07:43:00,841 - {}
2017-12-03 07:43:04,673 - {'content_title': "Google's AI Wins Pivotal Second Game in Match With Go Grandmaster", 'content_text': 'After more than four hours of tight play and a rapid-fire endgame, Google\'s artificially intelligent Go-playing computer system has won a second contest against grandmaster Lee Sedol, taking a two-games-to-none lead in their historic best-of-five match in downtown Seoul.  The surprisingly skillful Google machine, known as AlphaGo, now needs only one more win to claim victory in the match. The Korean-born Lee Sedol will go down in defeat unless he takes each of the match\'s last three games. Though machines have beaten the best humans at chess, checkers, Othello, Scrabble, Jeopardy!, and so many other games considered tests of human intellect, they have never beaten the very best at Go. Game Three is set for Saturday afternoon inside Seoul\'s Four Seasons hotel.  The match is a way of judging the suddenly rapid progress of artificial intelligence. One of the machine-learning techniques at the heart of AlphaGo has already reinvented myriad online services inside Google and other big-name Internet companies, helping to identify images, recognize commands spoken into smartphones, improve search engine results, and more. Meanwhile, another AlphaGo technique is now driving experimental robotics at Google and places like the University of California at Berkeley. This week\'s match can show how far these technologies have come - and perhaps how far they will go.  Created in Asia over 2,500 year ago, Go is exponentially more complex than chess, and at least among humans, it requires an added degree of intuition. Lee Sedol is widely-regarded as the top Go player of the last decade, after winning more international titles than all but one other player. He is currently ranked number five in the world, and according to Demis Hassabis, who leads DeepMind, the Google AI lab that created AlphaGo, his team chose the Korean for this all-important match because they wanted an opponent who would be remembered as one of history\'s great players.  Although AlphaGo topped Lee Sedol in the match\'s first game on Wednesday afternoon, the outcome of Game Two was no easier to predict. In his 1996 match with IBM\'s Deep Blue supercomputer, world chess champion Gary Kasparov lost the first game but then came back to win the second game and, eventually, the match as a whole. It wasn\'t until the following year that Deep Blue topped Kasparov over the course of a six-game contest. The thing to realize is that, after playing AlphaGo for the first time on Wednesday, Lee Sedol could adjust his style of play - just as Kasparov did back in 1996. But AlphaGo could not. Because this Google creation relies so heavily on machine learning techniques, the DeepMind team needs a good four to six weeks to train a new incarnation of the system. And that means they can\'t really change things during this eight-day match.  "This is about teaching and learning," Hassabis told us just before Game Two. "One game is not enough data to learn from - for a machine - and training takes an awful lot of time.'}
2017-12-03 07:43:24,314 - CombinedMultiDict([ImmutableMultiDict([]), ImmutableMultiDict([])])
2017-12-03 07:43:24,314 - None
2017-12-03 07:43:31,703 - CombinedMultiDict([ImmutableMultiDict([]), ImmutableMultiDict([])])
2017-12-03 07:43:31,704 - None
2017-12-03 07:43:44,593 - CombinedMultiDict([ImmutableMultiDict([('q', '{}'), ('1512305024481', '')]), ImmutableMultiDict([])])
2017-12-03 07:43:44,593 - {}
2017-12-03 07:43:47,898 - {'content_title': "Google's AI Wins Pivotal Second Game in Match With Go Grandmaster", 'content_text': 'After more than four hours of tight play and a rapid-fire endgame, Google\'s artificially intelligent Go-playing computer system has won a second contest against grandmaster Lee Sedol, taking a two-games-to-none lead in their historic best-of-five match in downtown Seoul.  The surprisingly skillful Google machine, known as AlphaGo, now needs only one more win to claim victory in the match. The Korean-born Lee Sedol will go down in defeat unless he takes each of the match\'s last three games. Though machines have beaten the best humans at chess, checkers, Othello, Scrabble, Jeopardy!, and so many other games considered tests of human intellect, they have never beaten the very best at Go. Game Three is set for Saturday afternoon inside Seoul\'s Four Seasons hotel.  The match is a way of judging the suddenly rapid progress of artificial intelligence. One of the machine-learning techniques at the heart of AlphaGo has already reinvented myriad online services inside Google and other big-name Internet companies, helping to identify images, recognize commands spoken into smartphones, improve search engine results, and more. Meanwhile, another AlphaGo technique is now driving experimental robotics at Google and places like the University of California at Berkeley. This week\'s match can show how far these technologies have come - and perhaps how far they will go.  Created in Asia over 2,500 year ago, Go is exponentially more complex than chess, and at least among humans, it requires an added degree of intuition. Lee Sedol is widely-regarded as the top Go player of the last decade, after winning more international titles than all but one other player. He is currently ranked number five in the world, and according to Demis Hassabis, who leads DeepMind, the Google AI lab that created AlphaGo, his team chose the Korean for this all-important match because they wanted an opponent who would be remembered as one of history\'s great players.  Although AlphaGo topped Lee Sedol in the match\'s first game on Wednesday afternoon, the outcome of Game Two was no easier to predict. In his 1996 match with IBM\'s Deep Blue supercomputer, world chess champion Gary Kasparov lost the first game but then came back to win the second game and, eventually, the match as a whole. It wasn\'t until the following year that Deep Blue topped Kasparov over the course of a six-game contest. The thing to realize is that, after playing AlphaGo for the first time on Wednesday, Lee Sedol could adjust his style of play - just as Kasparov did back in 1996. But AlphaGo could not. Because this Google creation relies so heavily on machine learning techniques, the DeepMind team needs a good four to six weeks to train a new incarnation of the system. And that means they can\'t really change things during this eight-day match.  "This is about teaching and learning," Hassabis told us just before Game Two. "One game is not enough data to learn from - for a machine - and training takes an awful lot of time.'}
2017-12-03 07:44:09,175 - Starting Flask Service......
2017-12-03 07:51:19,531 - CombinedMultiDict([ImmutableMultiDict([('q', '{}'), ('1512305479415', '')]), ImmutableMultiDict([])])
2017-12-03 07:51:19,531 - {}
2017-12-03 07:51:23,038 - {'content_title': "Google's AI Wins Pivotal Second Game in Match With Go Grandmaster", 'content_text': 'After more than four hours of tight play and a rapid-fire endgame, Google\'s artificially intelligent Go-playing computer system has won a second contest against grandmaster Lee Sedol, taking a two-games-to-none lead in their historic best-of-five match in downtown Seoul.  The surprisingly skillful Google machine, known as AlphaGo, now needs only one more win to claim victory in the match. The Korean-born Lee Sedol will go down in defeat unless he takes each of the match\'s last three games. Though machines have beaten the best humans at chess, checkers, Othello, Scrabble, Jeopardy!, and so many other games considered tests of human intellect, they have never beaten the very best at Go. Game Three is set for Saturday afternoon inside Seoul\'s Four Seasons hotel.  The match is a way of judging the suddenly rapid progress of artificial intelligence. One of the machine-learning techniques at the heart of AlphaGo has already reinvented myriad online services inside Google and other big-name Internet companies, helping to identify images, recognize commands spoken into smartphones, improve search engine results, and more. Meanwhile, another AlphaGo technique is now driving experimental robotics at Google and places like the University of California at Berkeley. This week\'s match can show how far these technologies have come - and perhaps how far they will go.  Created in Asia over 2,500 year ago, Go is exponentially more complex than chess, and at least among humans, it requires an added degree of intuition. Lee Sedol is widely-regarded as the top Go player of the last decade, after winning more international titles than all but one other player. He is currently ranked number five in the world, and according to Demis Hassabis, who leads DeepMind, the Google AI lab that created AlphaGo, his team chose the Korean for this all-important match because they wanted an opponent who would be remembered as one of history\'s great players.  Although AlphaGo topped Lee Sedol in the match\'s first game on Wednesday afternoon, the outcome of Game Two was no easier to predict. In his 1996 match with IBM\'s Deep Blue supercomputer, world chess champion Gary Kasparov lost the first game but then came back to win the second game and, eventually, the match as a whole. It wasn\'t until the following year that Deep Blue topped Kasparov over the course of a six-game contest. The thing to realize is that, after playing AlphaGo for the first time on Wednesday, Lee Sedol could adjust his style of play - just as Kasparov did back in 1996. But AlphaGo could not. Because this Google creation relies so heavily on machine learning techniques, the DeepMind team needs a good four to six weeks to train a new incarnation of the system. And that means they can\'t really change things during this eight-day match.  "This is about teaching and learning," Hassabis told us just before Game Two. "One game is not enough data to learn from - for a machine - and training takes an awful lot of time.'}
2017-12-03 07:51:23,147 - 5a22fa6ae3e52a40f793f587
2017-12-03 07:51:23,147 - TextSummarizer ------------------------------------ Init
2017-12-03 07:52:38,147 - Starting Flask Service......
2017-12-03 07:52:43,227 - CombinedMultiDict([ImmutableMultiDict([('q', '{}'), ('1512305560989', '')]), ImmutableMultiDict([])])
2017-12-03 07:52:43,227 - {}
2017-12-03 07:52:46,047 - {'content_title': "Google's AI Wins Pivotal Second Game in Match With Go Grandmaster", 'content_text': 'After more than four hours of tight play and a rapid-fire endgame, Google\'s artificially intelligent Go-playing computer system has won a second contest against grandmaster Lee Sedol, taking a two-games-to-none lead in their historic best-of-five match in downtown Seoul.  The surprisingly skillful Google machine, known as AlphaGo, now needs only one more win to claim victory in the match. The Korean-born Lee Sedol will go down in defeat unless he takes each of the match\'s last three games. Though machines have beaten the best humans at chess, checkers, Othello, Scrabble, Jeopardy!, and so many other games considered tests of human intellect, they have never beaten the very best at Go. Game Three is set for Saturday afternoon inside Seoul\'s Four Seasons hotel.  The match is a way of judging the suddenly rapid progress of artificial intelligence. One of the machine-learning techniques at the heart of AlphaGo has already reinvented myriad online services inside Google and other big-name Internet companies, helping to identify images, recognize commands spoken into smartphones, improve search engine results, and more. Meanwhile, another AlphaGo technique is now driving experimental robotics at Google and places like the University of California at Berkeley. This week\'s match can show how far these technologies have come - and perhaps how far they will go.  Created in Asia over 2,500 year ago, Go is exponentially more complex than chess, and at least among humans, it requires an added degree of intuition. Lee Sedol is widely-regarded as the top Go player of the last decade, after winning more international titles than all but one other player. He is currently ranked number five in the world, and according to Demis Hassabis, who leads DeepMind, the Google AI lab that created AlphaGo, his team chose the Korean for this all-important match because they wanted an opponent who would be remembered as one of history\'s great players.  Although AlphaGo topped Lee Sedol in the match\'s first game on Wednesday afternoon, the outcome of Game Two was no easier to predict. In his 1996 match with IBM\'s Deep Blue supercomputer, world chess champion Gary Kasparov lost the first game but then came back to win the second game and, eventually, the match as a whole. It wasn\'t until the following year that Deep Blue topped Kasparov over the course of a six-game contest. The thing to realize is that, after playing AlphaGo for the first time on Wednesday, Lee Sedol could adjust his style of play - just as Kasparov did back in 1996. But AlphaGo could not. Because this Google creation relies so heavily on machine learning techniques, the DeepMind team needs a good four to six weeks to train a new incarnation of the system. And that means they can\'t really change things during this eight-day match.  "This is about teaching and learning," Hassabis told us just before Game Two. "One game is not enough data to learn from - for a machine - and training takes an awful lot of time.'}
2017-12-03 07:52:46,162 - 5a22fa6ae3e52a40f793f587
2017-12-03 07:52:46,162 - TextSummarizer ------------------------------------ Init
2017-12-03 07:52:46,163 - _id: 5a22fa6ae3e52a40f793f587
2017-12-03 07:54:51,268 - Starting Flask Service......
2017-12-03 07:54:57,915 - CombinedMultiDict([ImmutableMultiDict([('q', '{}'), ('1512305697800', '')]), ImmutableMultiDict([])])
2017-12-03 07:54:57,915 - {}
2017-12-03 07:55:00,820 - {'content_title': "Google's AI Wins Pivotal Second Game in Match With Go Grandmaster", 'content_text': 'After more than four hours of tight play and a rapid-fire endgame, Google\'s artificially intelligent Go-playing computer system has won a second contest against grandmaster Lee Sedol, taking a two-games-to-none lead in their historic best-of-five match in downtown Seoul.  The surprisingly skillful Google machine, known as AlphaGo, now needs only one more win to claim victory in the match. The Korean-born Lee Sedol will go down in defeat unless he takes each of the match\'s last three games. Though machines have beaten the best humans at chess, checkers, Othello, Scrabble, Jeopardy!, and so many other games considered tests of human intellect, they have never beaten the very best at Go. Game Three is set for Saturday afternoon inside Seoul\'s Four Seasons hotel.  The match is a way of judging the suddenly rapid progress of artificial intelligence. One of the machine-learning techniques at the heart of AlphaGo has already reinvented myriad online services inside Google and other big-name Internet companies, helping to identify images, recognize commands spoken into smartphones, improve search engine results, and more. Meanwhile, another AlphaGo technique is now driving experimental robotics at Google and places like the University of California at Berkeley. This week\'s match can show how far these technologies have come - and perhaps how far they will go.  Created in Asia over 2,500 year ago, Go is exponentially more complex than chess, and at least among humans, it requires an added degree of intuition. Lee Sedol is widely-regarded as the top Go player of the last decade, after winning more international titles than all but one other player. He is currently ranked number five in the world, and according to Demis Hassabis, who leads DeepMind, the Google AI lab that created AlphaGo, his team chose the Korean for this all-important match because they wanted an opponent who would be remembered as one of history\'s great players.  Although AlphaGo topped Lee Sedol in the match\'s first game on Wednesday afternoon, the outcome of Game Two was no easier to predict. In his 1996 match with IBM\'s Deep Blue supercomputer, world chess champion Gary Kasparov lost the first game but then came back to win the second game and, eventually, the match as a whole. It wasn\'t until the following year that Deep Blue topped Kasparov over the course of a six-game contest. The thing to realize is that, after playing AlphaGo for the first time on Wednesday, Lee Sedol could adjust his style of play - just as Kasparov did back in 1996. But AlphaGo could not. Because this Google creation relies so heavily on machine learning techniques, the DeepMind team needs a good four to six weeks to train a new incarnation of the system. And that means they can\'t really change things during this eight-day match.  "This is about teaching and learning," Hassabis told us just before Game Two. "One game is not enough data to learn from - for a machine - and training takes an awful lot of time.'}
2017-12-03 07:55:00,932 - 5a22fa6ae3e52a40f793f587
2017-12-03 07:55:00,942 - TextSummarizer ------------------------------------ Init
2017-12-03 07:55:00,942 - _id: 5a22fa6ae3e52a40f793f587
2017-12-03 07:57:17,617 - Starting Flask Service......
2017-12-03 07:57:26,005 - CombinedMultiDict([ImmutableMultiDict([('q', '{}'), ('1512305845894', '')]), ImmutableMultiDict([])])
2017-12-03 07:57:26,005 - {}
2017-12-03 07:57:29,538 - {'content_title': "Google's AI Wins Pivotal Second Game in Match With Go Grandmaster", 'content_text': 'After more than four hours of tight play and a rapid-fire endgame, Google\'s artificially intelligent Go-playing computer system has won a second contest against grandmaster Lee Sedol, taking a two-games-to-none lead in their historic best-of-five match in downtown Seoul.  The surprisingly skillful Google machine, known as AlphaGo, now needs only one more win to claim victory in the match. The Korean-born Lee Sedol will go down in defeat unless he takes each of the match\'s last three games. Though machines have beaten the best humans at chess, checkers, Othello, Scrabble, Jeopardy!, and so many other games considered tests of human intellect, they have never beaten the very best at Go. Game Three is set for Saturday afternoon inside Seoul\'s Four Seasons hotel.  The match is a way of judging the suddenly rapid progress of artificial intelligence. One of the machine-learning techniques at the heart of AlphaGo has already reinvented myriad online services inside Google and other big-name Internet companies, helping to identify images, recognize commands spoken into smartphones, improve search engine results, and more. Meanwhile, another AlphaGo technique is now driving experimental robotics at Google and places like the University of California at Berkeley. This week\'s match can show how far these technologies have come - and perhaps how far they will go.  Created in Asia over 2,500 year ago, Go is exponentially more complex than chess, and at least among humans, it requires an added degree of intuition. Lee Sedol is widely-regarded as the top Go player of the last decade, after winning more international titles than all but one other player. He is currently ranked number five in the world, and according to Demis Hassabis, who leads DeepMind, the Google AI lab that created AlphaGo, his team chose the Korean for this all-important match because they wanted an opponent who would be remembered as one of history\'s great players.  Although AlphaGo topped Lee Sedol in the match\'s first game on Wednesday afternoon, the outcome of Game Two was no easier to predict. In his 1996 match with IBM\'s Deep Blue supercomputer, world chess champion Gary Kasparov lost the first game but then came back to win the second game and, eventually, the match as a whole. It wasn\'t until the following year that Deep Blue topped Kasparov over the course of a six-game contest. The thing to realize is that, after playing AlphaGo for the first time on Wednesday, Lee Sedol could adjust his style of play - just as Kasparov did back in 1996. But AlphaGo could not. Because this Google creation relies so heavily on machine learning techniques, the DeepMind team needs a good four to six weeks to train a new incarnation of the system. And that means they can\'t really change things during this eight-day match.  "This is about teaching and learning," Hassabis told us just before Game Two. "One game is not enough data to learn from - for a machine - and training takes an awful lot of time.'}
2017-12-03 07:57:29,646 - 5a22fa6ae3e52a40f793f587
2017-12-03 07:57:29,647 - After more than four hours of tight play and a rapid-fire endgame, Google's artificially intelligent Go-playing computer system has won a second contest against grandmaster Lee Sedol, taking a two-games-to-none lead in their historic best-of-five match in downtown Seoul.  The surprisingly skillful Google machine, known as AlphaGo, now needs only one more win to claim victory in the match. The Korean-born Lee Sedol will go down in defeat unless he takes each of the match's last three games. Though machines have beaten the best humans at chess, checkers, Othello, Scrabble, Jeopardy!, and so many other games considered tests of human intellect, they have never beaten the very best at Go. Game Three is set for Saturday afternoon inside Seoul's Four Seasons hotel.  The match is a way of judging the suddenly rapid progress of artificial intelligence. One of the machine-learning techniques at the heart of AlphaGo has already reinvented myriad online services inside Google and other big-name Internet companies, helping to identify images, recognize commands spoken into smartphones, improve search engine results, and more. Meanwhile, another AlphaGo technique is now driving experimental robotics at Google and places like the University of California at Berkeley. This week's match can show how far these technologies have come - and perhaps how far they will go.  Created in Asia over 2,500 year ago, Go is exponentially more complex than chess, and at least among humans, it requires an added degree of intuition. Lee Sedol is widely-regarded as the top Go player of the last decade, after winning more international titles than all but one other player. He is currently ranked number five in the world, and according to Demis Hassabis, who leads DeepMind, the Google AI lab that created AlphaGo, his team chose the Korean for this all-important match because they wanted an opponent who would be remembered as one of history's great players.  Although AlphaGo topped Lee Sedol in the match's first game on Wednesday afternoon, the outcome of Game Two was no easier to predict. In his 1996 match with IBM's Deep Blue supercomputer, world chess champion Gary Kasparov lost the first game but then came back to win the second game and, eventually, the match as a whole. It wasn't until the following year that Deep Blue topped Kasparov over the course of a six-game contest. The thing to realize is that, after playing AlphaGo for the first time on Wednesday, Lee Sedol could adjust his style of play - just as Kasparov did back in 1996. But AlphaGo could not. Because this Google creation relies so heavily on machine learning techniques, the DeepMind team needs a good four to six weeks to train a new incarnation of the system. And that means they can't really change things during this eight-day match.  "This is about teaching and learning," Hassabis told us just before Game Two. "One game is not enough data to learn from - for a machine - and training takes an awful lot of time.
2017-12-03 07:57:29,647 - TextSummarizer ------------------------------------ Init
2017-12-03 07:57:29,647 - _id: 5a22fa6ae3e52a40f793f587
2017-12-03 08:02:52,678 - Starting Flask Service......
2017-12-03 08:02:59,006 - CombinedMultiDict([ImmutableMultiDict([('q', '{}'), ('1512306178894', '')]), ImmutableMultiDict([])])
2017-12-03 08:02:59,006 - {}
2017-12-03 08:03:03,437 - {'content_title': "Google's AI Wins Pivotal Second Game in Match With Go Grandmaster", 'content_text': 'After more than four hours of tight play and a rapid-fire endgame, Google\'s artificially intelligent Go-playing computer system has won a second contest against grandmaster Lee Sedol, taking a two-games-to-none lead in their historic best-of-five match in downtown Seoul.  The surprisingly skillful Google machine, known as AlphaGo, now needs only one more win to claim victory in the match. The Korean-born Lee Sedol will go down in defeat unless he takes each of the match\'s last three games. Though machines have beaten the best humans at chess, checkers, Othello, Scrabble, Jeopardy!, and so many other games considered tests of human intellect, they have never beaten the very best at Go. Game Three is set for Saturday afternoon inside Seoul\'s Four Seasons hotel.  The match is a way of judging the suddenly rapid progress of artificial intelligence. One of the machine-learning techniques at the heart of AlphaGo has already reinvented myriad online services inside Google and other big-name Internet companies, helping to identify images, recognize commands spoken into smartphones, improve search engine results, and more. Meanwhile, another AlphaGo technique is now driving experimental robotics at Google and places like the University of California at Berkeley. This week\'s match can show how far these technologies have come - and perhaps how far they will go.  Created in Asia over 2,500 year ago, Go is exponentially more complex than chess, and at least among humans, it requires an added degree of intuition. Lee Sedol is widely-regarded as the top Go player of the last decade, after winning more international titles than all but one other player. He is currently ranked number five in the world, and according to Demis Hassabis, who leads DeepMind, the Google AI lab that created AlphaGo, his team chose the Korean for this all-important match because they wanted an opponent who would be remembered as one of history\'s great players.  Although AlphaGo topped Lee Sedol in the match\'s first game on Wednesday afternoon, the outcome of Game Two was no easier to predict. In his 1996 match with IBM\'s Deep Blue supercomputer, world chess champion Gary Kasparov lost the first game but then came back to win the second game and, eventually, the match as a whole. It wasn\'t until the following year that Deep Blue topped Kasparov over the course of a six-game contest. The thing to realize is that, after playing AlphaGo for the first time on Wednesday, Lee Sedol could adjust his style of play - just as Kasparov did back in 1996. But AlphaGo could not. Because this Google creation relies so heavily on machine learning techniques, the DeepMind team needs a good four to six weeks to train a new incarnation of the system. And that means they can\'t really change things during this eight-day match.  "This is about teaching and learning," Hassabis told us just before Game Two. "One game is not enough data to learn from - for a machine - and training takes an awful lot of time.'}
2017-12-03 08:03:03,552 - 5a22fa6ae3e52a40f793f587
2017-12-03 08:03:03,553 - TextSummarizer ------------------------------------ Init
2017-12-03 08:03:03,553 - _id: 5a22fa6ae3e52a40f793f587
2017-12-03 08:03:27,622 - {'content_title': "Google's AI Wins Pivotal Second Game in Match With Go Grandmaster", 'content_text': 'After more than four hours of tight play and a rapid-fire endgame, Google\'s artificially intelligent Go-playing computer system has won a second contest against grandmaster Lee Sedol, taking a two-games-to-none lead in their historic best-of-five match in downtown Seoul.  The surprisingly skillful Google machine, known as AlphaGo, now needs only one more win to claim victory in the match. The Korean-born Lee Sedol will go down in defeat unless he takes each of the match\'s last three games. Though machines have beaten the best humans at chess, checkers, Othello, Scrabble, Jeopardy!, and so many other games considered tests of human intellect, they have never beaten the very best at Go. Game Three is set for Saturday afternoon inside Seoul\'s Four Seasons hotel.  The match is a way of judging the suddenly rapid progress of artificial intelligence. One of the machine-learning techniques at the heart of AlphaGo has already reinvented myriad online services inside Google and other big-name Internet companies, helping to identify images, recognize commands spoken into smartphones, improve search engine results, and more. Meanwhile, another AlphaGo technique is now driving experimental robotics at Google and places like the University of California at Berkeley. This week\'s match can show how far these technologies have come - and perhaps how far they will go.  Created in Asia over 2,500 year ago, Go is exponentially more complex than chess, and at least among humans, it requires an added degree of intuition. Lee Sedol is widely-regarded as the top Go player of the last decade, after winning more international titles than all but one other player. He is currently ranked number five in the world, and according to Demis Hassabis, who leads DeepMind, the Google AI lab that created AlphaGo, his team chose the Korean for this all-important match because they wanted an opponent who would be remembered as one of history\'s great players.  Although AlphaGo topped Lee Sedol in the match\'s first game on Wednesday afternoon, the outcome of Game Two was no easier to predict. In his 1996 match with IBM\'s Deep Blue supercomputer, world chess champion Gary Kasparov lost the first game but then came back to win the second game and, eventually, the match as a whole. It wasn\'t until the following year that Deep Blue topped Kasparov over the course of a six-game contest. The thing to realize is that, after playing AlphaGo for the first time on Wednesday, Lee Sedol could adjust his style of play - just as Kasparov did back in 1996. But AlphaGo could not. Because this Google creation relies so heavily on machine learning techniques, the DeepMind team needs a good four to six weeks to train a new incarnation of the system. And that means they can\'t really change things during this eight-day match.  "This is about teaching and learning," Hassabis told us just before Game Two. "One game is not enough data to learn from - for a machine - and training takes an awful lot of time.'}
2017-12-03 08:03:27,731 - 5a22fa6ae3e52a40f793f587
2017-12-03 08:03:27,732 - TextSummarizer ------------------------------------ Init
2017-12-03 08:03:27,732 - _id: 5a22fa6ae3e52a40f793f587
2017-12-03 08:10:31,347 - Starting Flask Service......
2017-12-03 08:10:36,749 - CombinedMultiDict([ImmutableMultiDict([('q', '{}'), ('1512306636634', '')]), ImmutableMultiDict([])])
2017-12-03 08:10:36,749 - {}
2017-12-03 08:10:42,573 - {'content_title': "Google's AI Wins Pivotal Second Game in Match With Go Grandmaster", 'content_text': 'After more than four hours of tight play and a rapid-fire endgame, Google\'s artificially intelligent Go-playing computer system has won a second contest against grandmaster Lee Sedol, taking a two-games-to-none lead in their historic best-of-five match in downtown Seoul.  The surprisingly skillful Google machine, known as AlphaGo, now needs only one more win to claim victory in the match. The Korean-born Lee Sedol will go down in defeat unless he takes each of the match\'s last three games. Though machines have beaten the best humans at chess, checkers, Othello, Scrabble, Jeopardy!, and so many other games considered tests of human intellect, they have never beaten the very best at Go. Game Three is set for Saturday afternoon inside Seoul\'s Four Seasons hotel.  The match is a way of judging the suddenly rapid progress of artificial intelligence. One of the machine-learning techniques at the heart of AlphaGo has already reinvented myriad online services inside Google and other big-name Internet companies, helping to identify images, recognize commands spoken into smartphones, improve search engine results, and more. Meanwhile, another AlphaGo technique is now driving experimental robotics at Google and places like the University of California at Berkeley. This week\'s match can show how far these technologies have come - and perhaps how far they will go.  Created in Asia over 2,500 year ago, Go is exponentially more complex than chess, and at least among humans, it requires an added degree of intuition. Lee Sedol is widely-regarded as the top Go player of the last decade, after winning more international titles than all but one other player. He is currently ranked number five in the world, and according to Demis Hassabis, who leads DeepMind, the Google AI lab that created AlphaGo, his team chose the Korean for this all-important match because they wanted an opponent who would be remembered as one of history\'s great players.  Although AlphaGo topped Lee Sedol in the match\'s first game on Wednesday afternoon, the outcome of Game Two was no easier to predict. In his 1996 match with IBM\'s Deep Blue supercomputer, world chess champion Gary Kasparov lost the first game but then came back to win the second game and, eventually, the match as a whole. It wasn\'t until the following year that Deep Blue topped Kasparov over the course of a six-game contest. The thing to realize is that, after playing AlphaGo for the first time on Wednesday, Lee Sedol could adjust his style of play - just as Kasparov did back in 1996. But AlphaGo could not. Because this Google creation relies so heavily on machine learning techniques, the DeepMind team needs a good four to six weeks to train a new incarnation of the system. And that means they can\'t really change things during this eight-day match.  "This is about teaching and learning," Hassabis told us just before Game Two. "One game is not enough data to learn from - for a machine - and training takes an awful lot of time.'}
2017-12-03 08:10:42,682 - 5a22fa6ae3e52a40f793f587
2017-12-03 08:10:42,683 - TextSummarizer ------------------------------------ Init
2017-12-03 08:10:42,684 - _id: 5a22fa6ae3e52a40f793f587
2017-12-03 08:13:05,048 - Starting Flask Service......
2017-12-03 08:13:10,102 - CombinedMultiDict([ImmutableMultiDict([('q', '{}'), ('1512306789326', '')]), ImmutableMultiDict([])])
2017-12-03 08:13:10,102 - {}
2017-12-03 08:13:44,305 - {'content_title': "Google's AI Wins Pivotal Second Game in Match With Go Grandmaster", 'content_text': 'After more than four hours of tight play and a rapid-fire endgame, Google\'s artificially intelligent Go-playing computer system has won a second contest against grandmaster Lee Sedol, taking a two-games-to-none lead in their historic best-of-five match in downtown Seoul.  The surprisingly skillful Google machine, known as AlphaGo, now needs only one more win to claim victory in the match. The Korean-born Lee Sedol will go down in defeat unless he takes each of the match\'s last three games. Though machines have beaten the best humans at chess, checkers, Othello, Scrabble, Jeopardy!, and so many other games considered tests of human intellect, they have never beaten the very best at Go. Game Three is set for Saturday afternoon inside Seoul\'s Four Seasons hotel.  The match is a way of judging the suddenly rapid progress of artificial intelligence. One of the machine-learning techniques at the heart of AlphaGo has already reinvented myriad online services inside Google and other big-name Internet companies, helping to identify images, recognize commands spoken into smartphones, improve search engine results, and more. Meanwhile, another AlphaGo technique is now driving experimental robotics at Google and places like the University of California at Berkeley. This week\'s match can show how far these technologies have come - and perhaps how far they will go.  Created in Asia over 2,500 year ago, Go is exponentially more complex than chess, and at least among humans, it requires an added degree of intuition. Lee Sedol is widely-regarded as the top Go player of the last decade, after winning more international titles than all but one other player. He is currently ranked number five in the world, and according to Demis Hassabis, who leads DeepMind, the Google AI lab that created AlphaGo, his team chose the Korean for this all-important match because they wanted an opponent who would be remembered as one of history\'s great players.  Although AlphaGo topped Lee Sedol in the match\'s first game on Wednesday afternoon, the outcome of Game Two was no easier to predict. In his 1996 match with IBM\'s Deep Blue supercomputer, world chess champion Gary Kasparov lost the first game but then came back to win the second game and, eventually, the match as a whole. It wasn\'t until the following year that Deep Blue topped Kasparov over the course of a six-game contest. The thing to realize is that, after playing AlphaGo for the first time on Wednesday, Lee Sedol could adjust his style of play - just as Kasparov did back in 1996. But AlphaGo could not. Because this Google creation relies so heavily on machine learning techniques, the DeepMind team needs a good four to six weeks to train a new incarnation of the system. And that means they can\'t really change things during this eight-day match.  "This is about teaching and learning," Hassabis told us just before Game Two. "One game is not enough data to learn from - for a machine - and training takes an awful lot of time.'}
2017-12-03 08:13:44,414 - 5a22fa6ae3e52a40f793f587
2017-12-03 08:13:44,416 - TextSummarizer ------------------------------------ Init
2017-12-03 08:13:44,416 - _id: 5a22fa6ae3e52a40f793f587
2017-12-03 08:14:37,070 - CombinedMultiDict([ImmutableMultiDict([('q', '{}'), ('1512306876956', '')]), ImmutableMultiDict([])])
2017-12-03 08:14:37,070 - {}
2017-12-03 08:16:40,291 - CombinedMultiDict([ImmutableMultiDict([('q', '{}'), ('1512307000172', '')]), ImmutableMultiDict([])])
2017-12-03 08:16:40,291 - {}
2017-12-03 08:22:05,633 - {'content_title': 'Giving Computers the Ability to Learn from Data', 'content_text': 'In my opinion, machine learning, the application and science of algorithms that makes sense of data, is the most exciting field of all the computer sciences! We are living in an age where data comes in abundance; using the self-learning algorithms from the field of machine learning, we can turn this data into knowledge. Thanks to the many powerful open source libraries that have been developed in recent years, there has probably never been a better time to break into the machine learning field and learn how to utilize powerful algorithms to spot patterns in data and make predictions about future events.\nBuilding intelligent machines to transform data into knowledge. In this age of modern technology, there is one resource that we have in abundance: a large amount of structured and unstructured data. In the second half of the twentieth century, machine learning evolved as a subfield of artificial intelligence that involved the development of self-learning algorithms to gain knowledge from that data in order to make predictions. Instead of requiring humans to manually derive rules and build models from analyzing large amounts of data, machine learning offers a more efficient alternative for capturing the knowledge in data to gradually improve the performance of predictive models, and make data-driven decisions. Not only is machine learning becoming increasingly important in computer science research but it also plays an ever greater role in our everyday life. Thanks to machine learning, we enjoy robust e-mail spam filters, convenient text and voice recognition software, reliable Web search engines, challenging chess players, and, hopefully soon, safe and efficient self-driving cars.\nIn this section, we will take a look at the three types of machine learning: supervised learning, unsupervised learning, and reinforcement learning. We will learn about the fundamental differences between the three different learning types and, using conceptual examples, we will develop an intuition for the practical problem domains where these can be applied. Making predictions about the future with supervised learning. The main goal in supervised learning is to learn a model from labeled training data that allows us to make predictions about unseen or future data. Here, the term supervised refers to a set of samples where the desired output signals (labels) are already known.\nConsidering the example of e-mail spam filtering, we can train a model using a supervised machine learning algorithm on a corpus of labeled e-mail, e-mail that are correctly marked as spam or not-spam, to predict whether a new e-mail belongs to either of the two categories. A supervised learning task with discrete class labels, such as in the previous e-mail spam-filtering example, is also called a classification task. Another subcategory of supervised learning is regression, where the outcome signal is a continuous value. Classification for predicting class labels. Classification is a subcategory of supervised learning where the goal is to predict the categorical class labels of new instances based on past observations. Those class labels are discrete, unordered values that can be understood as the group memberships of the instances. The previously mentioned example of e-mail-spam detection represents a typical example of a binary classification task, where the machine learning algorithm learns a set of rules in order to distinguish between two possible classes: spam and non-spam e-mail.\nHowever, the set of class labels does not have to be of a binary nature. The predictive model learned by a supervised learning algorithm can assign any class label that was presented in the training dataset to a new, unlabeled instance. A typical example of a multi-class classification task is handwritten character recognition. Here, we could collect a training dataset that consists of multiple handwritten examples of each letter in the alphabet. Now, if a user provides a new handwritten character via an input device, our predictive model will be able to predict the correct letter in the alphabet with certain accuracy. However, our machine learning system would be unable to correctly recognize any of the digits zero to nine, for example, if they were not part of our training dataset.'}
2017-12-03 08:22:05,641 - <pymongo.results.InsertOneResult object at 0x12243b288>
2017-12-03 08:22:05,642 - 5a23fa7de3e52a606c38797f
2017-12-03 08:22:05,750 - 5a23fa7de3e52a606c38797f
2017-12-03 08:22:05,757 - TextSummarizer ------------------------------------ Init
2017-12-03 08:22:05,757 - _id: 5a23fa7de3e52a606c38797f
2017-12-03 08:22:28,690 - CombinedMultiDict([ImmutableMultiDict([('q', '{}'), ('1512307348579', '')]), ImmutableMultiDict([])])
2017-12-03 08:22:28,690 - {}
2017-12-03 08:26:22,504 - CombinedMultiDict([ImmutableMultiDict([('q', '{}'), ('1512307582394', '')]), ImmutableMultiDict([])])
2017-12-03 08:26:22,511 - {}
2017-12-03 08:32:26,084 - {'content_title': 'Text Classification Through Time', 'content_text': 'One of the fundamental assumptions for machine-learning based text classification systems is that the underlying distribution from which the set of labeled-text is drawn is identical to the distribution from which the text-to-be-labeled is drawn. However, in live news aggregation sites, this assumption is rarely correct. Instead, the events and topics discussed in news stories dramatically change over time. Rather than ignoring this phenomenon, we attempt to explicitly model the transitions of news stories and classifications over time to label stories that may be acquired months after the initial examples are labeled. We test our system, based on efficiently propagating labels in time-based graphs, with recently published news stories collected over an eighty day period. Experiments presented in this paper include the use of training labels from each story within the first several days of gathering stories, to using a single story as a label.\nThe writing, vocabulary, and topic of news stories rapidly shift within extremely small periods of time. In recent years, new events and breaking, “hot”, stories almost instantaneously dominate the majority of the press, while older topics just as quickly recede from popularity [19]. For typical automated news-classification systems, this can present severe challenges. For example, the ‘Political’ and ‘Entertainment’ breaking news stories of one week may have very little in common, in terms of subject or even vocabulary, with the news stories of the next week. An automated news classifier that is trained to accurately recognize the previous day/month/year’s stories may not have encountered the type of news story that will arise tomorrow.\nUnlike previous work on topic detection and tracking, we are not attempting to follow a particular topic over time or to determine when a new topic has emerged. Instead, we are addressing a related problem of immediate interest to live news aggregation sites: given that a news story has been published, in which of the site’s preset categories should it be placed?\nThe volume of news stories necessitates the use of an automated classifier. However, one of the fundamental assumptions in machine learning based approaches to news classification is that the underlying distribution from which the set of labeled-text is drawn is identical to the distribution from which the text-to-be-labeled is drawn. Because of the rapidly changing nature of news stories, this may not hold true. In this paper, we present a graph-based\napproach to address the problem of explicitly capturing both strong and weak similarities within news stories over time and to use these efficiently for categorization. Our approach combines the paradigm of Min-Hashing and label propagation in graphs in a novel way. While Min-Hashing is well-understood in information retrieval applications, our application of it to create a temporal similarity graph appears to be new. Label propagation is gaining popularity in the field of machine learning as a technique for semi- supervised learning. Our approach to label propagation follows our previous work [4], where equivalent views of a basic algorithm termed Adsorption were established, and the technique was successfully employed for propagating weak information in extremely large graphs to create a video recommendation system for YouTube.\nThe aims of this paper are to present the following techniques that we anticipate will have general applicability for data mining in industrial settings: formulation of temporal similarities via graphs created using Min-Hashes, and the application of label propagation as an off-the-shelf tool for classification tasks when very little ground truth is available.\nThe next section describes the data collected and presents a series of experiments to develop strong, realistic, baselines for performance. Section 3 gives a detailed description of the Adsorption algorithm. Section 4 presents the empirical results to establish the Adsorption baselines for this task. Section 5 presents extensive results with tiny amounts of labeled data (e.g., a single labeled example). Section 6 concludes the paper and offers avenues for future exploration.'}
2017-12-03 08:32:26,097 - <pymongo.results.InsertOneResult object at 0x1224acd38>
2017-12-03 08:32:26,097 - 5a23fceae3e52a606c387980
2017-12-03 08:32:26,206 - 5a23fceae3e52a606c387980
2017-12-03 08:32:26,212 - TextSummarizer ------------------------------------ Init
2017-12-03 08:32:26,213 - _id: 5a23fceae3e52a606c387980
2017-12-03 08:32:44,831 - CombinedMultiDict([ImmutableMultiDict([('q', '{}'), ('1512307964718', '')]), ImmutableMultiDict([])])
2017-12-03 08:32:44,834 - {}
2017-12-03 08:33:29,825 - {'content_title': 'Text Classification Through Time', 'content_text': 'One of the fundamental assumptions for machine-learning based text classification systems is that the underlying distribution from which the set of labeled-text is drawn is identical to the distribution from which the text-to-be-labeled is drawn. However, in live news aggregation sites, this assumption is rarely correct. Instead, the events and topics discussed in news stories dramatically change over time. Rather than ignoring this phenomenon, we attempt to explicitly model the transitions of news stories and classifications over time to label stories that may be acquired months after the initial examples are labeled. We test our system, based on efficiently propagating labels in time-based graphs, with recently published news stories collected over an eighty day period. Experiments presented in this paper include the use of training labels from each story within the first several days of gathering stories, to using a single story as a label.\nThe writing, vocabulary, and topic of news stories rapidly shift within extremely small periods of time. In recent years, new events and breaking, “hot”, stories almost instantaneously dominate the majority of the press, while older topics just as quickly recede from popularity. For typical automated news-classification systems, this can present severe challenges. For example, the ‘Political’ and ‘Entertainment’ breaking news stories of one week may have very little in common, in terms of subject or even vocabulary, with the news stories of the next week. An automated news classifier that is trained to accurately recognize the previous day/month/year’s stories may not have encountered the type of news story that will arise tomorrow.\nUnlike previous work on topic detection and tracking, we are not attempting to follow a particular topic over time or to determine when a new topic has emerged. Instead, we are addressing a related problem of immediate interest to live news aggregation sites: given that a news story has been published, in which of the site’s preset categories should it be placed?\nThe volume of news stories necessitates the use of an automated classifier. However, one of the fundamental assumptions in machine learning based approaches to news classification is that the underlying distribution from which the set of labeled-text is drawn is identical to the distribution from which the text-to-be-labeled is drawn. Because of the rapidly changing nature of news stories, this may not hold true. In this paper, we present a graph-based\napproach to address the problem of explicitly capturing both strong and weak similarities within news stories over time and to use these efficiently for categorization. Our approach combines the paradigm of Min-Hashing and label propagation in graphs in a novel way. While Min-Hashing is well-understood in information retrieval applications, our application of it to create a temporal similarity graph appears to be new. Label propagation is gaining popularity in the field of machine learning as a technique for semi- supervised learning. Our approach to label propagation follows our previous work [4], where equivalent views of a basic algorithm termed Adsorption were established, and the technique was successfully employed for propagating weak information in extremely large graphs to create a video recommendation system for YouTube.\nThe aims of this paper are to present the following techniques that we anticipate will have general applicability for data mining in industrial settings: formulation of temporal similarities via graphs created using Min-Hashes, and the application of label propagation as an off-the-shelf tool for classification tasks when very little ground truth is available.\nThe next section describes the data collected and presents a series of experiments to develop strong, realistic, baselines for performance. Section 3 gives a detailed description of the Adsorption algorithm. Section 4 presents the empirical results to establish the Adsorption baselines for this task. Section 5 presents extensive results with tiny amounts of labeled data (e.g., a single labeled example). Section 6 concludes the paper and offers avenues for future exploration.'}
2017-12-03 08:33:29,941 - 5a23fceae3e52a606c387980
2017-12-03 08:33:29,944 - TextSummarizer ------------------------------------ Init
2017-12-03 08:33:29,944 - _id: 5a23fceae3e52a606c387980
2017-12-03 08:38:44,659 - CombinedMultiDict([ImmutableMultiDict([('q', '{}'), ('1512308324548', '')]), ImmutableMultiDict([])])
2017-12-03 08:38:44,662 - {}
2017-12-03 08:38:57,700 - {'content_title': 'Visualizing and Understanding Convolutional Networks', 'content_text': 'Since their introduction by (LeCun et al., 1989) in the early 1990’s, Convolutional Networks (convnets) have demonstrated excellent performance at tasks such as hand-written digit classification and face detec- tion. In the last year, several papers have shown that they can also deliver outstanding performance on more challenging visual classification tasks. (Ciresan et al., 2012) demonstrate state-of-the-art performance on NORB and CIFAR-10 datasets. Most notably, (Krizhevsky et al., 2012) show record beating perfor- mance on the ImageNet 2012 classification benchmark, with their convnet model achieving an error rate of 16.4%, compared to the 2nd place result of 26.1%. Several factors are responsible for this renewed interest in convnet models: (i) the availability of much larger training sets, with millions of labeled exam- ples; (ii) powerful GPU implementations, making the training of very large models practical and (iii) bet- ter model regularization strategies, such as Dropout.\nDespite this encouraging progress, there is still lit- tle insight into the internal operation and behavior of these complex models, or how they achieve such good performance. From a scientific standpoint, this is deeply unsatisfactory. Without clear understanding of how and why they work, the development of better models is reduced to trial-and-error. In this paper we introduce a visualization technique that reveals the in- put stimuli that excite individual feature maps at any layer in the model. It also allows us to observe the evolution of features during training and to diagnose potential problems with the model. The visualization technique we propose uses a multi-layered Deconvolutional Network (deconvnet), as proposed by (Zeiler et al., 2011), to project the feature activations back to the input pixel space. We also perform a sensitivity analysis of the classifier output by occluding portions of the input image, revealing which parts of the scene are important for classification.\nUsing these tools, we start with the architecture of (Krizhevsky et al., 2012) and explore different archi- tectures, discovering ones that outperform their results on ImageNet. We then explore the generalization abil- ity of the model to other datasets, just retraining the softmax classifier on top. As such, this is a form of su- pervised pre-training, which contrasts with the unsu- pervised pre-training methods popularized by (Hinton et al., 2006) and others (Bengio et al., 2007; Vincent et al., 2008). The generalization ability of convnet fea- tures is also explored in concurrent work by (Donahue et al., 2013).\nVisualizing features to gain intuition about the net- work is common practice, but mostly limited to the 1st layer where projections to pixel space are possible. In higher layers this is not the case, and there are limited methods for interpreting activity. (Erhan et al., 2009) find the optimal stimulus for each unit by perform- ing gradient descent in image space to maximize the unit’s activation. This requires a careful initialization and does not give any information about the unit’s in- variances. Motivated by the latter’s short-coming, (Le et al., 2010) (extending an idea by (Berkes & Wiskott, 2006)) show how the Hessian of a given unit may be computed numerically around the optimal response, giving some insight into invariances. The problem is that for higher layers, the invariances are extremely complex so are poorly captured by a simple quadratic approximation. Our approach, by contrast, provides a non-parametric view of invariance, showing which pat- terns from the training set activate the feature map. (Donahue et al., 2013) show visualizations that iden- tify patches within a dataset that are responsible for strong activations at higher layers in the model. Our visualizations differ in that they are not just crops of input images, but rather top-down projections that reveal structures within each patch that stimulate a particular feature map.\nWe use standard fully supervised convnet models\nthroughout the paper, as defined by (LeCun et al.,\n1989) and (Krizhevsky et al., 2012). These models map a color 2D input image xi, via a series of layers, to a probability vector yˆ over the C different i classes. Each layer consists of (i) convolution of the previous layer output (or, in the case of the 1st layer, the input image) with a set of learned filters; (ii) pass- ing the responses through a rectified linear function (relu(x) = max(x,0)); (iii) [optionally] max pooling over local neighborhoods and (iv) [optionally] a lo- cal contrast operation that normalizes the responses across feature maps. For more details of these opera- tions, see (Krizhevsky et al., 2012) and (Jarrett et al., 2009). The top few layers of the network are conventional fully-connected networks and the final layer is a softmax classifier. Fig. 3 shows the model used in many of our experiments.\nWe train these models using a large set of N labeled images {x,y}, where label yi is a discrete variable indicating the true class. A cross-entropy loss func- tion, suitable for image classification, is used to compare yˆ and y . The parameters of the network (filters in the convolutional layers, weight matrices in the fully-connected layers and biases) are trained by back- propagating the derivative of the loss with respect to the parameters throughout the network, and updating the parameters via stochastic gradient descent.\nUnderstanding the operation of a convnet requires in- terpreting the feature activity in intermediate layers. We present a novel way to map these activities back to the input pixel space, showing what input pattern orig- inally caused a given activation in the feature maps. We perform this mapping with a Deconvolutional Net- work (deconvnet) (Zeiler et al., 2011). A deconvnet can be thought of as a convnet model that uses the same components (filtering, pooling) but in reverse, so instead of mapping pixels to features does the oppo- site. In (Zeiler et al., 2011), deconvnets were proposed as a way of performing unsupervised learning. Here, they are not used in any learning capacity, just as a probe of an already trained convnet.\nTo examine a convnet, a deconvnet is attached to each of its layers, as illustrated in Fig. 1(top), providing a continuous path back to image pixels. To start, an input image is presented to the convnet and features computed throughout the layers. To examine a given convnet activation, we set all other activations in the layer to zero and pass the feature maps as input to the attached deconvnet layer. Then we successively (i) unpool, (ii) rectify and (iii) filter to reconstruct the activity in the layer beneath that gave rise to the chosen activation. This is then repeated until input pixel space is reached.\nUnpooling: In the convnet, the max pooling opera- tion is non-invertible, however we can obtain an ap- proximate inverse by recording the locations of the maxima within each pooling region in a set of switch variables. In the deconvnet, the unpooling operation uses these switches to place the reconstructions from the layer above into appropriate locations, preserving the structure of the stimulus. See Fig. 1(bottom) for an illustration of the procedure.\nRectification: The convnet uses relu non-linearities, which rectify the feature maps thus ensuring the fea- ture maps are always positive. To obtain valid fea- ture reconstructions at each layer (which also should be positive), we pass the reconstructed signal through a relu non-linearity.\nFiltering: The convnet uses learned filters to con- volve the feature maps from the previous layer. To invert this, the deconvnet uses transposed versions of the same filters, but applied to the rectified maps, not the output of the layer beneath. In practice this means flipping each filter vertically and horizontally.\nProjecting down from higher layers uses the switch settings generated by the max pooling in the convnet on the way up. As these switch settings are peculiar to a given input image, the reconstruction obtained from a single activation thus resembles a small piece of the original input image, with structures weighted according to their contribution toward to the feature activation. Since the model is trained discriminatively, they implicitly show which parts of the input image are discriminative. Note that these projections are not samples from the model, since there is no generative process involved.'}
2017-12-03 08:38:57,701 - <pymongo.results.InsertOneResult object at 0x1224f8870>
2017-12-03 08:38:57,701 - 5a23fe71e3e52a606c387981
2017-12-03 08:38:57,811 - 5a23fe71e3e52a606c387981
2017-12-03 08:38:57,818 - TextSummarizer ------------------------------------ Init
2017-12-03 08:38:57,818 - _id: 5a23fe71e3e52a606c387981
2017-12-03 08:41:01,106 - {'content_title': 'Visualizing and Understanding Convolutional Networks', 'content_text': 'Since their introduction by (LeCun et al., 1989) in the early 1990’s, Convolutional Networks (convnets) have demonstrated excellent performance at tasks such as hand-written digit classification and face detection. In the last year, several papers have shown that they can also deliver outstanding performance on more challenging visual classification tasks. (Ciresan et al., 2012) demonstrate state-of-the-art performance on NORB and CIFAR-10 datasets. Most notably, (Krizhevsky et al., 2012) show record beating performance on the ImageNet 2012 classification benchmark, with their convnet model achieving an error rate of 16.4%, compared to the 2nd place result of 26.1%. Several factors are responsible for this renewed interest in convnet models: (i) the availability of much larger training sets, with millions of labeled examples; (ii) powerful GPU implementations, making the training of very large models practical and (iii) better model regularization strategies, such as Dropout.\nDespite this encouraging progress, there is still lit- tle insight into the internal operation and behavior of these complex models, or how they achieve such good performance. From a scientific standpoint, this is deeply unsatisfactory. Without clear understanding of how and why they work, the development of better models is reduced to trial-and-error. In this paper we introduce a visualization technique that reveals the in- put stimuli that excite individual feature maps at any layer in the model. It also allows us to observe the evolution of features during training and to diagnose potential problems with the model. The visualization technique we propose uses a multi-layered Deconvolutional Network (deconvnet), as proposed by (Zeiler et al., 2011), to project the feature activations back to the input pixel space. We also perform a sensitivity analysis of the classifier output by occluding portions of the input image, revealing which parts of the scene are important for classification.\nUsing these tools, we start with the architecture of (Krizhevsky et al., 2012) and explore different architectures, discovering ones that outperform their results on ImageNet. We then explore the generalization abil- ity of the model to other datasets, just retraining the softmax classifier on top. As such, this is a form of su- pervised pre-training, which contrasts with the unsupervised pre-training methods popularized by (Hinton et al., 2006) and others (Bengio et al., 2007; Vincent et al., 2008). The generalization ability of convnet features is also explored in concurrent work by (Donahue et al., 2013).\nVisualizing features to gain intuition about the net- work is common practice, but mostly limited to the 1st layer where projections to pixel space are possible. In higher layers this is not the case, and there are limited methods for interpreting activity. (Erhan et al., 2009) find the optimal stimulus for each unit by performing gradient descent in image space to maximize the unit’s activation. This requires a careful initialization and does not give any information about the unit’s in- variances. Motivated by the latter’s short-coming, (Le et al., 2010) (extending an idea by (Berkes & Wiskott, 2006)) show how the Hessian of a given unit may be computed numerically around the optimal response, giving some insight into invariances. The problem is that for higher layers, the invariances are extremely complex so are poorly captured by a simple quadratic approximation. Our approach, by contrast, provides a non-parametric view of invariance, showing which pat- terns from the training set activate the feature map. (Donahue et al., 2013) show visualizations that iden- tify patches within a dataset that are responsible for strong activations at higher layers in the model. Our visualizations differ in that they are not just crops of input images, but rather top-down projections that reveal structures within each patch that stimulate a particular feature map.\nWe use standard fully supervised convnet models\nthroughout the paper, as defined by (LeCun et al.,\n1989) and (Krizhevsky et al., 2012). These models map a color 2D input image xi, via a series of layers, to a probability vector yˆ over the C different i classes. Each layer consists of (i) convolution of the previous layer output (or, in the case of the 1st layer, the input image) with a set of learned filters; (ii) pass- ing the responses through a rectified linear function (relu(x) = max(x,0)); (iii) [optionally] max pooling over local neighborhoods and (iv) [optionally] a lo- cal contrast operation that normalizes the responses across feature maps. For more details of these opera- tions, see (Krizhevsky et al., 2012) and (Jarrett et al., 2009). The top few layers of the network are conventional fully-connected networks and the final layer is a softmax classifier. Fig. 3 shows the model used in many of our experiments.\nWe train these models using a large set of N labeled images {x,y}, where label yi is a discrete variable indicating the true class. A cross-entropy loss function, suitable for image classification, is used to compare yˆ and y . The parameters of the network (filters in the convolutional layers, weight matrices in the fully-connected layers and biases) are trained by back- propagating the derivative of the loss with respect to the parameters throughout the network, and updating the parameters via stochastic gradient descent.\nUnderstanding the operation of a convnet requires in- terpreting the feature activity in intermediate layers. We present a novel way to map these activities back to the input pixel space, showing what input pattern orig- inally caused a given activation in the feature maps. We perform this mapping with a Deconvolutional Network (deconvnet) (Zeiler et al., 2011). A deconvnet can be thought of as a convnet model that uses the same components (filtering, pooling) but in reverse, so instead of mapping pixels to features does the oppo- site. In (Zeiler et al., 2011), deconvnets were proposed as a way of performing unsupervised learning. Here, they are not used in any learning capacity, just as a probe of an already trained convnet.\nTo examine a convnet, a deconvnet is attached to each of its layers, as illustrated in Fig. 1(top), providing a continuous path back to image pixels. To start, an input image is presented to the convnet and features computed throughout the layers. To examine a given convnet activation, we set all other activations in the layer to zero and pass the feature maps as input to the attached deconvnet layer. Then we successively (i) unpool, (ii) rectify and (iii) filter to reconstruct the activity in the layer beneath that gave rise to the chosen activation. This is then repeated until input pixel space is reached.\nUnpooling: In the convnet, the max pooling opera- tion is non-invertible, however we can obtain an ap- proximate inverse by recording the locations of the maxima within each pooling region in a set of switch variables. In the deconvnet, the unpooling operation uses these switches to place the reconstructions from the layer above into appropriate locations, preserving the structure of the stimulus. See Fig. 1(bottom) for an illustration of the procedure.\nRectification: The convnet uses relu non-linearities, which rectify the feature maps thus ensuring the fea- ture maps are always positive. To obtain valid feature reconstructions at each layer (which also should be positive), we pass the reconstructed signal through a relu non-linearity.\nFiltering: The convnet uses learned filters to convolve the feature maps from the previous layer. To invert this, the deconvnet uses transposed versions of the same filters, but applied to the rectified maps, not the output of the layer beneath. In practice this means flipping each filter vertically and horizontally.\nProjecting down from higher layers uses the switch settings generated by the max pooling in the convnet on the way up. As these switch settings are peculiar to a given input image, the reconstruction obtained from a single activation thus resembles a small piece of the original input image, with structures weighted according to their contribution toward to the feature activation. Since the model is trained discriminatively, they implicitly show which parts of the input image are discriminative. Note that these projections are not samples from the model, since there is no generative process involved.'}
2017-12-03 08:41:01,216 - 5a23fe71e3e52a606c387981
2017-12-03 08:41:01,217 - TextSummarizer ------------------------------------ Init
2017-12-03 08:41:01,217 - _id: 5a23fe71e3e52a606c387981
2017-12-03 08:42:11,975 - {'content_title': 'Visualizing and Understanding Convolutional Networks', 'content_text': 'Since their introduction by (LeCun et al., 1989) in the early 1990’s, Convolutional Networks (convnets) have demonstrated excellent performance at tasks such as hand-written digit classification and face detection. In the last year, several papers have shown that they can also deliver outstanding performance on more challenging visual classification tasks. (Ciresan et al., 2012) demonstrate state-of-the-art performance on NORB and CIFAR-10 datasets. Most notably, (Krizhevsky et al., 2012) show record beating performance on the ImageNet 2012 classification benchmark, with their convnet model achieving an error rate of 16.4%, compared to the 2nd place result of 26.1%. Several factors are responsible for this renewed interest in convnet models: (i) the availability of much larger training sets, with millions of labeled examples; (ii) powerful GPU implementations, making the training of very large models practical and (iii) better model regularization strategies, such as Dropout.\nDespite this encouraging progress, there is still lit- tle insight into the internal operation and behavior of these complex models, or how they achieve such good performance. From a scientific standpoint, this is deeply unsatisfactory. Without clear understanding of how and why they work, the development of better models is reduced to trial-and-error. In this paper we introduce a visualization technique that reveals the input stimuli that excite individual feature maps at any layer in the model. It also allows us to observe the evolution of features during training and to diagnose potential problems with the model. The visualization technique we propose uses a multi-layered Deconvolutional Network (deconvnet), as proposed by (Zeiler et al., 2011), to project the feature activations back to the input pixel space. We also perform a sensitivity analysis of the classifier output by occluding portions of the input image, revealing which parts of the scene are important for classification.\nUsing these tools, we start with the architecture of (Krizhevsky et al., 2012) and explore different architectures, discovering ones that outperform their results on ImageNet. We then explore the generalization ability of the model to other datasets, just retraining the softmax classifier on top. As such, this is a form of supervised pre-training, which contrasts with the unsupervised pre-training methods popularized by (Hinton et al., 2006) and others (Bengio et al., 2007; Vincent et al., 2008). The generalization ability of convnet features is also explored in concurrent work by (Donahue et al., 2013).\nVisualizing features to gain intuition about the net- work is common practice, but mostly limited to the 1st layer where projections to pixel space are possible. In higher layers this is not the case, and there are limited methods for interpreting activity. (Erhan et al., 2009) find the optimal stimulus for each unit by performing gradient descent in image space to maximize the unit’s activation. This requires a careful initialization and does not give any information about the unit’s in- variances. Motivated by the latter’s short-coming, (Le et al., 2010) (extending an idea by (Berkes & Wiskott, 2006)) show how the Hessian of a given unit may be computed numerically around the optimal response, giving some insight into invariances. The problem is that for higher layers, the invariances are extremely complex so are poorly captured by a simple quadratic approximation. Our approach, by contrast, provides a non-parametric view of invariance, showing which pat- terns from the training set activate the feature map. (Donahue et al., 2013) show visualizations that iden- tify patches within a dataset that are responsible for strong activations at higher layers in the model. Our visualizations differ in that they are not just crops of input images, but rather top-down projections that reveal structures within each patch that stimulate a particular feature map.\nWe use standard fully supervised convnet models\nthroughout the paper, as defined by (LeCun et al.,\n1989) and (Krizhevsky et al., 2012). These models map a color 2D input image xi, via a series of layers, to a probability vector yˆ over the C different i classes. Each layer consists of (i) convolution of the previous layer output (or, in the case of the 1st layer, the input image) with a set of learned filters; (ii) pass- ing the responses through a rectified linear function (relu(x) = max(x,0)); (iii) [optionally] max pooling over local neighborhoods and (iv) [optionally] a lo- cal contrast operation that normalizes the responses across feature maps. For more details of these opera- tions, see (Krizhevsky et al., 2012) and (Jarrett et al., 2009). The top few layers of the network are conventional fully-connected networks and the final layer is a softmax classifier. Fig. 3 shows the model used in many of our experiments.\nWe train these models using a large set of N labeled images {x,y}, where label yi is a discrete variable indicating the true class. A cross-entropy loss function, suitable for image classification, is used to compare yˆ and y . The parameters of the network (filters in the convolutional layers, weight matrices in the fully-connected layers and biases) are trained by back- propagating the derivative of the loss with respect to the parameters throughout the network, and updating the parameters via stochastic gradient descent.\nUnderstanding the operation of a convnet requires interpreting the feature activity in intermediate layers. We present a novel way to map these activities back to the input pixel space, showing what input pattern orig- inally caused a given activation in the feature maps. We perform this mapping with a Deconvolutional Network (deconvnet) (Zeiler et al., 2011). A deconvnet can be thought of as a convnet model that uses the same components (filtering, pooling) but in reverse, so instead of mapping pixels to features does the opposite. In (Zeiler et al., 2011), deconvnets were proposed as a way of performing unsupervised learning. Here, they are not used in any learning capacity, just as a probe of an already trained convnet.\nTo examine a convnet, a deconvnet is attached to each of its layers, as illustrated in Fig. 1(top), providing a continuous path back to image pixels. To start, an input image is presented to the convnet and features computed throughout the layers. To examine a given convnet activation, we set all other activations in the layer to zero and pass the feature maps as input to the attached deconvnet layer. Then we successively (i) unpool, (ii) rectify and (iii) filter to reconstruct the activity in the layer beneath that gave rise to the chosen activation. This is then repeated until input pixel space is reached.\nUnpooling: In the convnet, the max pooling opera- tion is non-invertible, however we can obtain an ap- proximate inverse by recording the locations of the maxima within each pooling region in a set of switch variables. In the deconvnet, the unpooling operation uses these switches to place the reconstructions from the layer above into appropriate locations, preserving the structure of the stimulus. See Fig. 1(bottom) for an illustration of the procedure.\nRectification: The convnet uses relu non-linearities, which rectify the feature maps thus ensuring the feature maps are always positive. To obtain valid feature reconstructions at each layer (which also should be positive), we pass the reconstructed signal through a relu non-linearity.\nFiltering: The convnet uses learned filters to convolve the feature maps from the previous layer. To invert this, the deconvnet uses transposed versions of the same filters, but applied to the rectified maps, not the output of the layer beneath. In practice this means flipping each filter vertically and horizontally.\nProjecting down from higher layers uses the switch settings generated by the max pooling in the convnet on the way up. As these switch settings are peculiar to a given input image, the reconstruction obtained from a single activation thus resembles a small piece of the original input image, with structures weighted according to their contribution toward to the feature activation. Since the model is trained discriminatively, they implicitly show which parts of the input image are discriminative. Note that these projections are not samples from the model, since there is no generative process involved.'}
2017-12-03 08:42:12,090 - 5a23fe71e3e52a606c387981
2017-12-03 08:42:12,094 - TextSummarizer ------------------------------------ Init
2017-12-03 08:42:12,094 - _id: 5a23fe71e3e52a606c387981
2017-12-03 08:49:42,532 - CombinedMultiDict([ImmutableMultiDict([('q', '{}'), ('1512308982422', '')]), ImmutableMultiDict([])])
2017-12-03 08:49:42,535 - {}
2017-12-03 08:50:03,415 - CombinedMultiDict([ImmutableMultiDict([('q', '{}'), ('1512309003303', '')]), ImmutableMultiDict([])])
2017-12-03 08:50:03,416 - {}
2017-12-03 08:50:18,811 - CombinedMultiDict([ImmutableMultiDict([('q', '{}'), ('1512309018697', '')]), ImmutableMultiDict([])])
2017-12-03 08:50:18,811 - {}
2017-12-03 08:50:23,418 - CombinedMultiDict([ImmutableMultiDict([('q', '{}'), ('1512309023311', '')]), ImmutableMultiDict([])])
2017-12-03 08:50:23,419 - {}
2017-12-03 08:50:31,604 - {'content_title': "Google's AI Wins Pivotal Second Game in Match With Go Grandmaster", 'content_text': 'After more than four hours of tight play and a rapid-fire endgame, Google\'s artificially intelligent Go-playing computer system has won a second contest against grandmaster Lee Sedol, taking a two-games-to-none lead in their historic best-of-five match in downtown Seoul.  The surprisingly skillful Google machine, known as AlphaGo, now needs only one more win to claim victory in the match. The Korean-born Lee Sedol will go down in defeat unless he takes each of the match\'s last three games. Though machines have beaten the best humans at chess, checkers, Othello, Scrabble, Jeopardy!, and so many other games considered tests of human intellect, they have never beaten the very best at Go. Game Three is set for Saturday afternoon inside Seoul\'s Four Seasons hotel.  The match is a way of judging the suddenly rapid progress of artificial intelligence. One of the machine-learning techniques at the heart of AlphaGo has already reinvented myriad online services inside Google and other big-name Internet companies, helping to identify images, recognize commands spoken into smartphones, improve search engine results, and more. Meanwhile, another AlphaGo technique is now driving experimental robotics at Google and places like the University of California at Berkeley. This week\'s match can show how far these technologies have come - and perhaps how far they will go.  Created in Asia over 2,500 year ago, Go is exponentially more complex than chess, and at least among humans, it requires an added degree of intuition. Lee Sedol is widely-regarded as the top Go player of the last decade, after winning more international titles than all but one other player. He is currently ranked number five in the world, and according to Demis Hassabis, who leads DeepMind, the Google AI lab that created AlphaGo, his team chose the Korean for this all-important match because they wanted an opponent who would be remembered as one of history\'s great players.  Although AlphaGo topped Lee Sedol in the match\'s first game on Wednesday afternoon, the outcome of Game Two was no easier to predict. In his 1996 match with IBM\'s Deep Blue supercomputer, world chess champion Gary Kasparov lost the first game but then came back to win the second game and, eventually, the match as a whole. It wasn\'t until the following year that Deep Blue topped Kasparov over the course of a six-game contest. The thing to realize is that, after playing AlphaGo for the first time on Wednesday, Lee Sedol could adjust his style of play - just as Kasparov did back in 1996. But AlphaGo could not. Because this Google creation relies so heavily on machine learning techniques, the DeepMind team needs a good four to six weeks to train a new incarnation of the system. And that means they can\'t really change things during this eight-day match.  "This is about teaching and learning," Hassabis told us just before Game Two. "One game is not enough data to learn from - for a machine - and training takes an awful lot of time.', 'word_limit': '150'}
2017-12-03 08:50:31,712 - 5a22fa6ae3e52a40f793f587
2017-12-03 08:50:31,720 - TextSummarizer ------------------------------------ Init
2017-12-03 08:50:31,720 - _id: 5a22fa6ae3e52a40f793f587
2017-12-03 08:50:35,478 - CombinedMultiDict([ImmutableMultiDict([('q', '{}'), ('1512309035372', '')]), ImmutableMultiDict([])])
2017-12-03 08:50:35,478 - {}
2017-12-03 08:50:43,517 - {'content_title': 'Giving Computers the Ability to Learn from Data', 'content_text': 'In my opinion, machine learning, the application and science of algorithms that makes sense of data, is the most exciting field of all the computer sciences! We are living in an age where data comes in abundance; using the self-learning algorithms from the field of machine learning, we can turn this data into knowledge. Thanks to the many powerful open source libraries that have been developed in recent years, there has probably never been a better time to break into the machine learning field and learn how to utilize powerful algorithms to spot patterns in data and make predictions about future events.\nBuilding intelligent machines to transform data into knowledge. In this age of modern technology, there is one resource that we have in abundance: a large amount of structured and unstructured data. In the second half of the twentieth century, machine learning evolved as a subfield of artificial intelligence that involved the development of self-learning algorithms to gain knowledge from that data in order to make predictions. Instead of requiring humans to manually derive rules and build models from analyzing large amounts of data, machine learning offers a more efficient alternative for capturing the knowledge in data to gradually improve the performance of predictive models, and make data-driven decisions. Not only is machine learning becoming increasingly important in computer science research but it also plays an ever greater role in our everyday life. Thanks to machine learning, we enjoy robust e-mail spam filters, convenient text and voice recognition software, reliable Web search engines, challenging chess players, and, hopefully soon, safe and efficient self-driving cars.\nIn this section, we will take a look at the three types of machine learning: supervised learning, unsupervised learning, and reinforcement learning. We will learn about the fundamental differences between the three different learning types and, using conceptual examples, we will develop an intuition for the practical problem domains where these can be applied. Making predictions about the future with supervised learning. The main goal in supervised learning is to learn a model from labeled training data that allows us to make predictions about unseen or future data. Here, the term supervised refers to a set of samples where the desired output signals (labels) are already known.\nConsidering the example of e-mail spam filtering, we can train a model using a supervised machine learning algorithm on a corpus of labeled e-mail, e-mail that are correctly marked as spam or not-spam, to predict whether a new e-mail belongs to either of the two categories. A supervised learning task with discrete class labels, such as in the previous e-mail spam-filtering example, is also called a classification task. Another subcategory of supervised learning is regression, where the outcome signal is a continuous value. Classification for predicting class labels. Classification is a subcategory of supervised learning where the goal is to predict the categorical class labels of new instances based on past observations. Those class labels are discrete, unordered values that can be understood as the group memberships of the instances. The previously mentioned example of e-mail-spam detection represents a typical example of a binary classification task, where the machine learning algorithm learns a set of rules in order to distinguish between two possible classes: spam and non-spam e-mail.\nHowever, the set of class labels does not have to be of a binary nature. The predictive model learned by a supervised learning algorithm can assign any class label that was presented in the training dataset to a new, unlabeled instance. A typical example of a multi-class classification task is handwritten character recognition. Here, we could collect a training dataset that consists of multiple handwritten examples of each letter in the alphabet. Now, if a user provides a new handwritten character via an input device, our predictive model will be able to predict the correct letter in the alphabet with certain accuracy. However, our machine learning system would be unable to correctly recognize any of the digits zero to nine, for example, if they were not part of our training dataset.', 'word_limit': '150'}
2017-12-03 08:50:43,626 - 5a23fa7de3e52a606c38797f
2017-12-03 08:50:43,629 - TextSummarizer ------------------------------------ Init
2017-12-03 08:50:43,629 - _id: 5a23fa7de3e52a606c38797f
2017-12-03 08:50:54,541 - {'content_title': 'Text Classification Through Time', 'content_text': 'One of the fundamental assumptions for machine-learning based text classification systems is that the underlying distribution from which the set of labeled-text is drawn is identical to the distribution from which the text-to-be-labeled is drawn. However, in live news aggregation sites, this assumption is rarely correct. Instead, the events and topics discussed in news stories dramatically change over time. Rather than ignoring this phenomenon, we attempt to explicitly model the transitions of news stories and classifications over time to label stories that may be acquired months after the initial examples are labeled. We test our system, based on efficiently propagating labels in time-based graphs, with recently published news stories collected over an eighty day period. Experiments presented in this paper include the use of training labels from each story within the first several days of gathering stories, to using a single story as a label.\nThe writing, vocabulary, and topic of news stories rapidly shift within extremely small periods of time. In recent years, new events and breaking, “hot”, stories almost instantaneously dominate the majority of the press, while older topics just as quickly recede from popularity. For typical automated news-classification systems, this can present severe challenges. For example, the ‘Political’ and ‘Entertainment’ breaking news stories of one week may have very little in common, in terms of subject or even vocabulary, with the news stories of the next week. An automated news classifier that is trained to accurately recognize the previous day/month/year’s stories may not have encountered the type of news story that will arise tomorrow.\nUnlike previous work on topic detection and tracking, we are not attempting to follow a particular topic over time or to determine when a new topic has emerged. Instead, we are addressing a related problem of immediate interest to live news aggregation sites: given that a news story has been published, in which of the site’s preset categories should it be placed?\nThe volume of news stories necessitates the use of an automated classifier. However, one of the fundamental assumptions in machine learning based approaches to news classification is that the underlying distribution from which the set of labeled-text is drawn is identical to the distribution from which the text-to-be-labeled is drawn. Because of the rapidly changing nature of news stories, this may not hold true. In this paper, we present a graph-based\napproach to address the problem of explicitly capturing both strong and weak similarities within news stories over time and to use these efficiently for categorization. Our approach combines the paradigm of Min-Hashing and label propagation in graphs in a novel way. While Min-Hashing is well-understood in information retrieval applications, our application of it to create a temporal similarity graph appears to be new. Label propagation is gaining popularity in the field of machine learning as a technique for semi- supervised learning. Our approach to label propagation follows our previous work [4], where equivalent views of a basic algorithm termed Adsorption were established, and the technique was successfully employed for propagating weak information in extremely large graphs to create a video recommendation system for YouTube.\nThe aims of this paper are to present the following techniques that we anticipate will have general applicability for data mining in industrial settings: formulation of temporal similarities via graphs created using Min-Hashes, and the application of label propagation as an off-the-shelf tool for classification tasks when very little ground truth is available.\nThe next section describes the data collected and presents a series of experiments to develop strong, realistic, baselines for performance. Section 3 gives a detailed description of the Adsorption algorithm. Section 4 presents the empirical results to establish the Adsorption baselines for this task. Section 5 presents extensive results with tiny amounts of labeled data (e.g., a single labeled example). Section 6 concludes the paper and offers avenues for future exploration.', 'word_limit': '150'}
2017-12-03 08:50:54,648 - 5a23fceae3e52a606c387980
2017-12-03 08:50:54,648 - TextSummarizer ------------------------------------ Init
2017-12-03 08:50:54,649 - _id: 5a23fceae3e52a606c387980
2017-12-03 08:51:06,209 - {'content_title': 'Visualizing and Understanding Convolutional Networks', 'content_text': 'Since their introduction by (LeCun et al., 1989) in the early 1990’s, Convolutional Networks (convnets) have demonstrated excellent performance at tasks such as hand-written digit classification and face detection. In the last year, several papers have shown that they can also deliver outstanding performance on more challenging visual classification tasks. (Ciresan et al., 2012) demonstrate state-of-the-art performance on NORB and CIFAR-10 datasets. Most notably, (Krizhevsky et al., 2012) show record beating performance on the ImageNet 2012 classification benchmark, with their convnet model achieving an error rate of 16.4%, compared to the 2nd place result of 26.1%. Several factors are responsible for this renewed interest in convnet models: (i) the availability of much larger training sets, with millions of labeled examples; (ii) powerful GPU implementations, making the training of very large models practical and (iii) better model regularization strategies, such as Dropout.\nDespite this encouraging progress, there is still lit- tle insight into the internal operation and behavior of these complex models, or how they achieve such good performance. From a scientific standpoint, this is deeply unsatisfactory. Without clear understanding of how and why they work, the development of better models is reduced to trial-and-error. In this paper we introduce a visualization technique that reveals the input stimuli that excite individual feature maps at any layer in the model. It also allows us to observe the evolution of features during training and to diagnose potential problems with the model. The visualization technique we propose uses a multi-layered Deconvolutional Network (deconvnet), as proposed by (Zeiler et al., 2011), to project the feature activations back to the input pixel space. We also perform a sensitivity analysis of the classifier output by occluding portions of the input image, revealing which parts of the scene are important for classification.\nUsing these tools, we start with the architecture of (Krizhevsky et al., 2012) and explore different architectures, discovering ones that outperform their results on ImageNet. We then explore the generalization ability of the model to other datasets, just retraining the softmax classifier on top. As such, this is a form of supervised pre-training, which contrasts with the unsupervised pre-training methods popularized by (Hinton et al., 2006) and others (Bengio et al., 2007; Vincent et al., 2008). The generalization ability of convnet features is also explored in concurrent work by (Donahue et al., 2013).\nVisualizing features to gain intuition about the net- work is common practice, but mostly limited to the 1st layer where projections to pixel space are possible. In higher layers this is not the case, and there are limited methods for interpreting activity. (Erhan et al., 2009) find the optimal stimulus for each unit by performing gradient descent in image space to maximize the unit’s activation. This requires a careful initialization and does not give any information about the unit’s in- variances. Motivated by the latter’s short-coming, (Le et al., 2010) (extending an idea by (Berkes & Wiskott, 2006)) show how the Hessian of a given unit may be computed numerically around the optimal response, giving some insight into invariances. The problem is that for higher layers, the invariances are extremely complex so are poorly captured by a simple quadratic approximation. Our approach, by contrast, provides a non-parametric view of invariance, showing which pat- terns from the training set activate the feature map. (Donahue et al., 2013) show visualizations that iden- tify patches within a dataset that are responsible for strong activations at higher layers in the model. Our visualizations differ in that they are not just crops of input images, but rather top-down projections that reveal structures within each patch that stimulate a particular feature map.\nWe use standard fully supervised convnet models\nthroughout the paper, as defined by (LeCun et al.,\n1989) and (Krizhevsky et al., 2012). These models map a color 2D input image xi, via a series of layers, to a probability vector yˆ over the C different i classes. Each layer consists of (i) convolution of the previous layer output (or, in the case of the 1st layer, the input image) with a set of learned filters; (ii) pass- ing the responses through a rectified linear function (relu(x) = max(x,0)); (iii) [optionally] max pooling over local neighborhoods and (iv) [optionally] a lo- cal contrast operation that normalizes the responses across feature maps. For more details of these opera- tions, see (Krizhevsky et al., 2012) and (Jarrett et al., 2009). The top few layers of the network are conventional fully-connected networks and the final layer is a softmax classifier. Fig. 3 shows the model used in many of our experiments.\nWe train these models using a large set of N labeled images {x,y}, where label yi is a discrete variable indicating the true class. A cross-entropy loss function, suitable for image classification, is used to compare yˆ and y . The parameters of the network (filters in the convolutional layers, weight matrices in the fully-connected layers and biases) are trained by back- propagating the derivative of the loss with respect to the parameters throughout the network, and updating the parameters via stochastic gradient descent.\nUnderstanding the operation of a convnet requires interpreting the feature activity in intermediate layers. We present a novel way to map these activities back to the input pixel space, showing what input pattern orig- inally caused a given activation in the feature maps. We perform this mapping with a Deconvolutional Network (deconvnet) (Zeiler et al., 2011). A deconvnet can be thought of as a convnet model that uses the same components (filtering, pooling) but in reverse, so instead of mapping pixels to features does the opposite. In (Zeiler et al., 2011), deconvnets were proposed as a way of performing unsupervised learning. Here, they are not used in any learning capacity, just as a probe of an already trained convnet.\nTo examine a convnet, a deconvnet is attached to each of its layers, as illustrated in Fig. 1(top), providing a continuous path back to image pixels. To start, an input image is presented to the convnet and features computed throughout the layers. To examine a given convnet activation, we set all other activations in the layer to zero and pass the feature maps as input to the attached deconvnet layer. Then we successively (i) unpool, (ii) rectify and (iii) filter to reconstruct the activity in the layer beneath that gave rise to the chosen activation. This is then repeated until input pixel space is reached.\nUnpooling: In the convnet, the max pooling opera- tion is non-invertible, however we can obtain an ap- proximate inverse by recording the locations of the maxima within each pooling region in a set of switch variables. In the deconvnet, the unpooling operation uses these switches to place the reconstructions from the layer above into appropriate locations, preserving the structure of the stimulus. See Fig. 1(bottom) for an illustration of the procedure.\nRectification: The convnet uses relu non-linearities, which rectify the feature maps thus ensuring the feature maps are always positive. To obtain valid feature reconstructions at each layer (which also should be positive), we pass the reconstructed signal through a relu non-linearity.\nFiltering: The convnet uses learned filters to convolve the feature maps from the previous layer. To invert this, the deconvnet uses transposed versions of the same filters, but applied to the rectified maps, not the output of the layer beneath. In practice this means flipping each filter vertically and horizontally.\nProjecting down from higher layers uses the switch settings generated by the max pooling in the convnet on the way up. As these switch settings are peculiar to a given input image, the reconstruction obtained from a single activation thus resembles a small piece of the original input image, with structures weighted according to their contribution toward to the feature activation. Since the model is trained discriminatively, they implicitly show which parts of the input image are discriminative. Note that these projections are not samples from the model, since there is no generative process involved.', 'word_limit': '150'}
2017-12-03 08:51:06,317 - 5a23fe71e3e52a606c387981
2017-12-03 08:51:06,321 - TextSummarizer ------------------------------------ Init
2017-12-03 08:51:06,321 - _id: 5a23fe71e3e52a606c387981
2017-12-03 08:51:49,257 - Starting Flask Service......
2017-12-03 08:51:58,504 - CombinedMultiDict([ImmutableMultiDict([('q', '{}'), ('1512309118393', '')]), ImmutableMultiDict([])])
2017-12-03 08:51:58,505 - {}
2017-12-03 08:52:04,219 - {'content_title': "Google's AI Wins Pivotal Second Game in Match With Go Grandmaster", 'content_text': 'After more than four hours of tight play and a rapid-fire endgame, Google\'s artificially intelligent Go-playing computer system has won a second contest against grandmaster Lee Sedol, taking a two-games-to-none lead in their historic best-of-five match in downtown Seoul.  The surprisingly skillful Google machine, known as AlphaGo, now needs only one more win to claim victory in the match. The Korean-born Lee Sedol will go down in defeat unless he takes each of the match\'s last three games. Though machines have beaten the best humans at chess, checkers, Othello, Scrabble, Jeopardy!, and so many other games considered tests of human intellect, they have never beaten the very best at Go. Game Three is set for Saturday afternoon inside Seoul\'s Four Seasons hotel.  The match is a way of judging the suddenly rapid progress of artificial intelligence. One of the machine-learning techniques at the heart of AlphaGo has already reinvented myriad online services inside Google and other big-name Internet companies, helping to identify images, recognize commands spoken into smartphones, improve search engine results, and more. Meanwhile, another AlphaGo technique is now driving experimental robotics at Google and places like the University of California at Berkeley. This week\'s match can show how far these technologies have come - and perhaps how far they will go.  Created in Asia over 2,500 year ago, Go is exponentially more complex than chess, and at least among humans, it requires an added degree of intuition. Lee Sedol is widely-regarded as the top Go player of the last decade, after winning more international titles than all but one other player. He is currently ranked number five in the world, and according to Demis Hassabis, who leads DeepMind, the Google AI lab that created AlphaGo, his team chose the Korean for this all-important match because they wanted an opponent who would be remembered as one of history\'s great players.  Although AlphaGo topped Lee Sedol in the match\'s first game on Wednesday afternoon, the outcome of Game Two was no easier to predict. In his 1996 match with IBM\'s Deep Blue supercomputer, world chess champion Gary Kasparov lost the first game but then came back to win the second game and, eventually, the match as a whole. It wasn\'t until the following year that Deep Blue topped Kasparov over the course of a six-game contest. The thing to realize is that, after playing AlphaGo for the first time on Wednesday, Lee Sedol could adjust his style of play - just as Kasparov did back in 1996. But AlphaGo could not. Because this Google creation relies so heavily on machine learning techniques, the DeepMind team needs a good four to six weeks to train a new incarnation of the system. And that means they can\'t really change things during this eight-day match.  "This is about teaching and learning," Hassabis told us just before Game Two. "One game is not enough data to learn from - for a machine - and training takes an awful lot of time.', 'word_limit': '150'}
2017-12-03 08:52:04,329 - 5a22fa6ae3e52a40f793f587
2017-12-03 08:52:04,330 - TextSummarizer ------------------------------------ Init
2017-12-03 08:52:04,330 - _id: 5a22fa6ae3e52a40f793f587
2017-12-03 08:52:04,330 - word_limit: 150
2017-12-03 08:52:44,968 - Starting Flask Service......
2017-12-03 08:52:50,026 - CombinedMultiDict([ImmutableMultiDict([('q', '{}'), ('1512309168748', '')]), ImmutableMultiDict([])])
2017-12-03 08:52:50,026 - {}
2017-12-03 08:52:55,327 - {'content_title': "Google's AI Wins Pivotal Second Game in Match With Go Grandmaster", 'content_text': 'After more than four hours of tight play and a rapid-fire endgame, Google\'s artificially intelligent Go-playing computer system has won a second contest against grandmaster Lee Sedol, taking a two-games-to-none lead in their historic best-of-five match in downtown Seoul.  The surprisingly skillful Google machine, known as AlphaGo, now needs only one more win to claim victory in the match. The Korean-born Lee Sedol will go down in defeat unless he takes each of the match\'s last three games. Though machines have beaten the best humans at chess, checkers, Othello, Scrabble, Jeopardy!, and so many other games considered tests of human intellect, they have never beaten the very best at Go. Game Three is set for Saturday afternoon inside Seoul\'s Four Seasons hotel.  The match is a way of judging the suddenly rapid progress of artificial intelligence. One of the machine-learning techniques at the heart of AlphaGo has already reinvented myriad online services inside Google and other big-name Internet companies, helping to identify images, recognize commands spoken into smartphones, improve search engine results, and more. Meanwhile, another AlphaGo technique is now driving experimental robotics at Google and places like the University of California at Berkeley. This week\'s match can show how far these technologies have come - and perhaps how far they will go.  Created in Asia over 2,500 year ago, Go is exponentially more complex than chess, and at least among humans, it requires an added degree of intuition. Lee Sedol is widely-regarded as the top Go player of the last decade, after winning more international titles than all but one other player. He is currently ranked number five in the world, and according to Demis Hassabis, who leads DeepMind, the Google AI lab that created AlphaGo, his team chose the Korean for this all-important match because they wanted an opponent who would be remembered as one of history\'s great players.  Although AlphaGo topped Lee Sedol in the match\'s first game on Wednesday afternoon, the outcome of Game Two was no easier to predict. In his 1996 match with IBM\'s Deep Blue supercomputer, world chess champion Gary Kasparov lost the first game but then came back to win the second game and, eventually, the match as a whole. It wasn\'t until the following year that Deep Blue topped Kasparov over the course of a six-game contest. The thing to realize is that, after playing AlphaGo for the first time on Wednesday, Lee Sedol could adjust his style of play - just as Kasparov did back in 1996. But AlphaGo could not. Because this Google creation relies so heavily on machine learning techniques, the DeepMind team needs a good four to six weeks to train a new incarnation of the system. And that means they can\'t really change things during this eight-day match.  "This is about teaching and learning," Hassabis told us just before Game Two. "One game is not enough data to learn from - for a machine - and training takes an awful lot of time.', 'word_limit': '150'}
2017-12-03 08:52:55,448 - 5a22fa6ae3e52a40f793f587
2017-12-03 08:52:55,450 - TextSummarizer ------------------------------------ Init
2017-12-03 08:52:55,450 - _id: 5a22fa6ae3e52a40f793f587
2017-12-03 08:52:55,451 - word_limit: 150
2017-12-03 08:54:01,213 - Starting Flask Service......
2017-12-03 08:54:06,262 - CombinedMultiDict([ImmutableMultiDict([('q', '{}'), ('1512309245794', '')]), ImmutableMultiDict([])])
2017-12-03 08:54:06,262 - {}
2017-12-03 08:54:11,275 - {'content_title': "Google's AI Wins Pivotal Second Game in Match With Go Grandmaster", 'content_text': 'After more than four hours of tight play and a rapid-fire endgame, Google\'s artificially intelligent Go-playing computer system has won a second contest against grandmaster Lee Sedol, taking a two-games-to-none lead in their historic best-of-five match in downtown Seoul.  The surprisingly skillful Google machine, known as AlphaGo, now needs only one more win to claim victory in the match. The Korean-born Lee Sedol will go down in defeat unless he takes each of the match\'s last three games. Though machines have beaten the best humans at chess, checkers, Othello, Scrabble, Jeopardy!, and so many other games considered tests of human intellect, they have never beaten the very best at Go. Game Three is set for Saturday afternoon inside Seoul\'s Four Seasons hotel.  The match is a way of judging the suddenly rapid progress of artificial intelligence. One of the machine-learning techniques at the heart of AlphaGo has already reinvented myriad online services inside Google and other big-name Internet companies, helping to identify images, recognize commands spoken into smartphones, improve search engine results, and more. Meanwhile, another AlphaGo technique is now driving experimental robotics at Google and places like the University of California at Berkeley. This week\'s match can show how far these technologies have come - and perhaps how far they will go.  Created in Asia over 2,500 year ago, Go is exponentially more complex than chess, and at least among humans, it requires an added degree of intuition. Lee Sedol is widely-regarded as the top Go player of the last decade, after winning more international titles than all but one other player. He is currently ranked number five in the world, and according to Demis Hassabis, who leads DeepMind, the Google AI lab that created AlphaGo, his team chose the Korean for this all-important match because they wanted an opponent who would be remembered as one of history\'s great players.  Although AlphaGo topped Lee Sedol in the match\'s first game on Wednesday afternoon, the outcome of Game Two was no easier to predict. In his 1996 match with IBM\'s Deep Blue supercomputer, world chess champion Gary Kasparov lost the first game but then came back to win the second game and, eventually, the match as a whole. It wasn\'t until the following year that Deep Blue topped Kasparov over the course of a six-game contest. The thing to realize is that, after playing AlphaGo for the first time on Wednesday, Lee Sedol could adjust his style of play - just as Kasparov did back in 1996. But AlphaGo could not. Because this Google creation relies so heavily on machine learning techniques, the DeepMind team needs a good four to six weeks to train a new incarnation of the system. And that means they can\'t really change things during this eight-day match.  "This is about teaching and learning," Hassabis told us just before Game Two. "One game is not enough data to learn from - for a machine - and training takes an awful lot of time.', 'word_limit': '150'}
2017-12-03 08:54:11,384 - 5a22fa6ae3e52a40f793f587
2017-12-03 08:54:11,385 - TextSummarizer ------------------------------------ Init
2017-12-03 08:54:11,385 - _id: 5a22fa6ae3e52a40f793f587
2017-12-03 08:54:50,516 - Starting Flask Service......
2017-12-03 08:54:55,589 - {'content_title': "Google's AI Wins Pivotal Second Game in Match With Go Grandmaster", 'content_text': 'After more than four hours of tight play and a rapid-fire endgame, Google\'s artificially intelligent Go-playing computer system has won a second contest against grandmaster Lee Sedol, taking a two-games-to-none lead in their historic best-of-five match in downtown Seoul.  The surprisingly skillful Google machine, known as AlphaGo, now needs only one more win to claim victory in the match. The Korean-born Lee Sedol will go down in defeat unless he takes each of the match\'s last three games. Though machines have beaten the best humans at chess, checkers, Othello, Scrabble, Jeopardy!, and so many other games considered tests of human intellect, they have never beaten the very best at Go. Game Three is set for Saturday afternoon inside Seoul\'s Four Seasons hotel.  The match is a way of judging the suddenly rapid progress of artificial intelligence. One of the machine-learning techniques at the heart of AlphaGo has already reinvented myriad online services inside Google and other big-name Internet companies, helping to identify images, recognize commands spoken into smartphones, improve search engine results, and more. Meanwhile, another AlphaGo technique is now driving experimental robotics at Google and places like the University of California at Berkeley. This week\'s match can show how far these technologies have come - and perhaps how far they will go.  Created in Asia over 2,500 year ago, Go is exponentially more complex than chess, and at least among humans, it requires an added degree of intuition. Lee Sedol is widely-regarded as the top Go player of the last decade, after winning more international titles than all but one other player. He is currently ranked number five in the world, and according to Demis Hassabis, who leads DeepMind, the Google AI lab that created AlphaGo, his team chose the Korean for this all-important match because they wanted an opponent who would be remembered as one of history\'s great players.  Although AlphaGo topped Lee Sedol in the match\'s first game on Wednesday afternoon, the outcome of Game Two was no easier to predict. In his 1996 match with IBM\'s Deep Blue supercomputer, world chess champion Gary Kasparov lost the first game but then came back to win the second game and, eventually, the match as a whole. It wasn\'t until the following year that Deep Blue topped Kasparov over the course of a six-game contest. The thing to realize is that, after playing AlphaGo for the first time on Wednesday, Lee Sedol could adjust his style of play - just as Kasparov did back in 1996. But AlphaGo could not. Because this Google creation relies so heavily on machine learning techniques, the DeepMind team needs a good four to six weeks to train a new incarnation of the system. And that means they can\'t really change things during this eight-day match.  "This is about teaching and learning," Hassabis told us just before Game Two. "One game is not enough data to learn from - for a machine - and training takes an awful lot of time.', 'word_limit': '150'}
2017-12-03 08:54:55,650 - 5a22fa6ae3e52a40f793f587
2017-12-03 08:54:55,651 - TextSummarizer ------------------------------------ Init
2017-12-03 08:54:55,652 - _id: 5a22fa6ae3e52a40f793f587
2017-12-03 08:54:55,652 - word_limit: 150
2017-12-03 08:55:14,020 - {'content_title': 'Visualizing and Understanding Convolutional Networks', 'content_text': 'Since their introduction by (LeCun et al., 1989) in the early 1990’s, Convolutional Networks (convnets) have demonstrated excellent performance at tasks such as hand-written digit classification and face detection. In the last year, several papers have shown that they can also deliver outstanding performance on more challenging visual classification tasks. (Ciresan et al., 2012) demonstrate state-of-the-art performance on NORB and CIFAR-10 datasets. Most notably, (Krizhevsky et al., 2012) show record beating performance on the ImageNet 2012 classification benchmark, with their convnet model achieving an error rate of 16.4%, compared to the 2nd place result of 26.1%. Several factors are responsible for this renewed interest in convnet models: (i) the availability of much larger training sets, with millions of labeled examples; (ii) powerful GPU implementations, making the training of very large models practical and (iii) better model regularization strategies, such as Dropout.\nDespite this encouraging progress, there is still lit- tle insight into the internal operation and behavior of these complex models, or how they achieve such good performance. From a scientific standpoint, this is deeply unsatisfactory. Without clear understanding of how and why they work, the development of better models is reduced to trial-and-error. In this paper we introduce a visualization technique that reveals the input stimuli that excite individual feature maps at any layer in the model. It also allows us to observe the evolution of features during training and to diagnose potential problems with the model. The visualization technique we propose uses a multi-layered Deconvolutional Network (deconvnet), as proposed by (Zeiler et al., 2011), to project the feature activations back to the input pixel space. We also perform a sensitivity analysis of the classifier output by occluding portions of the input image, revealing which parts of the scene are important for classification.\nUsing these tools, we start with the architecture of (Krizhevsky et al., 2012) and explore different architectures, discovering ones that outperform their results on ImageNet. We then explore the generalization ability of the model to other datasets, just retraining the softmax classifier on top. As such, this is a form of supervised pre-training, which contrasts with the unsupervised pre-training methods popularized by (Hinton et al., 2006) and others (Bengio et al., 2007; Vincent et al., 2008). The generalization ability of convnet features is also explored in concurrent work by (Donahue et al., 2013).\nVisualizing features to gain intuition about the net- work is common practice, but mostly limited to the 1st layer where projections to pixel space are possible. In higher layers this is not the case, and there are limited methods for interpreting activity. (Erhan et al., 2009) find the optimal stimulus for each unit by performing gradient descent in image space to maximize the unit’s activation. This requires a careful initialization and does not give any information about the unit’s in- variances. Motivated by the latter’s short-coming, (Le et al., 2010) (extending an idea by (Berkes & Wiskott, 2006)) show how the Hessian of a given unit may be computed numerically around the optimal response, giving some insight into invariances. The problem is that for higher layers, the invariances are extremely complex so are poorly captured by a simple quadratic approximation. Our approach, by contrast, provides a non-parametric view of invariance, showing which pat- terns from the training set activate the feature map. (Donahue et al., 2013) show visualizations that iden- tify patches within a dataset that are responsible for strong activations at higher layers in the model. Our visualizations differ in that they are not just crops of input images, but rather top-down projections that reveal structures within each patch that stimulate a particular feature map.\nWe use standard fully supervised convnet models\nthroughout the paper, as defined by (LeCun et al.,\n1989) and (Krizhevsky et al., 2012). These models map a color 2D input image xi, via a series of layers, to a probability vector yˆ over the C different i classes. Each layer consists of (i) convolution of the previous layer output (or, in the case of the 1st layer, the input image) with a set of learned filters; (ii) pass- ing the responses through a rectified linear function (relu(x) = max(x,0)); (iii) [optionally] max pooling over local neighborhoods and (iv) [optionally] a lo- cal contrast operation that normalizes the responses across feature maps. For more details of these opera- tions, see (Krizhevsky et al., 2012) and (Jarrett et al., 2009). The top few layers of the network are conventional fully-connected networks and the final layer is a softmax classifier. Fig. 3 shows the model used in many of our experiments.\nWe train these models using a large set of N labeled images {x,y}, where label yi is a discrete variable indicating the true class. A cross-entropy loss function, suitable for image classification, is used to compare yˆ and y . The parameters of the network (filters in the convolutional layers, weight matrices in the fully-connected layers and biases) are trained by back- propagating the derivative of the loss with respect to the parameters throughout the network, and updating the parameters via stochastic gradient descent.\nUnderstanding the operation of a convnet requires interpreting the feature activity in intermediate layers. We present a novel way to map these activities back to the input pixel space, showing what input pattern orig- inally caused a given activation in the feature maps. We perform this mapping with a Deconvolutional Network (deconvnet) (Zeiler et al., 2011). A deconvnet can be thought of as a convnet model that uses the same components (filtering, pooling) but in reverse, so instead of mapping pixels to features does the opposite. In (Zeiler et al., 2011), deconvnets were proposed as a way of performing unsupervised learning. Here, they are not used in any learning capacity, just as a probe of an already trained convnet.\nTo examine a convnet, a deconvnet is attached to each of its layers, as illustrated in Fig. 1(top), providing a continuous path back to image pixels. To start, an input image is presented to the convnet and features computed throughout the layers. To examine a given convnet activation, we set all other activations in the layer to zero and pass the feature maps as input to the attached deconvnet layer. Then we successively (i) unpool, (ii) rectify and (iii) filter to reconstruct the activity in the layer beneath that gave rise to the chosen activation. This is then repeated until input pixel space is reached.\nUnpooling: In the convnet, the max pooling opera- tion is non-invertible, however we can obtain an ap- proximate inverse by recording the locations of the maxima within each pooling region in a set of switch variables. In the deconvnet, the unpooling operation uses these switches to place the reconstructions from the layer above into appropriate locations, preserving the structure of the stimulus. See Fig. 1(bottom) for an illustration of the procedure.\nRectification: The convnet uses relu non-linearities, which rectify the feature maps thus ensuring the feature maps are always positive. To obtain valid feature reconstructions at each layer (which also should be positive), we pass the reconstructed signal through a relu non-linearity.\nFiltering: The convnet uses learned filters to convolve the feature maps from the previous layer. To invert this, the deconvnet uses transposed versions of the same filters, but applied to the rectified maps, not the output of the layer beneath. In practice this means flipping each filter vertically and horizontally.\nProjecting down from higher layers uses the switch settings generated by the max pooling in the convnet on the way up. As these switch settings are peculiar to a given input image, the reconstruction obtained from a single activation thus resembles a small piece of the original input image, with structures weighted according to their contribution toward to the feature activation. Since the model is trained discriminatively, they implicitly show which parts of the input image are discriminative. Note that these projections are not samples from the model, since there is no generative process involved.', 'word_limit': '200'}
2017-12-03 08:55:14,129 - 5a23fe71e3e52a606c387981
2017-12-03 08:55:14,135 - TextSummarizer ------------------------------------ Init
2017-12-03 08:55:14,135 - _id: 5a23fe71e3e52a606c387981
2017-12-03 08:55:14,135 - word_limit: 200
2017-12-03 08:55:40,457 - {'content_title': 'Visualizing and Understanding Convolutional Networks', 'content_text': 'Since their introduction by (LeCun et al., 1989) in the early 1990’s, Convolutional Networks (convnets) have demonstrated excellent performance at tasks such as hand-written digit classification and face detection. In the last year, several papers have shown that they can also deliver outstanding performance on more challenging visual classification tasks. (Ciresan et al., 2012) demonstrate state-of-the-art performance on NORB and CIFAR-10 datasets. Most notably, (Krizhevsky et al., 2012) show record beating performance on the ImageNet 2012 classification benchmark, with their convnet model achieving an error rate of 16.4%, compared to the 2nd place result of 26.1%. Several factors are responsible for this renewed interest in convnet models: (i) the availability of much larger training sets, with millions of labeled examples; (ii) powerful GPU implementations, making the training of very large models practical and (iii) better model regularization strategies, such as Dropout.\nDespite this encouraging progress, there is still lit- tle insight into the internal operation and behavior of these complex models, or how they achieve such good performance. From a scientific standpoint, this is deeply unsatisfactory. Without clear understanding of how and why they work, the development of better models is reduced to trial-and-error. In this paper we introduce a visualization technique that reveals the input stimuli that excite individual feature maps at any layer in the model. It also allows us to observe the evolution of features during training and to diagnose potential problems with the model. The visualization technique we propose uses a multi-layered Deconvolutional Network (deconvnet), as proposed by (Zeiler et al., 2011), to project the feature activations back to the input pixel space. We also perform a sensitivity analysis of the classifier output by occluding portions of the input image, revealing which parts of the scene are important for classification.\nUsing these tools, we start with the architecture of (Krizhevsky et al., 2012) and explore different architectures, discovering ones that outperform their results on ImageNet. We then explore the generalization ability of the model to other datasets, just retraining the softmax classifier on top. As such, this is a form of supervised pre-training, which contrasts with the unsupervised pre-training methods popularized by (Hinton et al., 2006) and others (Bengio et al., 2007; Vincent et al., 2008). The generalization ability of convnet features is also explored in concurrent work by (Donahue et al., 2013).\nVisualizing features to gain intuition about the net- work is common practice, but mostly limited to the 1st layer where projections to pixel space are possible. In higher layers this is not the case, and there are limited methods for interpreting activity. (Erhan et al., 2009) find the optimal stimulus for each unit by performing gradient descent in image space to maximize the unit’s activation. This requires a careful initialization and does not give any information about the unit’s in- variances. Motivated by the latter’s short-coming, (Le et al., 2010) (extending an idea by (Berkes & Wiskott, 2006)) show how the Hessian of a given unit may be computed numerically around the optimal response, giving some insight into invariances. The problem is that for higher layers, the invariances are extremely complex so are poorly captured by a simple quadratic approximation. Our approach, by contrast, provides a non-parametric view of invariance, showing which pat- terns from the training set activate the feature map. (Donahue et al., 2013) show visualizations that iden- tify patches within a dataset that are responsible for strong activations at higher layers in the model. Our visualizations differ in that they are not just crops of input images, but rather top-down projections that reveal structures within each patch that stimulate a particular feature map.\nWe use standard fully supervised convnet models\nthroughout the paper, as defined by (LeCun et al.,\n1989) and (Krizhevsky et al., 2012). These models map a color 2D input image xi, via a series of layers, to a probability vector yˆ over the C different i classes. Each layer consists of (i) convolution of the previous layer output (or, in the case of the 1st layer, the input image) with a set of learned filters; (ii) pass- ing the responses through a rectified linear function (relu(x) = max(x,0)); (iii) [optionally] max pooling over local neighborhoods and (iv) [optionally] a lo- cal contrast operation that normalizes the responses across feature maps. For more details of these opera- tions, see (Krizhevsky et al., 2012) and (Jarrett et al., 2009). The top few layers of the network are conventional fully-connected networks and the final layer is a softmax classifier. Fig. 3 shows the model used in many of our experiments.\nWe train these models using a large set of N labeled images {x,y}, where label yi is a discrete variable indicating the true class. A cross-entropy loss function, suitable for image classification, is used to compare yˆ and y . The parameters of the network (filters in the convolutional layers, weight matrices in the fully-connected layers and biases) are trained by back- propagating the derivative of the loss with respect to the parameters throughout the network, and updating the parameters via stochastic gradient descent.\nUnderstanding the operation of a convnet requires interpreting the feature activity in intermediate layers. We present a novel way to map these activities back to the input pixel space, showing what input pattern orig- inally caused a given activation in the feature maps. We perform this mapping with a Deconvolutional Network (deconvnet) (Zeiler et al., 2011). A deconvnet can be thought of as a convnet model that uses the same components (filtering, pooling) but in reverse, so instead of mapping pixels to features does the opposite. In (Zeiler et al., 2011), deconvnets were proposed as a way of performing unsupervised learning. Here, they are not used in any learning capacity, just as a probe of an already trained convnet.\nTo examine a convnet, a deconvnet is attached to each of its layers, as illustrated in Fig. 1(top), providing a continuous path back to image pixels. To start, an input image is presented to the convnet and features computed throughout the layers. To examine a given convnet activation, we set all other activations in the layer to zero and pass the feature maps as input to the attached deconvnet layer. Then we successively (i) unpool, (ii) rectify and (iii) filter to reconstruct the activity in the layer beneath that gave rise to the chosen activation. This is then repeated until input pixel space is reached.\nUnpooling: In the convnet, the max pooling opera- tion is non-invertible, however we can obtain an ap- proximate inverse by recording the locations of the maxima within each pooling region in a set of switch variables. In the deconvnet, the unpooling operation uses these switches to place the reconstructions from the layer above into appropriate locations, preserving the structure of the stimulus. See Fig. 1(bottom) for an illustration of the procedure.\nRectification: The convnet uses relu non-linearities, which rectify the feature maps thus ensuring the feature maps are always positive. To obtain valid feature reconstructions at each layer (which also should be positive), we pass the reconstructed signal through a relu non-linearity.\nFiltering: The convnet uses learned filters to convolve the feature maps from the previous layer. To invert this, the deconvnet uses transposed versions of the same filters, but applied to the rectified maps, not the output of the layer beneath. In practice this means flipping each filter vertically and horizontally.\nProjecting down from higher layers uses the switch settings generated by the max pooling in the convnet on the way up. As these switch settings are peculiar to a given input image, the reconstruction obtained from a single activation thus resembles a small piece of the original input image, with structures weighted according to their contribution toward to the feature activation. Since the model is trained discriminatively, they implicitly show which parts of the input image are discriminative. Note that these projections are not samples from the model, since there is no generative process involved.', 'word_limit': '300'}
2017-12-03 08:55:40,566 - 5a23fe71e3e52a606c387981
2017-12-03 08:55:40,573 - TextSummarizer ------------------------------------ Init
2017-12-03 08:55:40,573 - _id: 5a23fe71e3e52a606c387981
2017-12-03 08:55:40,573 - word_limit: 300
2017-12-03 08:55:53,467 - CombinedMultiDict([ImmutableMultiDict([('q', '{}'), ('1512309353358', '')]), ImmutableMultiDict([])])
2017-12-03 08:55:53,467 - {}
2017-12-03 08:56:32,063 - CombinedMultiDict([ImmutableMultiDict([('q', '{}'), ('1512309391950', '')]), ImmutableMultiDict([])])
2017-12-03 08:56:32,063 - {}
2017-12-03 08:57:01,301 - CombinedMultiDict([ImmutableMultiDict([('q', '{}'), ('1512309421187', '')]), ImmutableMultiDict([])])
2017-12-03 08:57:01,302 - {}
2017-12-03 10:18:49,481 - Starting Flask Service......
2017-12-03 10:19:06,175 - CombinedMultiDict([ImmutableMultiDict([('q', '{}'), ('1512314346065', '')]), ImmutableMultiDict([])])
2017-12-03 10:19:06,176 - {}
2017-12-03 10:20:08,584 - CombinedMultiDict([ImmutableMultiDict([('q', '{}'), ('1512314408474', '')]), ImmutableMultiDict([])])
2017-12-03 10:20:08,585 - {}
2017-12-03 10:20:18,315 - {'review_text': 'I love this movie!'}
2017-12-03 10:20:18,373 - <pymongo.results.InsertOneResult object at 0x118a47900>
2017-12-03 10:20:18,377 - 5a241632e3e52a67cca60498
2017-12-03 10:20:18,494 - 5a241632e3e52a67cca60498
2017-12-03 10:20:18,495 - MovieClassifier ------------------------------------ Init
2017-12-03 10:26:01,588 - CombinedMultiDict([ImmutableMultiDict([('q', '{}'), ('1512314761476', '')]), ImmutableMultiDict([])])
2017-12-03 10:26:01,589 - {}
2017-12-03 10:26:44,334 - CombinedMultiDict([ImmutableMultiDict([('q', '{}'), ('1512314804223', '')]), ImmutableMultiDict([])])
2017-12-03 10:26:44,335 - {}
2017-12-03 10:27:06,826 - CombinedMultiDict([ImmutableMultiDict([('q', '{}'), ('1512314826711', '')]), ImmutableMultiDict([])])
2017-12-03 10:27:06,826 - {}
2017-12-03 10:29:08,782 - CombinedMultiDict([ImmutableMultiDict([('q', '{}'), ('1512314948650', '')]), ImmutableMultiDict([])])
2017-12-03 10:29:08,782 - {}
2017-12-03 10:29:47,496 - CombinedMultiDict([ImmutableMultiDict([('q', '{}'), ('1512314987383', '')]), ImmutableMultiDict([])])
2017-12-03 10:29:47,497 - {}
2017-12-03 10:35:26,924 - Starting Flask Service......
2017-12-03 10:35:36,455 - CombinedMultiDict([ImmutableMultiDict([('q', '{}'), ('1512315336345', '')]), ImmutableMultiDict([])])
2017-12-03 10:35:36,456 - {}
2017-12-03 10:35:39,643 - {'review_text': 'I love this movie!'}
2017-12-03 10:35:39,752 - 5a241632e3e52a67cca60498
2017-12-03 10:35:39,753 - MovieClassifier ------------------------------------ Init
2017-12-03 10:35:40,191 - _id: 5a241632e3e52a67cca60498
2017-12-03 10:40:40,454 - Starting Flask Service......
2017-12-03 10:40:54,671 - CombinedMultiDict([ImmutableMultiDict([('q', '{}'), ('1512315654555', '')]), ImmutableMultiDict([])])
2017-12-03 10:40:54,671 - {}
2017-12-03 10:40:59,676 - {'review_text': 'I love this movie!'}
2017-12-03 10:40:59,785 - 5a241632e3e52a67cca60498
2017-12-03 10:40:59,786 - MovieClassifier ------------------------------------ Init
2017-12-03 10:40:59,896 - _id: 5a241632e3e52a67cca60498
2017-12-03 10:42:48,435 - Starting Flask Service......
2017-12-03 10:42:54,553 - {'review_text': 'I love this movie!'}
2017-12-03 10:42:54,673 - 5a241632e3e52a67cca60498
2017-12-03 10:42:54,674 - MovieClassifier ------------------------------------ Init
2017-12-03 10:42:54,796 - _id: 5a241632e3e52a67cca60498
2017-12-03 10:42:54,797 - y: 1
2017-12-03 10:42:54,798 - proba: 0.825196747848
2017-12-03 10:44:06,443 - Starting Flask Service......
2017-12-03 10:44:11,750 - {'review_text': 'I love this movie!'}
2017-12-03 10:44:11,860 - 5a241632e3e52a67cca60498
2017-12-03 10:44:11,861 - MovieClassifier ------------------------------------ Init
2017-12-03 10:44:11,988 - _id: 5a241632e3e52a67cca60498
2017-12-03 10:44:11,989 - y: 1
2017-12-03 10:44:11,989 - proba: 0.825196747848
2017-12-03 10:44:46,418 - Starting Flask Service......
2017-12-03 10:44:51,961 - {'review_text': 'I love this movie!'}
2017-12-03 10:44:52,070 - 5a241632e3e52a67cca60498
2017-12-03 10:44:52,071 - MovieClassifier ------------------------------------ Init
2017-12-03 10:44:52,180 - _id: 5a241632e3e52a67cca60498
2017-12-03 10:44:52,181 - y: 1
2017-12-03 10:44:52,181 - proba: 0.825196747848
2017-12-03 10:45:15,126 - Starting Flask Service......
2017-12-03 10:45:24,476 - {'review_text': 'I love this movie!'}
2017-12-03 10:45:24,587 - 5a241632e3e52a67cca60498
2017-12-03 10:45:24,594 - MovieClassifier ------------------------------------ Init
2017-12-03 10:45:24,720 - _id: 5a241632e3e52a67cca60498
2017-12-03 10:45:24,721 - y: 1
2017-12-03 10:45:24,721 - proba: 0.825196747848
2017-12-03 10:45:39,981 - CombinedMultiDict([ImmutableMultiDict([('q', '{}'), ('1512315939871', '')]), ImmutableMultiDict([])])
2017-12-03 10:45:39,981 - {}
2017-12-03 10:45:43,228 - {'review_text': 'I love this movie!'}
2017-12-03 10:45:43,336 - 5a241632e3e52a67cca60498
2017-12-03 10:45:43,337 - MovieClassifier ------------------------------------ Init
2017-12-03 10:45:43,369 - _id: 5a241632e3e52a67cca60498
2017-12-03 10:45:43,370 - y: 1
2017-12-03 10:45:43,370 - proba: 0.825196747848
2017-12-03 11:00:36,558 - CombinedMultiDict([ImmutableMultiDict([('q', '{}'), ('1512316836446', '')]), ImmutableMultiDict([])])
2017-12-03 11:00:36,558 - {}
2017-12-03 11:04:15,770 - CombinedMultiDict([ImmutableMultiDict([('q', '{}'), ('1512317055658', '')]), ImmutableMultiDict([])])
2017-12-03 11:04:15,770 - {}
2017-12-03 11:04:18,668 - {'review_text': 'I love this movie!'}
2017-12-03 11:04:18,778 - 5a241632e3e52a67cca60498
2017-12-03 11:04:18,779 - MovieClassifier ------------------------------------ Init
2017-12-03 11:04:18,806 - _id: 5a241632e3e52a67cca60498
2017-12-03 11:04:18,807 - y: 1
2017-12-03 11:04:18,807 - proba: 0.825196747848
2017-12-03 11:06:40,152 - CombinedMultiDict([ImmutableMultiDict([('q', '{}'), ('1512317200039', '')]), ImmutableMultiDict([])])
2017-12-03 11:06:40,153 - {}
2017-12-03 11:06:42,637 - {'review_text': 'I love this movie!'}
2017-12-03 11:06:42,746 - 5a241632e3e52a67cca60498
2017-12-03 11:06:42,748 - MovieClassifier ------------------------------------ Init
2017-12-03 11:06:42,781 - _id: 5a241632e3e52a67cca60498
2017-12-03 11:06:42,782 - y: 1
2017-12-03 11:06:42,782 - proba: 0.825196747848
2017-12-03 11:08:14,203 - CombinedMultiDict([ImmutableMultiDict([('q', '{}'), ('1512317294092', '')]), ImmutableMultiDict([])])
2017-12-03 11:08:14,203 - {}
2017-12-03 11:08:22,756 - {'review_text': 'I love this movie!'}
2017-12-03 11:08:22,867 - 5a241632e3e52a67cca60498
2017-12-03 11:08:22,868 - MovieClassifier ------------------------------------ Init
2017-12-03 11:08:22,903 - _id: 5a241632e3e52a67cca60498
2017-12-03 11:08:22,904 - y: 1
2017-12-03 11:08:22,904 - proba: 0.825196747848
2017-12-03 11:09:01,548 - CombinedMultiDict([ImmutableMultiDict([('q', '{}'), ('1512317341436', '')]), ImmutableMultiDict([])])
2017-12-03 11:09:01,549 - {}
2017-12-03 11:09:39,723 - Starting Flask Service......
2017-12-03 11:09:44,761 - CombinedMultiDict([ImmutableMultiDict([('q', '{}'), ('1512317383086', '')]), ImmutableMultiDict([])])
2017-12-03 11:09:44,761 - {}
2017-12-03 11:09:48,344 - {'review_text': 'I love this movie!'}
2017-12-03 11:09:48,453 - 5a241632e3e52a67cca60498
2017-12-03 11:09:48,454 - MovieClassifier ------------------------------------ Init
2017-12-03 11:09:48,589 - _id: 5a241632e3e52a67cca60498
2017-12-03 11:09:48,590 - y: 1
2017-12-03 11:09:48,591 - proba: 0.825196747848
2017-12-03 11:10:57,144 - CombinedMultiDict([ImmutableMultiDict([('q', '{}'), ('1512317457033', '')]), ImmutableMultiDict([])])
2017-12-03 11:10:57,144 - {}
2017-12-03 11:11:51,870 - {'review_text': 'I did not like this movie'}
2017-12-03 11:11:51,871 - <pymongo.results.InsertOneResult object at 0x11092c048>
2017-12-03 11:11:51,871 - 5a242247e3e52a6de0092970
2017-12-03 11:11:51,985 - 5a242247e3e52a6de0092970
2017-12-03 11:11:51,991 - MovieClassifier ------------------------------------ Init
2017-12-03 11:11:52,019 - _id: 5a242247e3e52a6de0092970
2017-12-03 11:11:52,020 - y: 0
2017-12-03 11:11:52,020 - proba: 0.586085999751
2017-12-03 11:12:23,031 - CombinedMultiDict([ImmutableMultiDict([('q', '{}'), ('1512317542922', '')]), ImmutableMultiDict([])])
2017-12-03 11:12:23,031 - {}
2017-12-03 11:13:37,901 - CombinedMultiDict([ImmutableMultiDict([('q', '{}'), ('1512317617788', '')]), ImmutableMultiDict([])])
2017-12-03 11:13:37,901 - {}
2017-12-03 11:14:15,784 - {'review_text': 'The movie was ok'}
2017-12-03 11:14:15,785 - <pymongo.results.InsertOneResult object at 0x1109361b0>
2017-12-03 11:14:15,785 - 5a2422d7e3e52a6de0092971
2017-12-03 11:14:15,894 - 5a2422d7e3e52a6de0092971
2017-12-03 11:14:15,906 - MovieClassifier ------------------------------------ Init
2017-12-03 11:14:15,928 - _id: 5a2422d7e3e52a6de0092971
2017-12-03 11:14:15,929 - y: 0
2017-12-03 11:14:15,929 - proba: 0.769377399155
2017-12-03 11:15:18,703 - {'review_text': 'Great actors but really bad storyline'}
2017-12-03 11:15:18,704 - <pymongo.results.InsertOneResult object at 0x110936708>
2017-12-03 11:15:18,704 - 5a242316e3e52a6de0092972
2017-12-03 11:15:18,811 - 5a242316e3e52a6de0092972
2017-12-03 11:15:18,814 - MovieClassifier ------------------------------------ Init
2017-12-03 11:15:18,856 - _id: 5a242316e3e52a6de0092972
2017-12-03 11:15:18,856 - y: 0
2017-12-03 11:15:18,856 - proba: 0.74360345182
2017-12-03 11:16:03,185 - CombinedMultiDict([ImmutableMultiDict([('q', '{}'), ('1512317763072', '')]), ImmutableMultiDict([])])
2017-12-03 11:16:03,185 - {}
2017-12-03 11:16:08,116 - {'review_text': 'Great actors but really bad storyline'}
2017-12-03 11:16:08,227 - 5a242316e3e52a6de0092972
2017-12-03 11:16:08,230 - MovieClassifier ------------------------------------ Init
2017-12-03 11:16:08,248 - _id: 5a242316e3e52a6de0092972
2017-12-03 11:16:08,249 - y: 0
2017-12-03 11:16:08,249 - proba: 0.74360345182
2017-12-03 11:16:30,785 - CombinedMultiDict([ImmutableMultiDict([('q', '{}'), ('1512317790674', '')]), ImmutableMultiDict([])])
2017-12-03 11:16:30,786 - {}
2017-12-03 11:16:48,438 - {'review_text': 'I liked the story but not the move'}
2017-12-03 11:16:48,439 - <pymongo.results.InsertOneResult object at 0x10d7a5510>
2017-12-03 11:16:48,440 - 5a242370e3e52a6de0092973
2017-12-03 11:16:48,549 - 5a242370e3e52a6de0092973
2017-12-03 11:16:48,555 - MovieClassifier ------------------------------------ Init
2017-12-03 11:16:48,579 - _id: 5a242370e3e52a6de0092973
2017-12-03 11:16:48,580 - y: 1
2017-12-03 11:16:48,580 - proba: 0.75758597038
2017-12-03 11:16:56,861 - {'review_text': 'I liked the story but not the movie'}
2017-12-03 11:16:56,970 - 5a242370e3e52a6de0092973
2017-12-03 11:16:56,974 - MovieClassifier ------------------------------------ Init
2017-12-03 11:16:57,004 - _id: 5a242370e3e52a6de0092973
2017-12-03 11:16:57,005 - y: 1
2017-12-03 11:16:57,006 - proba: 0.733321291402
2017-12-03 11:19:01,343 - {'review_text': 'I love this movie!'}
2017-12-03 11:19:01,452 - 5a241632e3e52a67cca60498
2017-12-03 11:19:01,453 - MovieClassifier ------------------------------------ Init
2017-12-03 11:19:01,472 - _id: 5a241632e3e52a67cca60498
2017-12-03 11:19:01,473 - y: 1
2017-12-03 11:19:01,473 - proba: 0.825196747848
2017-12-03 11:23:39,846 - CombinedMultiDict([ImmutableMultiDict([('q', '{}'), ('1512318219734', '')]), ImmutableMultiDict([])])
2017-12-03 11:23:39,848 - {}
2017-12-03 11:23:42,480 - {'review_text': 'I love this movie!'}
2017-12-03 11:23:42,590 - 5a241632e3e52a67cca60498
2017-12-03 11:23:42,591 - MovieClassifier ------------------------------------ Init
2017-12-03 11:23:42,624 - _id: 5a241632e3e52a67cca60498
2017-12-03 11:23:42,625 - y: 1
2017-12-03 11:23:42,625 - proba: 0.825196747848
2017-12-03 11:24:14,810 - CombinedMultiDict([ImmutableMultiDict([('q', '{}'), ('1512318254698', '')]), ImmutableMultiDict([])])
2017-12-03 11:24:14,810 - {}
2017-12-03 11:24:17,276 - {'review_text': 'I love this movie!'}
2017-12-03 11:24:17,387 - 5a241632e3e52a67cca60498
2017-12-03 11:24:17,388 - MovieClassifier ------------------------------------ Init
2017-12-03 11:24:17,411 - _id: 5a241632e3e52a67cca60498
2017-12-03 11:24:17,412 - y: 1
2017-12-03 11:24:17,412 - proba: 0.825196747848
2017-12-03 11:37:20,217 - Starting Flask Service......
2017-12-03 11:37:25,329 - CombinedMultiDict([ImmutableMultiDict([('q', '{}'), ('1512319043090', '')]), ImmutableMultiDict([])])
2017-12-03 11:37:25,339 - {}
2017-12-03 11:37:36,671 - {'review_text': 'I liked the story but not the movie'}
2017-12-03 11:37:36,780 - 5a242370e3e52a6de0092973
2017-12-03 11:37:36,781 - MovieClassifier ------------------------------------ Init
2017-12-03 11:37:36,918 - _id: 5a242370e3e52a6de0092973
2017-12-03 11:37:36,919 - y: 1
2017-12-03 11:37:36,919 - proba: 0.733321291402
2017-12-03 11:37:43,086 - 5a242370e3e52a6de0092973
2017-12-03 11:37:43,087 - MovieClassifier ------------------------------------ Init
2017-12-03 11:37:43,101 - _id: 5a242370e3e52a6de0092973
2017-12-03 11:41:22,596 - CombinedMultiDict([ImmutableMultiDict([('q', '{}'), ('1512319282486', '')]), ImmutableMultiDict([])])
2017-12-03 11:41:22,598 - {}
2017-12-03 11:41:25,632 - {'review_text': 'I liked the story but not the movie'}
2017-12-03 11:41:25,742 - 5a242370e3e52a6de0092973
2017-12-03 11:41:25,743 - MovieClassifier ------------------------------------ Init
2017-12-03 11:41:25,771 - _id: 5a242370e3e52a6de0092973
2017-12-03 11:41:25,772 - y: 1
2017-12-03 11:41:25,772 - proba: 0.733321291402
2017-12-03 11:41:28,597 - 5a242370e3e52a6de0092973
2017-12-03 11:41:28,598 - MovieClassifier ------------------------------------ Init
2017-12-03 11:41:28,613 - _id: 5a242370e3e52a6de0092973
2017-12-03 11:44:30,153 - 5a242370e3e52a6de0092973
2017-12-03 11:44:30,161 - MovieClassifier ------------------------------------ Init
2017-12-03 11:44:30,186 - _id: 5a242370e3e52a6de0092973
2017-12-03 11:46:45,830 - Starting Flask Service......
2017-12-03 11:46:51,753 - CombinedMultiDict([ImmutableMultiDict([('q', '{}'), ('1512319611642', '')]), ImmutableMultiDict([])])
2017-12-03 11:46:51,754 - {}
2017-12-03 11:46:58,365 - {'review_text': 'I liked the story but not the movie'}
2017-12-03 11:46:58,475 - 5a242370e3e52a6de0092973
2017-12-03 11:46:58,476 - MovieClassifier ------------------------------------ Init
2017-12-03 11:46:58,596 - _id: 5a242370e3e52a6de0092973
2017-12-03 11:46:58,597 - y: 1
2017-12-03 11:46:58,597 - proba: 0.733321291402
2017-12-03 11:47:01,558 - 5a242370e3e52a6de0092973
2017-12-03 11:47:01,559 - MovieClassifier ------------------------------------ Init
2017-12-03 11:47:01,574 - _id: 5a242370e3e52a6de0092973
2017-12-03 11:49:24,465 - Starting Flask Service......
2017-12-03 11:49:53,847 - CombinedMultiDict([ImmutableMultiDict([('q', '{}'), ('1512319793736', '')]), ImmutableMultiDict([])])
2017-12-03 11:49:53,848 - {}
2017-12-03 11:49:56,699 - {'review_text': 'I liked the story but not the movie'}
2017-12-03 11:49:56,808 - 5a242370e3e52a6de0092973
2017-12-03 11:49:56,809 - MovieClassifier ------------------------------------ Init
2017-12-03 11:49:56,891 - _id: 5a242370e3e52a6de0092973
2017-12-03 11:49:56,892 - y: 1
2017-12-03 11:49:56,893 - proba: 0.733321291402
2017-12-03 11:49:57,927 - 5a242370e3e52a6de0092973
2017-12-03 11:49:57,928 - MovieClassifier ------------------------------------ Init
2017-12-03 11:49:57,944 - _id: 5a242370e3e52a6de0092973
2017-12-03 11:50:35,150 - Starting Flask Service......
2017-12-03 11:52:45,847 - CombinedMultiDict([ImmutableMultiDict([('q', '{}'), ('1512319965736', '')]), ImmutableMultiDict([])])
2017-12-03 11:52:45,847 - {}
2017-12-03 11:52:54,639 - {'review_text': 'I liked the story but not the movie'}
2017-12-03 11:52:54,749 - 5a242370e3e52a6de0092973
2017-12-03 11:52:54,751 - MovieClassifier ------------------------------------ Init
2017-12-03 11:52:54,821 - _id: 5a242370e3e52a6de0092973
2017-12-03 11:52:54,823 - y: 1
2017-12-03 11:52:54,823 - proba: 0.733321291402
2017-12-03 11:52:58,968 - 5a242370e3e52a6de0092973
2017-12-03 11:52:58,969 - MovieClassifier ------------------------------------ Init
2017-12-03 11:52:58,982 - _id: 5a242370e3e52a6de0092973
2017-12-03 12:04:12,459 - Starting Flask Service......
2017-12-03 12:04:25,252 - 5a242370e3e52a6de0092973
2017-12-03 12:04:25,253 - MovieClassifier ------------------------------------ Init
2017-12-03 12:04:25,360 - _id: 5a242370e3e52a6de0092973
2017-12-03 12:04:25,522 - CombinedMultiDict([ImmutableMultiDict([]), ImmutableMultiDict([])])
2017-12-03 12:04:25,522 - None
2017-12-03 12:04:43,258 - 5a242370e3e52a6de0092973
2017-12-03 12:04:43,259 - MovieClassifier ------------------------------------ Init
2017-12-03 12:04:43,272 - _id: 5a242370e3e52a6de0092973
2017-12-03 12:04:43,317 - CombinedMultiDict([ImmutableMultiDict([]), ImmutableMultiDict([])])
2017-12-03 12:04:43,317 - None
2017-12-03 12:05:50,847 - 5a242370e3e52a6de0092973
2017-12-03 12:05:50,849 - MovieClassifier ------------------------------------ Init
2017-12-03 12:05:50,863 - _id: 5a242370e3e52a6de0092973
2017-12-03 12:05:50,913 - CombinedMultiDict([ImmutableMultiDict([]), ImmutableMultiDict([])])
2017-12-03 12:05:50,914 - None
2017-12-03 12:06:18,063 - Starting Flask Service......
2017-12-03 12:06:23,592 - 5a242370e3e52a6de0092973
2017-12-03 12:06:23,595 - MovieClassifier ------------------------------------ Init
2017-12-03 12:06:23,665 - _id: 5a242370e3e52a6de0092973
2017-12-03 12:06:23,718 - CombinedMultiDict([ImmutableMultiDict([]), ImmutableMultiDict([])])
2017-12-03 12:06:23,718 - None
2017-12-03 12:07:16,384 - Starting Flask Service......
2017-12-03 12:07:21,422 - 5a242370e3e52a6de0092973
2017-12-03 12:07:21,426 - MovieClassifier ------------------------------------ Init
2017-12-03 12:07:21,498 - _id: 5a242370e3e52a6de0092973
2017-12-03 12:07:21,552 - CombinedMultiDict([ImmutableMultiDict([]), ImmutableMultiDict([])])
2017-12-03 12:07:21,552 - None
2017-12-03 12:09:06,444 - Starting Flask Service......
2017-12-03 12:09:19,028 - 5a242370e3e52a6de0092973
2017-12-03 12:09:19,030 - MovieClassifier ------------------------------------ Init
2017-12-03 12:09:19,101 - _id: 5a242370e3e52a6de0092973
2017-12-03 12:09:19,162 - CombinedMultiDict([ImmutableMultiDict([]), ImmutableMultiDict([])])
2017-12-03 12:09:19,162 - None
2017-12-03 12:16:53,130 - Starting Flask Service......
2017-12-03 12:17:05,179 - CombinedMultiDict([ImmutableMultiDict([('q', '{}'), ('1512321425067', '')]), ImmutableMultiDict([])])
2017-12-03 12:17:05,179 - {}
2017-12-03 12:17:08,431 - {'review_text': 'I liked the story but not the movie'}
2017-12-03 12:17:08,540 - 5a242370e3e52a6de0092973
2017-12-03 12:17:08,541 - MovieClassifier ------------------------------------ Init
2017-12-03 12:17:08,660 - _id: 5a242370e3e52a6de0092973
2017-12-03 12:17:08,661 - y: 1
2017-12-03 12:17:08,661 - proba: 0.733321291402
2017-12-03 12:17:10,786 - 5a242370e3e52a6de0092973
2017-12-03 12:17:10,787 - MovieClassifier ------------------------------------ Init
2017-12-03 12:17:10,803 - _id: 5a242370e3e52a6de0092973
2017-12-03 12:17:11,019 - {'review_text': 'I liked the story but not the movie'}
2017-12-03 12:17:11,141 - 5a242370e3e52a6de0092973
2017-12-03 12:17:11,142 - MovieClassifier ------------------------------------ Init
2017-12-03 12:17:11,156 - _id: 5a242370e3e52a6de0092973
2017-12-03 12:17:11,157 - y: 1
2017-12-03 12:17:11,157 - proba: 0.733321291402
2017-12-03 12:17:36,417 - {'review_text': 'I liked the story but not the movie'}
2017-12-03 12:17:36,533 - 5a242370e3e52a6de0092973
2017-12-03 12:17:36,534 - MovieClassifier ------------------------------------ Init
2017-12-03 12:17:36,555 - _id: 5a242370e3e52a6de0092973
2017-12-03 12:17:36,556 - y: 1
2017-12-03 12:17:36,556 - proba: 0.733321291402
2017-12-03 12:17:46,906 - CombinedMultiDict([ImmutableMultiDict([('q', '{}'), ('1512321466795', '')]), ImmutableMultiDict([])])
2017-12-03 12:17:46,906 - {}
2017-12-03 12:18:02,297 - {'content_title': "Google's AI Wins Pivotal Second Game in Match With Go Grandmaster", 'content_text': 'After more than four hours of tight play and a rapid-fire endgame, Google\'s artificially intelligent Go-playing computer system has won a second contest against grandmaster Lee Sedol, taking a two-games-to-none lead in their historic best-of-five match in downtown Seoul.  The surprisingly skillful Google machine, known as AlphaGo, now needs only one more win to claim victory in the match. The Korean-born Lee Sedol will go down in defeat unless he takes each of the match\'s last three games. Though machines have beaten the best humans at chess, checkers, Othello, Scrabble, Jeopardy!, and so many other games considered tests of human intellect, they have never beaten the very best at Go. Game Three is set for Saturday afternoon inside Seoul\'s Four Seasons hotel.  The match is a way of judging the suddenly rapid progress of artificial intelligence. One of the machine-learning techniques at the heart of AlphaGo has already reinvented myriad online services inside Google and other big-name Internet companies, helping to identify images, recognize commands spoken into smartphones, improve search engine results, and more. Meanwhile, another AlphaGo technique is now driving experimental robotics at Google and places like the University of California at Berkeley. This week\'s match can show how far these technologies have come - and perhaps how far they will go.  Created in Asia over 2,500 year ago, Go is exponentially more complex than chess, and at least among humans, it requires an added degree of intuition. Lee Sedol is widely-regarded as the top Go player of the last decade, after winning more international titles than all but one other player. He is currently ranked number five in the world, and according to Demis Hassabis, who leads DeepMind, the Google AI lab that created AlphaGo, his team chose the Korean for this all-important match because they wanted an opponent who would be remembered as one of history\'s great players.  Although AlphaGo topped Lee Sedol in the match\'s first game on Wednesday afternoon, the outcome of Game Two was no easier to predict. In his 1996 match with IBM\'s Deep Blue supercomputer, world chess champion Gary Kasparov lost the first game but then came back to win the second game and, eventually, the match as a whole. It wasn\'t until the following year that Deep Blue topped Kasparov over the course of a six-game contest. The thing to realize is that, after playing AlphaGo for the first time on Wednesday, Lee Sedol could adjust his style of play - just as Kasparov did back in 1996. But AlphaGo could not. Because this Google creation relies so heavily on machine learning techniques, the DeepMind team needs a good four to six weeks to train a new incarnation of the system. And that means they can\'t really change things during this eight-day match.  "This is about teaching and learning," Hassabis told us just before Game Two. "One game is not enough data to learn from - for a machine - and training takes an awful lot of time.', 'word_limit': '100'}
2017-12-03 12:18:02,407 - 5a22fa6ae3e52a40f793f587
2017-12-03 12:18:02,417 - TextSummarizer ------------------------------------ Init
2017-12-03 12:18:02,417 - _id: 5a22fa6ae3e52a40f793f587
2017-12-03 12:18:02,417 - word_limit: 100
2017-12-03 12:18:22,254 - {'content_title': "Google's AI Wins Pivotal Second Game in Match With Go Grandmaster", 'content_text': 'After more than four hours of tight play and a rapid-fire endgame, Google\'s artificially intelligent Go-playing computer system has won a second contest against grandmaster Lee Sedol, taking a two-games-to-none lead in their historic best-of-five match in downtown Seoul.  The surprisingly skillful Google machine, known as AlphaGo, now needs only one more win to claim victory in the match. The Korean-born Lee Sedol will go down in defeat unless he takes each of the match\'s last three games. Though machines have beaten the best humans at chess, checkers, Othello, Scrabble, Jeopardy!, and so many other games considered tests of human intellect, they have never beaten the very best at Go. Game Three is set for Saturday afternoon inside Seoul\'s Four Seasons hotel.  The match is a way of judging the suddenly rapid progress of artificial intelligence. One of the machine-learning techniques at the heart of AlphaGo has already reinvented myriad online services inside Google and other big-name Internet companies, helping to identify images, recognize commands spoken into smartphones, improve search engine results, and more. Meanwhile, another AlphaGo technique is now driving experimental robotics at Google and places like the University of California at Berkeley. This week\'s match can show how far these technologies have come - and perhaps how far they will go.  Created in Asia over 2,500 year ago, Go is exponentially more complex than chess, and at least among humans, it requires an added degree of intuition. Lee Sedol is widely-regarded as the top Go player of the last decade, after winning more international titles than all but one other player. He is currently ranked number five in the world, and according to Demis Hassabis, who leads DeepMind, the Google AI lab that created AlphaGo, his team chose the Korean for this all-important match because they wanted an opponent who would be remembered as one of history\'s great players.  Although AlphaGo topped Lee Sedol in the match\'s first game on Wednesday afternoon, the outcome of Game Two was no easier to predict. In his 1996 match with IBM\'s Deep Blue supercomputer, world chess champion Gary Kasparov lost the first game but then came back to win the second game and, eventually, the match as a whole. It wasn\'t until the following year that Deep Blue topped Kasparov over the course of a six-game contest. The thing to realize is that, after playing AlphaGo for the first time on Wednesday, Lee Sedol could adjust his style of play - just as Kasparov did back in 1996. But AlphaGo could not. Because this Google creation relies so heavily on machine learning techniques, the DeepMind team needs a good four to six weeks to train a new incarnation of the system. And that means they can\'t really change things during this eight-day match.  "This is about teaching and learning," Hassabis told us just before Game Two. "One game is not enough data to learn from - for a machine - and training takes an awful lot of time.', 'word_limit': '200'}
2017-12-03 12:18:22,371 - 5a22fa6ae3e52a40f793f587
2017-12-03 12:18:22,377 - TextSummarizer ------------------------------------ Init
2017-12-03 12:18:22,377 - _id: 5a22fa6ae3e52a40f793f587
2017-12-03 12:18:22,377 - word_limit: 200
2017-12-03 12:18:31,294 - {'content_title': "Google's AI Wins Pivotal Second Game in Match With Go Grandmaster", 'content_text': 'After more than four hours of tight play and a rapid-fire endgame, Google\'s artificially intelligent Go-playing computer system has won a second contest against grandmaster Lee Sedol, taking a two-games-to-none lead in their historic best-of-five match in downtown Seoul.  The surprisingly skillful Google machine, known as AlphaGo, now needs only one more win to claim victory in the match. The Korean-born Lee Sedol will go down in defeat unless he takes each of the match\'s last three games. Though machines have beaten the best humans at chess, checkers, Othello, Scrabble, Jeopardy!, and so many other games considered tests of human intellect, they have never beaten the very best at Go. Game Three is set for Saturday afternoon inside Seoul\'s Four Seasons hotel.  The match is a way of judging the suddenly rapid progress of artificial intelligence. One of the machine-learning techniques at the heart of AlphaGo has already reinvented myriad online services inside Google and other big-name Internet companies, helping to identify images, recognize commands spoken into smartphones, improve search engine results, and more. Meanwhile, another AlphaGo technique is now driving experimental robotics at Google and places like the University of California at Berkeley. This week\'s match can show how far these technologies have come - and perhaps how far they will go.  Created in Asia over 2,500 year ago, Go is exponentially more complex than chess, and at least among humans, it requires an added degree of intuition. Lee Sedol is widely-regarded as the top Go player of the last decade, after winning more international titles than all but one other player. He is currently ranked number five in the world, and according to Demis Hassabis, who leads DeepMind, the Google AI lab that created AlphaGo, his team chose the Korean for this all-important match because they wanted an opponent who would be remembered as one of history\'s great players.  Although AlphaGo topped Lee Sedol in the match\'s first game on Wednesday afternoon, the outcome of Game Two was no easier to predict. In his 1996 match with IBM\'s Deep Blue supercomputer, world chess champion Gary Kasparov lost the first game but then came back to win the second game and, eventually, the match as a whole. It wasn\'t until the following year that Deep Blue topped Kasparov over the course of a six-game contest. The thing to realize is that, after playing AlphaGo for the first time on Wednesday, Lee Sedol could adjust his style of play - just as Kasparov did back in 1996. But AlphaGo could not. Because this Google creation relies so heavily on machine learning techniques, the DeepMind team needs a good four to six weeks to train a new incarnation of the system. And that means they can\'t really change things during this eight-day match.  "This is about teaching and learning," Hassabis told us just before Game Two. "One game is not enough data to learn from - for a machine - and training takes an awful lot of time.', 'word_limit': '100'}
2017-12-03 12:18:31,403 - 5a22fa6ae3e52a40f793f587
2017-12-03 12:18:31,408 - TextSummarizer ------------------------------------ Init
2017-12-03 12:18:31,408 - _id: 5a22fa6ae3e52a40f793f587
2017-12-03 12:18:31,408 - word_limit: 100
2017-12-03 12:19:10,974 - CombinedMultiDict([ImmutableMultiDict([('q', '{}'), ('1512321550862', '')]), ImmutableMultiDict([])])
2017-12-03 12:19:10,975 - {}
2017-12-03 12:19:19,459 - {'content_title': "Google's AI Wins Pivotal Second Game in Match With Go Grandmaster", 'content_text': 'After more than four hours of tight play and a rapid-fire endgame, Google\'s artificially intelligent Go-playing computer system has won a second contest against grandmaster Lee Sedol, taking a two-games-to-none lead in their historic best-of-five match in downtown Seoul.  The surprisingly skillful Google machine, known as AlphaGo, now needs only one more win to claim victory in the match. The Korean-born Lee Sedol will go down in defeat unless he takes each of the match\'s last three games. Though machines have beaten the best humans at chess, checkers, Othello, Scrabble, Jeopardy!, and so many other games considered tests of human intellect, they have never beaten the very best at Go. Game Three is set for Saturday afternoon inside Seoul\'s Four Seasons hotel.  The match is a way of judging the suddenly rapid progress of artificial intelligence. One of the machine-learning techniques at the heart of AlphaGo has already reinvented myriad online services inside Google and other big-name Internet companies, helping to identify images, recognize commands spoken into smartphones, improve search engine results, and more. Meanwhile, another AlphaGo technique is now driving experimental robotics at Google and places like the University of California at Berkeley. This week\'s match can show how far these technologies have come - and perhaps how far they will go.  Created in Asia over 2,500 year ago, Go is exponentially more complex than chess, and at least among humans, it requires an added degree of intuition. Lee Sedol is widely-regarded as the top Go player of the last decade, after winning more international titles than all but one other player. He is currently ranked number five in the world, and according to Demis Hassabis, who leads DeepMind, the Google AI lab that created AlphaGo, his team chose the Korean for this all-important match because they wanted an opponent who would be remembered as one of history\'s great players.  Although AlphaGo topped Lee Sedol in the match\'s first game on Wednesday afternoon, the outcome of Game Two was no easier to predict. In his 1996 match with IBM\'s Deep Blue supercomputer, world chess champion Gary Kasparov lost the first game but then came back to win the second game and, eventually, the match as a whole. It wasn\'t until the following year that Deep Blue topped Kasparov over the course of a six-game contest. The thing to realize is that, after playing AlphaGo for the first time on Wednesday, Lee Sedol could adjust his style of play - just as Kasparov did back in 1996. But AlphaGo could not. Because this Google creation relies so heavily on machine learning techniques, the DeepMind team needs a good four to six weeks to train a new incarnation of the system. And that means they can\'t really change things during this eight-day match.  "This is about teaching and learning," Hassabis told us just before Game Two. "One game is not enough data to learn from - for a machine - and training takes an awful lot of time.', 'word_limit': '100'}
2017-12-03 12:19:19,568 - 5a22fa6ae3e52a40f793f587
2017-12-03 12:19:19,569 - TextSummarizer ------------------------------------ Init
2017-12-03 12:19:19,569 - _id: 5a22fa6ae3e52a40f793f587
2017-12-03 12:19:19,570 - word_limit: 100
2017-12-03 12:22:03,455 - {'content_title': 'How to Run Text Summarization with TensorFlow', 'content_text': 'Text summarization problem has many useful applications. If you run a website, you can create titles and short summaries for user generated content. If you want to read a lot of articles and don’t have time to do that, your virtual assistant can summarize main points from these articles for you.\nIt is not an easy problem to solve. There are multiple approaches, including various supervised and unsupervised algorithms. Some algorithms rank the importance of sentences within the text and then construct a summary out of important sentences, others are end-to-end generative models.\nEnd-to-end machine learning algorithms are interesting to try. After all, end-to-end algorithms demonstrate good results in other areas, like image recognition, speech recognition, language translation, and even question-answering.\nText summarization with TensorFlow\n\nIn August 2016, Peter Liu and Xin Pan, software engineers on Google Brain Team, published a blog post “Text summarization with TensorFlow”. Their algorithm is extracting interesting parts of the text and create a summary by using these parts of the text and allow for rephrasings to make summary more grammatically correct. This approach is called abstractive summarization.\n\nPeter and Xin trained a text summarization model to produce headlines for news articles, using Annotated English Gigaword, a dataset often used in summarization research. The dataset contains about 10 million documents. The model was trained end-to-end with a deep learning technique called sequence-to-sequence learning.\n\nCode for training and testing the model is included into TensorFlow Models GitHub repository. The core model is a sequence-to-sequence model with attention. When training, the model is using the first two sentences from the article as an input and generates a headline.\n\nWhen decoding, the algorithm is using beam search to find the best headline from candidate headlines generated by the model.\n\nGitHub repository doesn’t include a trained model. The dataset is not publicly available, a license costs $6000 for organizations which are not members of Linguistic Data Consortium. But they include a toy dataset which is enough to run the code.\n\nHow to run\n\nYou will need TensorFlow and Bazel as prerequisites for training the model.\n\nThe toy dataset included into the repository, contains two files in “data” directory: “data” and “vocab”. The first one contains a sequence of serialized tensorflow.core.example.example_pb2.Example objects. An example of code to create a file with this format:\n“vocab” file is a text file with the frequency of words in a vocabulary. Each line contains a word, space character and number of occurrences of that word in the dataset. The list is being used to vectorize texts.\nWhen running “decode” code, note that it will loop over the entire dataset indefinitely, so you will have to stop execution manually at some point. You can find results of decoding in log_root/decode folder. It will contain a few files, some of them have prefix “ref”, they contain original headlines from the test set. Other files have prefix “decode”, they contain headlines generated by the model.', 'word_limit': 150}
2017-12-03 12:22:03,456 - <pymongo.results.InsertOneResult object at 0x120ca8948>
2017-12-03 12:22:03,456 - 5a2432bbe3e52a74e2e0273d
2017-12-03 12:22:03,563 - 5a2432bbe3e52a74e2e0273d
2017-12-03 12:22:03,567 - TextSummarizer ------------------------------------ Init
2017-12-03 12:22:03,567 - _id: 5a2432bbe3e52a74e2e0273d
2017-12-03 12:22:03,567 - word_limit: 150
2017-12-03 12:25:35,626 - {'content_title': 'Linear Regression', 'content_text': 'Linear regression attempts to model the relationship between two variables by fitting a linear equation to observed data. One variable is considered to be an explanatory variable, and the other is considered to be a dependent variable. For example, a modeler might want to relate the weights of individuals to their heights using a linear regression model.\nBefore attempting to fit a linear model to observed data, a modeler should first determine whether or not there is a relationship between the variables of interest. This does not necessarily imply that one variable causes the other (for example, higher SAT scores do not cause higher college grades), but that there is some significant association between the two variables. A scatterplot can be a helpful tool in determining the strength of the relationship between two variables. If there appears to be no association between the proposed explanatory and dependent variables (i.e., the scatterplot does not indicate any increasing or decreasing trends), then fitting a linear regression model to the data probably will not provide a useful model. A valuable numerical measure of association between two variables is the correlation coefficient, which is a value between -1 and 1 indicating the strength of the association of the observed data for the two variables.\n\nA linear regression line has an equation of the form Y = a + bX, where X is the explanatory variable and Y is the dependent variable. The slope of the line is b, and a is the intercept (the value of y when x = 0).\nLeast-Squares Regression\n\nThe most common method for fitting a regression line is the method of least-squares. This method calculates the best-fitting line for the observed data by minimizing the sum of the squares of the vertical deviations from each data point to the line (if a point lies on the fitted line exactly, then its vertical deviation is 0). Because the deviations are first squared, then summed, there are no cancellations between positive and negative values.\nExample\n\nThe dataset "Televisions, Physicians, and Life Expectancy" contains, among other variables, the number of people per television set and the number of people per physician for 40 countries. Since both variables probably reflect the level of wealth in each country, it is reasonable to assume that there is some positive association between them. After removing 8 countries with missing values from the dataset, the remaining 32 countries have a correlation coefficient of 0.852 for number of people per television set and number of people per physician. The r² value is 0.726 (the square of the correlation coefficient), indicating that 72.6% of the variation in one variable may be explained by the other. (Note: see correlation for more detail.) Suppose we choose to consider number of people per television set as the explanatory variable, and number of people per physician as the dependent variable. Using the MINITAB "REGRESS" command gives the following results:\nThe regression equation is People.Phys. = 1019 + 56.2 People.Tel.\nTo view the fit of the model to the observed data, one may plot the computed regression line over the actual data points to evaluate the results. For this example, the plot appears to the right, with number of individuals per television set (the explanatory variable) on the x-axis and number of individuals per physician (the dependent variable) on the y-axis. While most of the data points are clustered towards the lower left corner of the plot (indicating relatively few individuals per television set and per physician), there are a few points which lie far away from the main cluster of the data. These points are known as outliers, and depending on their location may have a major impact on the regression line (see below).', 'word_limit': 150}
2017-12-03 12:25:35,633 - <pymongo.results.InsertOneResult object at 0x120d303f0>
2017-12-03 12:25:35,633 - 5a24338fe3e52a74e2e0273e
2017-12-03 12:25:35,740 - 5a24338fe3e52a74e2e0273e
2017-12-03 12:25:35,745 - TextSummarizer ------------------------------------ Init
2017-12-03 12:25:35,746 - _id: 5a24338fe3e52a74e2e0273e
2017-12-03 12:25:35,746 - word_limit: 150
2017-12-03 12:26:07,798 - {'content_title': 'Visualizing and Understanding Convolutional Networks', 'content_text': 'Since their introduction by (LeCun et al., 1989) in the early 1990’s, Convolutional Networks (convnets) have demonstrated excellent performance at tasks such as hand-written digit classification and face detection. In the last year, several papers have shown that they can also deliver outstanding performance on more challenging visual classification tasks. (Ciresan et al., 2012) demonstrate state-of-the-art performance on NORB and CIFAR-10 datasets. Most notably, (Krizhevsky et al., 2012) show record beating performance on the ImageNet 2012 classification benchmark, with their convnet model achieving an error rate of 16.4%, compared to the 2nd place result of 26.1%. Several factors are responsible for this renewed interest in convnet models: (i) the availability of much larger training sets, with millions of labeled examples; (ii) powerful GPU implementations, making the training of very large models practical and (iii) better model regularization strategies, such as Dropout.\nDespite this encouraging progress, there is still lit- tle insight into the internal operation and behavior of these complex models, or how they achieve such good performance. From a scientific standpoint, this is deeply unsatisfactory. Without clear understanding of how and why they work, the development of better models is reduced to trial-and-error. In this paper we introduce a visualization technique that reveals the input stimuli that excite individual feature maps at any layer in the model. It also allows us to observe the evolution of features during training and to diagnose potential problems with the model. The visualization technique we propose uses a multi-layered Deconvolutional Network (deconvnet), as proposed by (Zeiler et al., 2011), to project the feature activations back to the input pixel space. We also perform a sensitivity analysis of the classifier output by occluding portions of the input image, revealing which parts of the scene are important for classification.\nUsing these tools, we start with the architecture of (Krizhevsky et al., 2012) and explore different architectures, discovering ones that outperform their results on ImageNet. We then explore the generalization ability of the model to other datasets, just retraining the softmax classifier on top. As such, this is a form of supervised pre-training, which contrasts with the unsupervised pre-training methods popularized by (Hinton et al., 2006) and others (Bengio et al., 2007; Vincent et al., 2008). The generalization ability of convnet features is also explored in concurrent work by (Donahue et al., 2013).\nVisualizing features to gain intuition about the net- work is common practice, but mostly limited to the 1st layer where projections to pixel space are possible. In higher layers this is not the case, and there are limited methods for interpreting activity. (Erhan et al., 2009) find the optimal stimulus for each unit by performing gradient descent in image space to maximize the unit’s activation. This requires a careful initialization and does not give any information about the unit’s in- variances. Motivated by the latter’s short-coming, (Le et al., 2010) (extending an idea by (Berkes & Wiskott, 2006)) show how the Hessian of a given unit may be computed numerically around the optimal response, giving some insight into invariances. The problem is that for higher layers, the invariances are extremely complex so are poorly captured by a simple quadratic approximation. Our approach, by contrast, provides a non-parametric view of invariance, showing which pat- terns from the training set activate the feature map. (Donahue et al., 2013) show visualizations that iden- tify patches within a dataset that are responsible for strong activations at higher layers in the model. Our visualizations differ in that they are not just crops of input images, but rather top-down projections that reveal structures within each patch that stimulate a particular feature map.\nWe use standard fully supervised convnet models\nthroughout the paper, as defined by (LeCun et al.,\n1989) and (Krizhevsky et al., 2012). These models map a color 2D input image xi, via a series of layers, to a probability vector yˆ over the C different i classes. Each layer consists of (i) convolution of the previous layer output (or, in the case of the 1st layer, the input image) with a set of learned filters; (ii) pass- ing the responses through a rectified linear function (relu(x) = max(x,0)); (iii) [optionally] max pooling over local neighborhoods and (iv) [optionally] a lo- cal contrast operation that normalizes the responses across feature maps. For more details of these opera- tions, see (Krizhevsky et al., 2012) and (Jarrett et al., 2009). The top few layers of the network are conventional fully-connected networks and the final layer is a softmax classifier. Fig. 3 shows the model used in many of our experiments.\nWe train these models using a large set of N labeled images {x,y}, where label yi is a discrete variable indicating the true class. A cross-entropy loss function, suitable for image classification, is used to compare yˆ and y . The parameters of the network (filters in the convolutional layers, weight matrices in the fully-connected layers and biases) are trained by back- propagating the derivative of the loss with respect to the parameters throughout the network, and updating the parameters via stochastic gradient descent.\nUnderstanding the operation of a convnet requires interpreting the feature activity in intermediate layers. We present a novel way to map these activities back to the input pixel space, showing what input pattern orig- inally caused a given activation in the feature maps. We perform this mapping with a Deconvolutional Network (deconvnet) (Zeiler et al., 2011). A deconvnet can be thought of as a convnet model that uses the same components (filtering, pooling) but in reverse, so instead of mapping pixels to features does the opposite. In (Zeiler et al., 2011), deconvnets were proposed as a way of performing unsupervised learning. Here, they are not used in any learning capacity, just as a probe of an already trained convnet.\nTo examine a convnet, a deconvnet is attached to each of its layers, as illustrated in Fig. 1(top), providing a continuous path back to image pixels. To start, an input image is presented to the convnet and features computed throughout the layers. To examine a given convnet activation, we set all other activations in the layer to zero and pass the feature maps as input to the attached deconvnet layer. Then we successively (i) unpool, (ii) rectify and (iii) filter to reconstruct the activity in the layer beneath that gave rise to the chosen activation. This is then repeated until input pixel space is reached.\nUnpooling: In the convnet, the max pooling opera- tion is non-invertible, however we can obtain an ap- proximate inverse by recording the locations of the maxima within each pooling region in a set of switch variables. In the deconvnet, the unpooling operation uses these switches to place the reconstructions from the layer above into appropriate locations, preserving the structure of the stimulus. See Fig. 1(bottom) for an illustration of the procedure.\nRectification: The convnet uses relu non-linearities, which rectify the feature maps thus ensuring the feature maps are always positive. To obtain valid feature reconstructions at each layer (which also should be positive), we pass the reconstructed signal through a relu non-linearity.\nFiltering: The convnet uses learned filters to convolve the feature maps from the previous layer. To invert this, the deconvnet uses transposed versions of the same filters, but applied to the rectified maps, not the output of the layer beneath. In practice this means flipping each filter vertically and horizontally.\nProjecting down from higher layers uses the switch settings generated by the max pooling in the convnet on the way up. As these switch settings are peculiar to a given input image, the reconstruction obtained from a single activation thus resembles a small piece of the original input image, with structures weighted according to their contribution toward to the feature activation. Since the model is trained discriminatively, they implicitly show which parts of the input image are discriminative. Note that these projections are not samples from the model, since there is no generative process involved.', 'word_limit': '150'}
2017-12-03 12:26:07,914 - 5a23fe71e3e52a606c387981
2017-12-03 12:26:07,918 - TextSummarizer ------------------------------------ Init
2017-12-03 12:26:07,919 - _id: 5a23fe71e3e52a606c387981
2017-12-03 12:26:07,919 - word_limit: 150
2017-12-03 12:26:39,606 - CombinedMultiDict([ImmutableMultiDict([('q', '{}'), ('1512321999495', '')]), ImmutableMultiDict([])])
2017-12-03 12:26:39,606 - {}
2017-12-03 12:29:43,228 - {'content_title': 'Sequence to Sequence Learning with Neural Networks', 'content_text': 'Deep Neural Networks (DNNs) are extremely powerful machine learning models that achieve excellent\nperformance on difficult problems such as speech recognition [13, 7] and visual object recognition\n[19, 6, 21, 20]. DNNs are powerful because they can perform arbitrary parallel computation\nfor a modest number of steps. A surprising example of the power of DNNs is their ability to sort\nN N-bit numbers using only 2 hidden layers of quadratic size [27]. So, while neural networks are\nrelated to conventional statistical models, they learn an intricate computation. Furthermore, large\nDNNs can be trained with supervised backpropagation whenever the labeled training set has enough\ninformation to specify the network’s parameters. Thus, if there exists a parameter setting of a large\nDNN that achieves good results (for example, because humans can solve the task very rapidly),\nsupervised backpropagation will find these parameters and solve the problem.\nDespite their flexibility and power, DNNs can only be applied to problems whose inputs and targets\ncan be sensibly encoded with vectors of fixed dimensionality. It is a significant limitation, since\nmany important problems are best expressed with sequences whose lengths are not known a-priori.\nFor example, speech recognition and machine translation are sequential problems. Likewise, question\nanswering can also be seen as mapping a sequence of words representing the question to a sequence of words representing the answer. It is therefore clear that a domain-independent method\nthat learns to map sequences to sequences would be useful.\nSequences pose a challenge for DNNs because they require that the dimensionality of the inputs and\noutputs is known and fixed. In this paper, we show that a straightforward application of the Long\nShort-Term Memory (LSTM) architecture [16] can solve general sequence to sequence problems.\nThe idea is to use one LSTM to read the input sequence, one timestep at a time, to obtain large fixeddimensional\nvector representation, and then to use another LSTM to extract the output sequence\nfrom that vector (fig. 1). The second LSTM is essentially a recurrent neural network language model\n[28, 23, 30] except that it is conditioned on the input sequence. The LSTM’s ability to successfully\nlearn on data with long range temporal dependencies makes it a natural choice for this application\ndue to the considerable time lag between the inputs and their corresponding outputs (fig. 1).\nThere have been a number of related attempts to address the general sequence to sequence learning\nproblem with neural networks. Our approach is closely related to Kalchbrenner and Blunsom [18]\nwho were the first to map the entire input sentence to vector, and is related to Cho et al. [5] although\nthe latter was used only for rescoring hypotheses produced by a phrase-based system. Graves [10]\nintroduced a novel differentiable attention mechanism that allows neural networks to focus on different\nparts of their input, and an elegant variant of this idea was successfully applied to machine\ntranslation by Bahdanau et al. [2]. The Connectionist Sequence Classification is another popular\ntechnique for mapping sequences to sequences with neural networks, but it assumes a monotonic\nalignment between the inputs and the outputs [11]. The main result of this work is the following. On the WMT’14 English to French translation task,\nwe obtained a BLEU score of 34.81 by directly extracting translations from an ensemble of 5 deep\nLSTMs (with 384M parameters and 8,000 dimensional state each) using a simple left-to-right beamsearch\ndecoder. This is by far the best result achieved by direct translation with large neural networks.\nFor comparison, the BLEU score of an SMT baseline on this dataset is 33.30 [29]. The 34.81\nBLEU score was achieved by an LSTM with a vocabulary of 80k words, so the score was penalized\nwhenever the reference translation contained a word not covered by these 80k. This result shows\nthat a relatively unoptimized small-vocabulary neural network architecture which has much room\nfor improvement outperforms a phrase-based SMT system.\nFinally, we used the LSTM to rescore the publicly available 1000-best lists of the SMT baseline on\nthe same task [29]. By doing so, we obtained a BLEU score of 36.5, which improves the baseline by\n3.2 BLEU points and is close to the previous best published result on this task (which is 37.0 [9]).\nSurprisingly, the LSTM did not suffer on very long sentences, despite the recent experience of other\nresearchers with related architectures [26]. We were able to do well on long sentences because we\nreversed the order of words in the source sentence but not the target sentences in the training and test\nset. By doing so, we introduced many short term dependencies that made the optimization problem\nmuch simpler (see sec. 2 and 3.3). As a result, SGD could learn LSTMs that had no trouble with\nlong sentences. The simple trick of reversing the words in the source sentence is one of the key\ntechnical contributions of this work.\nA useful property of the LSTM is that it learns to map an input sentence of variable length into\na fixed-dimensional vector representation. Given that translations tend to be paraphrases of the\nsource sentences, the translation objective encourages the LSTM to find sentence representations\nthat capture their meaning, as sentences with similar meanings are close to each other while different sentences meanings will be far. A qualitative evaluation supports this claim, showing that our model\nis aware of word order and is fairly invariant to the active and passive voice.', 'word_limit': 150}
2017-12-03 12:29:43,235 - <pymongo.results.InsertOneResult object at 0x120d986c0>
2017-12-03 12:29:43,235 - 5a243487e3e52a74e2e0273f
2017-12-03 12:29:43,341 - 5a243487e3e52a74e2e0273f
2017-12-03 12:29:43,350 - TextSummarizer ------------------------------------ Init
2017-12-03 12:29:43,351 - _id: 5a243487e3e52a74e2e0273f
2017-12-03 12:29:43,351 - word_limit: 150
2017-12-03 12:30:03,782 - CombinedMultiDict([ImmutableMultiDict([('q', '{}'), ('1512322203674', '')]), ImmutableMultiDict([])])
2017-12-03 12:30:03,783 - {}
2017-12-03 16:13:36,927 - CombinedMultiDict([ImmutableMultiDict([('q', '{}'), ('1512335616805', '')]), ImmutableMultiDict([])])
2017-12-03 16:13:36,931 - {}
2017-12-03 16:29:22,694 - CombinedMultiDict([ImmutableMultiDict([('q', '{}'), ('1512336562580', '')]), ImmutableMultiDict([])])
2017-12-03 16:29:22,696 - {}
2017-12-03 16:29:51,185 - {'content_title': "Google's AI Wins Pivotal Second Game in Match With Go Grandmaster", 'content_text': 'After more than four hours of tight play and a rapid-fire endgame, Google\'s artificially intelligent Go-playing computer system has won a second contest against grandmaster Lee Sedol, taking a two-games-to-none lead in their historic best-of-five match in downtown Seoul.  The surprisingly skillful Google machine, known as AlphaGo, now needs only one more win to claim victory in the match. The Korean-born Lee Sedol will go down in defeat unless he takes each of the match\'s last three games. Though machines have beaten the best humans at chess, checkers, Othello, Scrabble, Jeopardy!, and so many other games considered tests of human intellect, they have never beaten the very best at Go. Game Three is set for Saturday afternoon inside Seoul\'s Four Seasons hotel.  The match is a way of judging the suddenly rapid progress of artificial intelligence. One of the machine-learning techniques at the heart of AlphaGo has already reinvented myriad online services inside Google and other big-name Internet companies, helping to identify images, recognize commands spoken into smartphones, improve search engine results, and more. Meanwhile, another AlphaGo technique is now driving experimental robotics at Google and places like the University of California at Berkeley. This week\'s match can show how far these technologies have come - and perhaps how far they will go.  Created in Asia over 2,500 year ago, Go is exponentially more complex than chess, and at least among humans, it requires an added degree of intuition. Lee Sedol is widely-regarded as the top Go player of the last decade, after winning more international titles than all but one other player. He is currently ranked number five in the world, and according to Demis Hassabis, who leads DeepMind, the Google AI lab that created AlphaGo, his team chose the Korean for this all-important match because they wanted an opponent who would be remembered as one of history\'s great players.  Although AlphaGo topped Lee Sedol in the match\'s first game on Wednesday afternoon, the outcome of Game Two was no easier to predict. In his 1996 match with IBM\'s Deep Blue supercomputer, world chess champion Gary Kasparov lost the first game but then came back to win the second game and, eventually, the match as a whole. It wasn\'t until the following year that Deep Blue topped Kasparov over the course of a six-game contest. The thing to realize is that, after playing AlphaGo for the first time on Wednesday, Lee Sedol could adjust his style of play - just as Kasparov did back in 1996. But AlphaGo could not. Because this Google creation relies so heavily on machine learning techniques, the DeepMind team needs a good four to six weeks to train a new incarnation of the system. And that means they can\'t really change things during this eight-day match.  "This is about teaching and learning," Hassabis told us just before Game Two. "One game is not enough data to learn from - for a machine - and training takes an awful lot of time.', 'word_limit': '200'}
2017-12-03 16:29:51,309 - 5a22fa6ae3e52a40f793f587
2017-12-03 16:29:51,312 - TextSummarizer ------------------------------------ Init
2017-12-03 16:29:51,313 - _id: 5a22fa6ae3e52a40f793f587
2017-12-03 16:29:51,313 - word_limit: 200
2017-12-03 16:31:43,937 - {'content_title': "Google's AI Wins Pivotal Second Game in Match With Go Grandmaster", 'content_text': 'After more than four hours of tight play and a rapid-fire endgame, Google\'s artificially intelligent Go-playing computer system has won a second contest against grandmaster Lee Sedol, taking a two-games-to-none lead in their historic best-of-five match in downtown Seoul.  The surprisingly skillful Google machine, known as AlphaGo, now needs only one more win to claim victory in the match. The Korean-born Lee Sedol will go down in defeat unless he takes each of the match\'s last three games. Though machines have beaten the best humans at chess, checkers, Othello, Scrabble, Jeopardy!, and so many other games considered tests of human intellect, they have never beaten the very best at Go. Game Three is set for Saturday afternoon inside Seoul\'s Four Seasons hotel.  The match is a way of judging the suddenly rapid progress of artificial intelligence. One of the machine-learning techniques at the heart of AlphaGo has already reinvented myriad online services inside Google and other big-name Internet companies, helping to identify images, recognize commands spoken into smartphones, improve search engine results, and more. Meanwhile, another AlphaGo technique is now driving experimental robotics at Google and places like the University of California at Berkeley. This week\'s match can show how far these technologies have come - and perhaps how far they will go.  Created in Asia over 2,500 year ago, Go is exponentially more complex than chess, and at least among humans, it requires an added degree of intuition. Lee Sedol is widely-regarded as the top Go player of the last decade, after winning more international titles than all but one other player. He is currently ranked number five in the world, and according to Demis Hassabis, who leads DeepMind, the Google AI lab that created AlphaGo, his team chose the Korean for this all-important match because they wanted an opponent who would be remembered as one of history\'s great players.  Although AlphaGo topped Lee Sedol in the match\'s first game on Wednesday afternoon, the outcome of Game Two was no easier to predict. In his 1996 match with IBM\'s Deep Blue supercomputer, world chess champion Gary Kasparov lost the first game but then came back to win the second game and, eventually, the match as a whole. It wasn\'t until the following year that Deep Blue topped Kasparov over the course of a six-game contest. The thing to realize is that, after playing AlphaGo for the first time on Wednesday, Lee Sedol could adjust his style of play - just as Kasparov did back in 1996. But AlphaGo could not. Because this Google creation relies so heavily on machine learning techniques, the DeepMind team needs a good four to six weeks to train a new incarnation of the system. And that means they can\'t really change things during this eight-day match.  "This is about teaching and learning," Hassabis told us just before Game Two. "One game is not enough data to learn from - for a machine - and training takes an awful lot of time.', 'word_limit': '100'}
2017-12-03 16:31:44,048 - 5a22fa6ae3e52a40f793f587
2017-12-03 16:31:44,054 - TextSummarizer ------------------------------------ Init
2017-12-03 16:31:44,054 - _id: 5a22fa6ae3e52a40f793f587
2017-12-03 16:31:44,054 - word_limit: 100
2017-12-03 16:33:26,895 - CombinedMultiDict([ImmutableMultiDict([('q', '{}'), ('1512336806782', '')]), ImmutableMultiDict([])])
2017-12-03 16:33:26,897 - {}
2017-12-03 16:33:43,674 - {'content_title': "Google's AI Wins Pivotal Second Game in Match With Go Grandmaster", 'content_text': 'After more than four hours of tight play and a rapid-fire endgame, Google\'s artificially intelligent Go-playing computer system has won a second contest against grandmaster Lee Sedol, taking a two-games-to-none lead in their historic best-of-five match in downtown Seoul.  The surprisingly skillful Google machine, known as AlphaGo, now needs only one more win to claim victory in the match. The Korean-born Lee Sedol will go down in defeat unless he takes each of the match\'s last three games. Though machines have beaten the best humans at chess, checkers, Othello, Scrabble, Jeopardy!, and so many other games considered tests of human intellect, they have never beaten the very best at Go. Game Three is set for Saturday afternoon inside Seoul\'s Four Seasons hotel.  The match is a way of judging the suddenly rapid progress of artificial intelligence. One of the machine-learning techniques at the heart of AlphaGo has already reinvented myriad online services inside Google and other big-name Internet companies, helping to identify images, recognize commands spoken into smartphones, improve search engine results, and more. Meanwhile, another AlphaGo technique is now driving experimental robotics at Google and places like the University of California at Berkeley. This week\'s match can show how far these technologies have come - and perhaps how far they will go.  Created in Asia over 2,500 year ago, Go is exponentially more complex than chess, and at least among humans, it requires an added degree of intuition. Lee Sedol is widely-regarded as the top Go player of the last decade, after winning more international titles than all but one other player. He is currently ranked number five in the world, and according to Demis Hassabis, who leads DeepMind, the Google AI lab that created AlphaGo, his team chose the Korean for this all-important match because they wanted an opponent who would be remembered as one of history\'s great players.  Although AlphaGo topped Lee Sedol in the match\'s first game on Wednesday afternoon, the outcome of Game Two was no easier to predict. In his 1996 match with IBM\'s Deep Blue supercomputer, world chess champion Gary Kasparov lost the first game but then came back to win the second game and, eventually, the match as a whole. It wasn\'t until the following year that Deep Blue topped Kasparov over the course of a six-game contest. The thing to realize is that, after playing AlphaGo for the first time on Wednesday, Lee Sedol could adjust his style of play - just as Kasparov did back in 1996. But AlphaGo could not. Because this Google creation relies so heavily on machine learning techniques, the DeepMind team needs a good four to six weeks to train a new incarnation of the system. And that means they can\'t really change things during this eight-day match.  "This is about teaching and learning," Hassabis told us just before Game Two. "One game is not enough data to learn from - for a machine - and training takes an awful lot of time.', 'word_limit': '300'}
2017-12-03 16:33:43,794 - 5a22fa6ae3e52a40f793f587
2017-12-03 16:33:43,798 - TextSummarizer ------------------------------------ Init
2017-12-03 16:33:43,798 - _id: 5a22fa6ae3e52a40f793f587
2017-12-03 16:33:43,799 - word_limit: 300
2017-12-03 16:34:31,579 - CombinedMultiDict([ImmutableMultiDict([('q', '{}'), ('1512336871473', '')]), ImmutableMultiDict([])])
2017-12-03 16:34:31,581 - {}
2017-12-03 16:34:38,382 - {'content_title': "Google's AI Wins Pivotal Second Game in Match With Go Grandmaster", 'content_text': 'After more than four hours of tight play and a rapid-fire endgame, Google\'s artificially intelligent Go-playing computer system has won a second contest against grandmaster Lee Sedol, taking a two-games-to-none lead in their historic best-of-five match in downtown Seoul.  The surprisingly skillful Google machine, known as AlphaGo, now needs only one more win to claim victory in the match. The Korean-born Lee Sedol will go down in defeat unless he takes each of the match\'s last three games. Though machines have beaten the best humans at chess, checkers, Othello, Scrabble, Jeopardy!, and so many other games considered tests of human intellect, they have never beaten the very best at Go. Game Three is set for Saturday afternoon inside Seoul\'s Four Seasons hotel.  The match is a way of judging the suddenly rapid progress of artificial intelligence. One of the machine-learning techniques at the heart of AlphaGo has already reinvented myriad online services inside Google and other big-name Internet companies, helping to identify images, recognize commands spoken into smartphones, improve search engine results, and more. Meanwhile, another AlphaGo technique is now driving experimental robotics at Google and places like the University of California at Berkeley. This week\'s match can show how far these technologies have come - and perhaps how far they will go.  Created in Asia over 2,500 year ago, Go is exponentially more complex than chess, and at least among humans, it requires an added degree of intuition. Lee Sedol is widely-regarded as the top Go player of the last decade, after winning more international titles than all but one other player. He is currently ranked number five in the world, and according to Demis Hassabis, who leads DeepMind, the Google AI lab that created AlphaGo, his team chose the Korean for this all-important match because they wanted an opponent who would be remembered as one of history\'s great players.  Although AlphaGo topped Lee Sedol in the match\'s first game on Wednesday afternoon, the outcome of Game Two was no easier to predict. In his 1996 match with IBM\'s Deep Blue supercomputer, world chess champion Gary Kasparov lost the first game but then came back to win the second game and, eventually, the match as a whole. It wasn\'t until the following year that Deep Blue topped Kasparov over the course of a six-game contest. The thing to realize is that, after playing AlphaGo for the first time on Wednesday, Lee Sedol could adjust his style of play - just as Kasparov did back in 1996. But AlphaGo could not. Because this Google creation relies so heavily on machine learning techniques, the DeepMind team needs a good four to six weeks to train a new incarnation of the system. And that means they can\'t really change things during this eight-day match.  "This is about teaching and learning," Hassabis told us just before Game Two. "One game is not enough data to learn from - for a machine - and training takes an awful lot of time.', 'word_limit': '100'}
2017-12-03 16:34:38,498 - 5a22fa6ae3e52a40f793f587
2017-12-03 16:34:38,499 - TextSummarizer ------------------------------------ Init
2017-12-03 16:34:38,499 - _id: 5a22fa6ae3e52a40f793f587
2017-12-03 16:34:38,499 - word_limit: 100
2017-12-03 16:38:50,916 - CombinedMultiDict([ImmutableMultiDict([('q', '{}'), ('1512337130804', '')]), ImmutableMultiDict([])])
2017-12-03 16:38:50,916 - {}
2017-12-03 16:39:17,020 - {'content_title': "Google's AI Wins Pivotal Second Game in Match With Go Grandmaster", 'content_text': 'After more than four hours of tight play and a rapid-fire endgame, Google\'s artificially intelligent Go-playing computer system has won a second contest against grandmaster Lee Sedol, taking a two-games-to-none lead in their historic best-of-five match in downtown Seoul.  The surprisingly skillful Google machine, known as AlphaGo, now needs only one more win to claim victory in the match. The Korean-born Lee Sedol will go down in defeat unless he takes each of the match\'s last three games. Though machines have beaten the best humans at chess, checkers, Othello, Scrabble, Jeopardy!, and so many other games considered tests of human intellect, they have never beaten the very best at Go. Game Three is set for Saturday afternoon inside Seoul\'s Four Seasons hotel.  The match is a way of judging the suddenly rapid progress of artificial intelligence. One of the machine-learning techniques at the heart of AlphaGo has already reinvented myriad online services inside Google and other big-name Internet companies, helping to identify images, recognize commands spoken into smartphones, improve search engine results, and more. Meanwhile, another AlphaGo technique is now driving experimental robotics at Google and places like the University of California at Berkeley. This week\'s match can show how far these technologies have come - and perhaps how far they will go.  Created in Asia over 2,500 year ago, Go is exponentially more complex than chess, and at least among humans, it requires an added degree of intuition. Lee Sedol is widely-regarded as the top Go player of the last decade, after winning more international titles than all but one other player. He is currently ranked number five in the world, and according to Demis Hassabis, who leads DeepMind, the Google AI lab that created AlphaGo, his team chose the Korean for this all-important match because they wanted an opponent who would be remembered as one of history\'s great players.  Although AlphaGo topped Lee Sedol in the match\'s first game on Wednesday afternoon, the outcome of Game Two was no easier to predict. In his 1996 match with IBM\'s Deep Blue supercomputer, world chess champion Gary Kasparov lost the first game but then came back to win the second game and, eventually, the match as a whole. It wasn\'t until the following year that Deep Blue topped Kasparov over the course of a six-game contest. The thing to realize is that, after playing AlphaGo for the first time on Wednesday, Lee Sedol could adjust his style of play - just as Kasparov did back in 1996. But AlphaGo could not. Because this Google creation relies so heavily on machine learning techniques, the DeepMind team needs a good four to six weeks to train a new incarnation of the system. And that means they can\'t really change things during this eight-day match.  "This is about teaching and learning," Hassabis told us just before Game Two. "One game is not enough data to learn from - for a machine - and training takes an awful lot of time.', 'word_limit': '300'}
2017-12-03 16:39:17,140 - 5a22fa6ae3e52a40f793f587
2017-12-03 16:39:17,141 - TextSummarizer ------------------------------------ Init
2017-12-03 16:39:17,142 - _id: 5a22fa6ae3e52a40f793f587
2017-12-03 16:39:17,142 - word_limit: 300
2017-12-03 16:39:49,772 - CombinedMultiDict([ImmutableMultiDict([('q', '{}'), ('1512337189663', '')]), ImmutableMultiDict([])])
2017-12-03 16:39:49,772 - {}
2017-12-03 16:40:15,185 - {'review_text': 'I really really like this movie'}
2017-12-03 16:40:15,187 - <pymongo.results.InsertOneResult object at 0x120ca8168>
2017-12-03 16:40:15,187 - 5a246f3fe3e52a74e2e02740
2017-12-03 16:40:15,297 - 5a246f3fe3e52a74e2e02740
2017-12-03 16:40:15,299 - MovieClassifier ------------------------------------ Init
2017-12-03 16:40:15,379 - _id: 5a246f3fe3e52a74e2e02740
2017-12-03 16:40:15,381 - y: 0
2017-12-03 16:40:15,381 - proba: 0.534294436493
2017-12-03 16:40:42,595 - {'review_text': 'I really liked this movie'}
2017-12-03 16:40:42,711 - 5a246f3fe3e52a74e2e02740
2017-12-03 16:40:42,717 - MovieClassifier ------------------------------------ Init
2017-12-03 16:40:42,732 - _id: 5a246f3fe3e52a74e2e02740
2017-12-03 16:40:42,733 - y: 1
2017-12-03 16:40:42,733 - proba: 0.694974259736
2017-12-03 16:40:54,961 - {'review_text': 'I hated this movie'}
2017-12-03 16:40:55,080 - 5a246f3fe3e52a74e2e02740
2017-12-03 16:40:55,084 - MovieClassifier ------------------------------------ Init
2017-12-03 16:40:55,100 - _id: 5a246f3fe3e52a74e2e02740
2017-12-03 16:40:55,101 - y: 0
2017-12-03 16:40:55,101 - proba: 0.667968736788
2017-12-03 16:44:53,389 - CombinedMultiDict([ImmutableMultiDict([('q', '{}'), ('1512337493278', '')]), ImmutableMultiDict([])])
2017-12-03 16:44:53,392 - {}
2017-12-03 16:45:03,189 - {'review_text': 'I really liked this movie'}
2017-12-03 16:45:03,198 - <pymongo.results.InsertOneResult object at 0x120d8a510>
2017-12-03 16:45:03,198 - 5a24705fe3e52a74e2e02741
2017-12-03 16:45:03,310 - 5a24705fe3e52a74e2e02741
2017-12-03 16:45:03,314 - MovieClassifier ------------------------------------ Init
2017-12-03 16:45:03,386 - _id: 5a24705fe3e52a74e2e02741
2017-12-03 16:45:03,387 - y: 1
2017-12-03 16:45:03,387 - proba: 0.694974259736
2017-12-03 16:45:48,303 - CombinedMultiDict([ImmutableMultiDict([('q', '{}'), ('1512337548192', '')]), ImmutableMultiDict([])])
2017-12-03 16:45:48,305 - {}
2017-12-03 16:46:11,119 - {'content_title': "Google's AI Wins Pivotal Second Game in Match With Go Grandmaster", 'content_text': 'After more than four hours of tight play and a rapid-fire endgame, Google\'s artificially intelligent Go-playing computer system has won a second contest against grandmaster Lee Sedol, taking a two-games-to-none lead in their historic best-of-five match in downtown Seoul.  The surprisingly skillful Google machine, known as AlphaGo, now needs only one more win to claim victory in the match. The Korean-born Lee Sedol will go down in defeat unless he takes each of the match\'s last three games. Though machines have beaten the best humans at chess, checkers, Othello, Scrabble, Jeopardy!, and so many other games considered tests of human intellect, they have never beaten the very best at Go. Game Three is set for Saturday afternoon inside Seoul\'s Four Seasons hotel.  The match is a way of judging the suddenly rapid progress of artificial intelligence. One of the machine-learning techniques at the heart of AlphaGo has already reinvented myriad online services inside Google and other big-name Internet companies, helping to identify images, recognize commands spoken into smartphones, improve search engine results, and more. Meanwhile, another AlphaGo technique is now driving experimental robotics at Google and places like the University of California at Berkeley. This week\'s match can show how far these technologies have come - and perhaps how far they will go.  Created in Asia over 2,500 year ago, Go is exponentially more complex than chess, and at least among humans, it requires an added degree of intuition. Lee Sedol is widely-regarded as the top Go player of the last decade, after winning more international titles than all but one other player. He is currently ranked number five in the world, and according to Demis Hassabis, who leads DeepMind, the Google AI lab that created AlphaGo, his team chose the Korean for this all-important match because they wanted an opponent who would be remembered as one of history\'s great players.  Although AlphaGo topped Lee Sedol in the match\'s first game on Wednesday afternoon, the outcome of Game Two was no easier to predict. In his 1996 match with IBM\'s Deep Blue supercomputer, world chess champion Gary Kasparov lost the first game but then came back to win the second game and, eventually, the match as a whole. It wasn\'t until the following year that Deep Blue topped Kasparov over the course of a six-game contest. The thing to realize is that, after playing AlphaGo for the first time on Wednesday, Lee Sedol could adjust his style of play - just as Kasparov did back in 1996. But AlphaGo could not. Because this Google creation relies so heavily on machine learning techniques, the DeepMind team needs a good four to six weeks to train a new incarnation of the system. And that means they can\'t really change things during this eight-day match.  "This is about teaching and learning," Hassabis told us just before Game Two. "One game is not enough data to learn from - for a machine - and training takes an awful lot of time.', 'word_limit': '100'}
2017-12-03 16:46:11,235 - 5a22fa6ae3e52a40f793f587
2017-12-03 16:46:11,236 - TextSummarizer ------------------------------------ Init
2017-12-03 16:46:11,236 - _id: 5a22fa6ae3e52a40f793f587
2017-12-03 16:46:11,236 - word_limit: 100
2017-12-03 19:32:28,584 - CombinedMultiDict([ImmutableMultiDict([('q', '{}'), ('1512347548465', '')]), ImmutableMultiDict([])])
2017-12-03 19:32:28,585 - {}
2017-12-03 19:32:32,695 - {'review_text': 'I really liked this movie'}
2017-12-03 19:32:32,806 - 5a24705fe3e52a74e2e02741
2017-12-03 19:32:32,807 - MovieClassifier ------------------------------------ Init
2017-12-03 19:32:32,845 - _id: 5a24705fe3e52a74e2e02741
2017-12-03 19:32:32,848 - y: 1
2017-12-03 19:32:32,848 - proba: 0.694974259736
2017-12-03 19:32:48,030 - 5a24705fe3e52a74e2e02741
2017-12-03 19:32:48,032 - MovieClassifier ------------------------------------ Init
2017-12-03 19:32:48,048 - _id: 5a24705fe3e52a74e2e02741
2017-12-03 19:32:48,050 - y: 1
2017-12-03 19:32:48,050 - proba: 0.694974259736
2017-12-03 19:32:48,279 - CombinedMultiDict([ImmutableMultiDict([]), ImmutableMultiDict([])])
2017-12-03 19:32:48,279 - None
2017-12-03 19:35:26,771 - CombinedMultiDict([ImmutableMultiDict([('q', '{}'), ('1512347726662', '')]), ImmutableMultiDict([])])
2017-12-03 19:35:26,771 - {}
2017-12-03 19:36:10,844 - {'review_text': 'The movie was awesome!!'}
2017-12-03 19:36:10,852 - <pymongo.results.InsertOneResult object at 0x118d185e8>
2017-12-03 19:36:10,852 - 5a24987ae3e52a74e2e02742
2017-12-03 19:36:10,961 - 5a24987ae3e52a74e2e02742
2017-12-03 19:36:10,967 - MovieClassifier ------------------------------------ Init
2017-12-03 19:36:10,981 - _id: 5a24987ae3e52a74e2e02742
2017-12-03 19:36:10,982 - y: 1
2017-12-03 19:36:10,982 - proba: 0.697682088771
